{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from envs.vases_grid import VasesGrid, VasesEnvState, print_state, str_to_state, state_to_str\n",
    "from envs.utils import unique_perm, zeros_with_ones, printoptions\n",
    "from envs.vases_spec import VasesEnvState2x3V2D3, VasesEnvSpec2x3V2D3, VasesEnvState2x3Broken, VasesEnvSpec2x3Broken\n",
    "#from envs.vases_spec import VasesEnvSpec2x4Broken, VasesEnvState2x4Broken\n",
    "\n",
    "from value_iter_and_policy import vi_boltzmann, vi_boltzmann_deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_d_last_step_discrete(mdp, policy, p_0, T, verbose=False):\n",
    "    '''Computes the last-step occupancy measure'''\n",
    "    \n",
    "    D_prev = p_0 \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(p_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # due to env being deterministic, sprime=self.P[s][a][0][1] and p_sprime=1\n",
    "                D[mdp.P[s][a][0][1]] += D_prev[s] * policy[s, a] \n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def om_method(mdp, s_current, p_0, horizon, temp=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''Modified MaxCausalEnt that maximizes last step occupancy measure for the current state'''\n",
    "     \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.f_matrix.shape[1])\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann_deterministic(mdp, 1, mdp.f_matrix @ r_vec, horizon, temp) \n",
    "            \n",
    "            D = compute_d_last_step_discrete(mdp, policy, p_0, horizon)   \n",
    "            dL_dr_vec = -(s_current - D) @ mdp.f_matrix\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should perform the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            if i%40==0:\n",
    "                with printoptions(precision=4, suppress=True):\n",
    "                    print('Epoch {}; Reward vector: {}'.format(i, r_vec))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_wrapper(env,\n",
    "                     horizon=22, #number of steps we assume the expert was acting previously\n",
    "                     temp=1,\n",
    "                     learning_rate=.1,\n",
    "                     epochs = 200,\n",
    "                     s_current=None):\n",
    "\n",
    "    print('Initial state:')\n",
    "    print_state(env.init_state)\n",
    "\n",
    "    p_0=np.zeros(env.nS)\n",
    "    p_0[env.state_num[state_to_str(env.init_state)]] = 1\n",
    "    \n",
    "    if s_current is None: s_current = np.copy(p_0)\n",
    "    \n",
    "    r_vec = om_method(env, s_current, p_0, horizon, temp, epochs, learning_rate)\n",
    "    with printoptions(precision=4, suppress=True):\n",
    "        print(); print('Final reward vector: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_rl(env, r, h=40, temp=.1, steps_printed=15, current_s=None):\n",
    "    '''Given an env and R, runs soft VI for h steps and rolls out the resulting policy'''\n",
    "    V, Q, policy = vi_boltzmann_deterministic(env, 1, env.f_matrix @ r, h, temp) \n",
    "    \n",
    "    if current_s is None: \n",
    "        env.reset()\n",
    "    else:\n",
    "        env.s = str_to_state(env.num_state[np.where(current_s)[0][0]])\n",
    "    print_state(env.s); print()\n",
    "    for i in range(steps_printed):\n",
    "        a = np.random.choice(5,p=policy[env.state_num[state_to_str(env.s)],:])\n",
    "        env.step(a)\n",
    "        print_state(env.s)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Baseline: use $R_{rl}$ that rewards the agent for tablecloths on tables; both vases get broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_rl = np.array([0, 0, 1, 0, 0, 0])\n",
    "env2x3v2d3 = VasesGrid(VasesEnvSpec2x3V2D3(), VasesEnvState2x3V2D3())\n",
    "forward_rl(env2x3v2d3, r_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment where we know that the starting state had no broken vases, and the expert broke no vases during its acting in the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current state\n",
    "print('Current state:')\n",
    "print_state(env2x3v2d3.init_state)\n",
    "\n",
    "np.random.seed(1)\n",
    "r_learned = experiment_wrapper(env2x3v2d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the learned R_h + R* with a reward function R_rl that rewards the \n",
    "# agent for tablecloths on tables.\n",
    "# No vases broken!\n",
    "forward_rl(env2x3v2d3, r_rl + r_learned, steps_printed=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Experiment where we know that the starting state had no broken vases, and the expert broke a vase while getting to the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "s_current = np.zeros(env2x3v2d3.nS)\n",
    "s_current[1050] = 1\n",
    "\n",
    "print('Current state:')\n",
    "print_state(str_to_state(env2x3v2d3.num_state[1050]))\n",
    "\n",
    "r_learned_broken = experiment_wrapper(env2x3v2d3, s_current=s_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The agent learns that the expert cared about breaking vases and vases on tables,\n",
    "# and breaks the remaining vase\n",
    "forward_rl(env2x3v2d3, r_rl + r_learned_broken, current_s=s_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiment in which the expert starts out in an env with a broken vase and doesn't break any vases during its acting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2x3v2d3.init_state = str_to_state(env2x3v2d3.num_state[1050])\n",
    "\n",
    "np.random.seed(1)\n",
    "s_current = np.zeros(env2x3v2d3.nS)\n",
    "s_current[1050] = 1\n",
    "\n",
    "print('Current state:')\n",
    "print_state(str_to_state(env2x3v2d3.num_state[1050]))\n",
    "\n",
    "r_learned_broken = experiment_wrapper(env2x3v2d3, s_current=s_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent learns that the expert cared about vases on tables, \n",
    "# and doesn't break the remaining vase\n",
    "forward_rl(env2x3v2d3, r_rl + r_learned_broken, current_s=s_current, steps_printed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Same experiment as one from sec 2 but without features for vases on tables & desks\n",
    "\n",
    "Removing these features does result in the agent learning a substantial negative reward on broken vases, and not breaking any vases when the learned reward is combined with $R_{rl}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the env since we changed it in sec 4, and remove the features\n",
    "features_to_keep = np.array([0, 2, 3, 5])\n",
    "env2x3v2d3 = VasesGrid(VasesEnvSpec2x3V2D3(), VasesEnvState2x3V2D3())\n",
    "env2x3v2d3.f_matrix = env2x3v2d3.f_matrix[:, features_to_keep]\n",
    "print(env2x3v2d3.f_matrix.shape)\n",
    "\n",
    "# print current state\n",
    "print('Current state:')\n",
    "print_state(env2x3v2d3.init_state)\n",
    "\n",
    "np.random.seed(1)\n",
    "r_learned = experiment_wrapper(env2x3v2d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the learned R_h + R* with a reward function R_rl that rewards the \n",
    "# agent for tablecloths on tables.\n",
    "# No vases broken!\n",
    "r_rl_4feat = np.array([0, 1, 0, 0])\n",
    "forward_rl(env2x3v2d3, r_rl_4feat + r_learned, steps_printed=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Experiments with different horizon lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "env2x4_broken = VasesGrid(VasesEnvSpec2x4Broken(), VasesEnvState2x4Broken())\n",
    "r_broken_very_small_h = experiment_wrapper(env2x4_broken, horizon=3)\n",
    "r_broken_small_h = experiment_wrapper(env2x4_broken, horizon=10)\n",
    "r_broken_medium_h = experiment_wrapper(env2x4_broken, horizon=25)\n",
    "r_broken_large_h = experiment_wrapper(env2x4_broken, horizon=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forward_rl(env2x4_broken, r_rl + r_broken_very_small_h, steps_printed=25)\n",
    "forward_rl(env2x4_broken, r_rl + r_broken_small_h, steps_printed=25)\n",
    "forward_rl(env2x4_broken, r_rl + r_broken_medium_h, steps_printed=25)\n",
    "forward_rl(env2x4_broken, r_rl + r_broken_large_h, steps_printed=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
