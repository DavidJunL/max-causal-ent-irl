{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from itertools import permutations\n",
    "\n",
    "from envs.vases_grid import VasesGrid, VasesEnvSpec, VasesEnvState, print_state, str_to_state, state_to_str\n",
    "from envs.utils import unique_perm, zeros_with_ones\n",
    "from envs.vases_spec import VasesEnvState2x3, VasesEnvSpec2x3, VasesEnvState3x3, VasesEnvSpec3x3\n",
    "\n",
    "from value_iter_and_policy import vi_boltzmann, vi_boltzmann_deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_D_last_step_discrete(mdp, policy, p_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = p_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(p_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # due to env being deterministic, sprime=self.P[s][a][1] and p_sprime=1\n",
    "                D[mdp.P[s][a][1]] += D_prev[s] * policy[s, a] \n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, p_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.feature_matrix.shape[1])\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann_deterministic(mdp, 1, mdp.feature_matrix @ r_vec, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step_discrete(mdp, policy, p_0, horizon)   \n",
    "            dL_dr_vec = -(current_state - D) @ mdp.feature_matrix\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            if i%20==0:\n",
    "                print('Epoch {}'.format(i))\n",
    "                print('Reward vector: {}'.format(r_vec))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            #print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment2x3(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 500):\n",
    "\n",
    "    # make env\n",
    "    env_spec_2x3 = VasesEnvSpec2x3()\n",
    "    init_s_2x3 = VasesEnvState2x3()\n",
    "    env2x3 = VasesGrid(env_spec_2x3, init_s_2x3)\n",
    "    env2x3.enumerate_states()\n",
    "    env2x3.make_feature_matrix()\n",
    "    env2x3.get_deterministic_transitions()\n",
    "    \n",
    "    print('Initial state:')\n",
    "    print_state(init_s_2x3)\n",
    "\n",
    "    p_0=np.zeros(env2x3.nS)\n",
    "    p_0[env2x3.state_num[state_to_str(init_s_2x3)]] = 1\n",
    "    \n",
    "    current_state = np.copy(p_0)\n",
    "    \n",
    "    r_vec = OM_method(env2x3, current_state, p_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "|\u001b[0;35;85m█\u001b[0m\u001b[0;32;85m█\u001b[0m|  |  |\n",
      "|\u001b[0;33;85m█\u001b[0m |\u001b[0m↑\u001b[0m |\u001b[0;33;85m█\u001b[0m |\n",
      "|––|––|––|\n",
      "|  |  |  |\n",
      "|  |  |  |\n",
      "Initial reward vector: [ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408 -0.02301539]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.01126047 -0.00956236 -0.00529616 -0.01076787  0.03454158 -0.02203465]\n",
      "Epoch 20\n",
      "Reward vector: [-0.00141373 -0.01893179 -0.00533257 -0.01086848  0.14138968 -0.01954165]\n",
      "Epoch 40\n",
      "Reward vector: [-0.00318339 -0.02029471 -0.00533808 -0.01088433  0.17692085 -0.01918785]\n",
      "Epoch 60\n",
      "Reward vector: [-0.00391209 -0.0208551  -0.00534045 -0.01089114  0.20157682 -0.019044  ]\n",
      "Epoch 80\n",
      "Reward vector: [-0.00429306 -0.02114793 -0.00534172 -0.01089478  0.22147148 -0.01897044]\n",
      "Epoch 100\n",
      "Reward vector: [-0.004517   -0.02132111 -0.00534248 -0.01089695  0.23863008 -0.01892835]\n",
      "Epoch 120\n",
      "Reward vector: [-0.00465841 -0.02143218 -0.00534296 -0.01089832  0.25398193 -0.01890254]\n",
      "Epoch 140\n",
      "Reward vector: [-0.00475219 -0.02150782 -0.00534329 -0.01089923  0.26803381 -0.01888594]\n",
      "Epoch 160\n",
      "Reward vector: [-0.00481667 -0.02156186 -0.00534352 -0.01089985  0.2810941  -0.01887488]\n",
      "Epoch 180\n",
      "Reward vector: [-0.00486227 -0.02160203 -0.00534368 -0.01090029  0.29336543 -0.01886729]\n",
      "Epoch 200\n",
      "Reward vector: [-0.00489525 -0.02163292 -0.0053438  -0.0109006   0.30498875 -0.01886196]\n",
      "Epoch 220\n",
      "Reward vector: [-0.00491954 -0.02165737 -0.00534388 -0.01090083  0.31606655 -0.01885815]\n",
      "Epoch 240\n",
      "Reward vector: [-0.00493772 -0.02167721 -0.00534395 -0.01090101  0.32667603 -0.01885537]\n",
      "Epoch 260\n",
      "Reward vector: [-0.0049515  -0.02169367 -0.005344   -0.01090114  0.33687707 -0.01885332]\n",
      "Epoch 280\n",
      "Reward vector: [-0.00496208 -0.02170758 -0.00534404 -0.01090123  0.3467172  -0.01885179]\n",
      "Epoch 300\n",
      "Reward vector: [-0.00497028 -0.02171952 -0.00534407 -0.01090131  0.35623495 -0.01885063]\n",
      "Epoch 320\n",
      "Reward vector: [-0.00497669 -0.02172991 -0.00534409 -0.01090137  0.36546209 -0.01884974]\n",
      "Epoch 340\n",
      "Reward vector: [-0.00498175 -0.02173906 -0.00534411 -0.01090142  0.37442521 -0.01884905]\n",
      "Epoch 360\n",
      "Reward vector: [-0.00498577 -0.02174719 -0.00534412 -0.01090145  0.38314685 -0.01884852]\n",
      "Epoch 380\n",
      "Reward vector: [-0.00498898 -0.0217545  -0.00534414 -0.01090148  0.39164632 -0.0188481 ]\n",
      "Epoch 400\n",
      "Reward vector: [-0.00499157 -0.0217611  -0.00534415 -0.0109015   0.39994036 -0.01884777]\n",
      "Epoch 420\n",
      "Reward vector: [-0.00499366 -0.02176711 -0.00534415 -0.01090152  0.40804357 -0.0188475 ]\n",
      "Epoch 440\n",
      "Reward vector: [-0.00499536 -0.0217726  -0.00534416 -0.01090154  0.41596881 -0.01884729]\n",
      "Epoch 460\n",
      "Reward vector: [-0.00499676 -0.02177766 -0.00534417 -0.01090155  0.42372748 -0.01884712]\n",
      "Epoch 480\n",
      "Reward vector: [-0.0049979  -0.02178232 -0.00534417 -0.01090156  0.43132974 -0.01884698]\n",
      "Final reward weights:  [-0.0049988  -0.02178644 -0.00534417 -0.01090157  0.43841533 -0.01884687]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0049988 , -0.02178644, -0.00534417, -0.01090157,  0.43841533,\n",
       "       -0.01884687])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "experiment2x3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
