{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdps import MDP_toy_irreversibility, MDP_chain, MDP_water\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  0.   0.5]\n",
      "[ 0.25  0.    0.25  0.5 ]\n",
      "[ 0.     0.25   0.125  0.625]\n",
      "[ 0.125   0.0625  0.1875  0.625 ]\n",
      "[ 0.03125  0.15625  0.125    0.6875 ]\n",
      "[ 0.078125  0.078125  0.140625  0.703125]\n",
      "[ 0.0390625  0.109375   0.109375   0.7421875]\n",
      "[ 0.0546875   0.07421875  0.109375    0.76171875]\n",
      "[ 0.03710938  0.08203125  0.09179688  0.7890625 ]\n",
      "[ 0.04101562  0.06445312  0.08691406  0.80761719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04101562,  0.06445312,  0.08691406,  0.80761719])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(P_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "\n",
    "mdp = MDP_toy_irreversibility()    \n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "# A small example demonstrating what last-step OM looks like for a uniformly random policy.\n",
    "# We can see that most of the probability mass is on the agent ending up in irreversible s_3.\n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.nS)\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vec, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vec = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vec))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_toy_irreversibility()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[1]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[1]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [ 0.00259818 -0.00662437 -0.01800419 -0.01326019]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.00057676  0.08973878 -0.02247768 -0.10312845]\n",
      "Last-step D: [ 0.02021416  0.03636841  0.04473486  0.89868257] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [-0.00756186  0.1685345  -0.05137277 -0.14489045]\n",
      "Last-step D: [ 0.08138625  0.21204283  0.28895088  0.41762004] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [-0.01843617  0.23854151 -0.09078604 -0.16460987]\n",
      "Last-step D: [ 0.10874313  0.29992992  0.39413277  0.19719418] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [-0.03076185  0.30515471 -0.13199238 -0.17769105]\n",
      "Last-step D: [ 0.12325679  0.33386796  0.41206338  0.13081187] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [-0.04429005  0.3695363  -0.17319462 -0.1873422 ]\n",
      "Last-step D: [ 0.13528201  0.35618418  0.41202241  0.09651141] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [-0.05889815  0.43216931 -0.21376509 -0.19479664]\n",
      "Last-step D: [ 0.146081    0.37366981  0.40570469  0.0745445 ] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [-0.07449095  0.49332628 -0.25343933 -0.20068658]\n",
      "Last-step D: [ 0.155928    0.3884303   0.39674238  0.05889932] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [-0.09097984  0.55318802 -0.29210265 -0.2053961 ]\n",
      "Last-step D: [ 0.16488884  0.40138266  0.38663324  0.04709527] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [-0.10827779  0.61188543 -0.32971266 -0.20918556]\n",
      "Last-step D: [ 0.17297951  0.41302588  0.37610005  0.03789456] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [-0.12629884  0.66951784 -0.36626522 -0.21224436]\n",
      "Last-step D: [ 0.1802105   0.42367591  0.3655256   0.03058799] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [-0.14495886  0.72616258 -0.40177757 -0.21471673]\n",
      "Last-step D: [ 0.1866002   0.43355255  0.35512352  0.02472373] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [-0.16417668  0.78188093 -0.43627913 -0.2167157 ]\n",
      "Last-step D: [ 0.19217818  0.44281655  0.34501557  0.01998969] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [-0.18387512  0.83672214 -0.46980619 -0.2183314 ]\n",
      "Last-step D: [ 0.19698443  0.4515879   0.33527064  0.01615703] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [-0.20398185  0.89072653 -0.50239877 -0.21963648]\n",
      "Last-step D: [ 0.20106729  0.45995613  0.3259258   0.01305078] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [-0.22442994  0.9439278  -0.53409862 -0.22068981]\n",
      "Last-step D: [ 0.20448091  0.46798724  0.31699853  0.01053333] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [-0.24515822  0.99635493 -0.56494802 -0.22153926]\n",
      "Last-step D: [ 0.20728286  0.47572874  0.30849396  0.00849444] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [-0.26611143  1.04803355 -0.59498896 -0.22222374]\n",
      "Last-step D: [ 0.20953201  0.48321378  0.30040937  0.00684484] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [-0.2872401   1.0989871  -0.62426266 -0.22277492]\n",
      "Last-step D: [ 0.21128674  0.49046447  0.292737    0.00551178] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [-0.30850046  1.14923763 -0.65280924 -0.2232185 ]\n",
      "Last-step D: [ 0.21260361  0.49749468  0.28546588  0.00443583] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [-0.32985409  1.1988064  -0.68066754 -0.22357534]\n",
      "Last-step D: [ 0.2135363   0.50431232  0.27858298  0.0035684 ] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [-0.35126759  1.24771428 -0.70787494 -0.22386232]\n",
      "Last-step D: [ 0.214135    0.51092121  0.27207399  0.0028698 ] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [-0.37271219  1.29598203 -0.73446733 -0.22409309]\n",
      "Last-step D: [ 0.21444595  0.51732255  0.26592385  0.00230766] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [-0.39416331  1.34363043 -0.76047904 -0.22427865]\n",
      "Last-step D: [ 0.21451123  0.523516    0.26011712  0.00185564] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [-0.41560019  1.39068037 -0.78594286 -0.22442789]\n",
      "Last-step D: [ 0.21436879  0.52950061  0.25463823  0.00149238] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [-0.43700544  1.43715283 -0.81089003 -0.22454795]\n",
      "Last-step D: [ 0.21405248  0.53527534  0.24947165  0.00120053] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [-0.45836467  1.48306888 -0.83535023 -0.22464456]\n",
      "Last-step D: [ 0.2135923   0.54083951  0.24460208  0.00096611] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [-0.47966613  1.52844957 -0.85935168 -0.22472234]\n",
      "Last-step D: [ 0.2130146   0.54619307  0.24001451  0.00077782] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [-0.50090037  1.5733159  -0.88292112 -0.22478499]\n",
      "Last-step D: [ 0.21234241  0.55133672  0.23569431  0.00062655] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [-0.52205994  1.6176887  -0.90608385 -0.22483549]\n",
      "Last-step D: [  2.11595679e-01   5.56271979e-01   2.31627329e-01   5.05012313e-04] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [-0.5431391   1.66158859 -0.92886384 -0.22487623]\n",
      "Last-step D: [  2.10791635e-01   5.61001164e-01   2.27799887e-01   4.07313593e-04] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [-0.5641336   1.70503585 -0.95128372 -0.2249091 ]\n",
      "Last-step D: [  2.09945036e-01   5.65527372e-01   2.24198847e-01   3.28744370e-04] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [-0.58504045  1.74805041 -0.97336488 -0.22493565]\n",
      "Last-step D: [  2.09068468e-01   5.69854388e-01   2.20811620e-01   2.65524284e-04] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [-0.60585771  1.79065175 -0.9951275  -0.22495711]\n",
      "Last-step D: [  2.08172594e-01   5.73986601e-01   2.17626182e-01   2.14623204e-04] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [-0.62658435  1.83285886 -1.01659061 -0.22497448]\n",
      "Last-step D: [  2.07266396e-01   5.77928911e-01   2.14631079e-01   1.73612921e-04] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [-0.64722009  1.8746902  -1.03777215 -0.22498853]\n",
      "Last-step D: [  2.06357392e-01   5.81686637e-01   2.11815423e-01   1.40547672e-04] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [-0.66776527  1.91616365 -1.05868904 -0.22499992]\n",
      "Last-step D: [  2.05451824e-01   5.85265422e-01   2.09168886e-01   1.13868107e-04] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [-0.68822075  1.95729654 -1.07935721 -0.22500915]\n",
      "Last-step D: [  2.04554837e-01   5.88671153e-01   2.06681686e-01   9.23242160e-05] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [-0.70858782  1.99810555 -1.09979167 -0.22501664]\n",
      "Last-step D: [  2.03670628e-01   5.91909883e-01   2.04344575e-01   7.49135716e-05] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [-0.72886807  2.03860677 -1.12000655 -0.22502272]\n",
      "Last-step D: [  2.02802582e-01   5.94987766e-01   2.02148820e-01   6.08318928e-05] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [-0.74906341  2.07881567 -1.14001517 -0.22502767]\n",
      "Last-step D: [  2.01953389e-01   5.97910996e-01   2.00086182e-01   4.94334966e-05] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [-0.76917593  2.1187471  -1.15983006 -0.22503169]\n",
      "Last-step D: [  2.01125148e-01   6.00685756e-01   1.98148897e-01   4.01996811e-05] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [-0.78920787  2.15841528 -1.17946302 -0.22503496]\n",
      "Last-step D: [  2.00319455e-01   6.03318180e-01   1.96329652e-01   3.27134509e-05] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [-0.80916162  2.19783385 -1.19892518 -0.22503762]\n",
      "Last-step D: [  1.99537482e-01   6.05814313e-01   1.94621566e-01   2.66393170e-05] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [-0.82903963  2.23701584 -1.21822699 -0.22503979]\n",
      "Last-step D: [  1.98780042e-01   6.08180088e-01   1.93018163e-01   2.17071473e-05] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [-0.84884439  2.27597371 -1.23737833 -0.22504156]\n",
      "Last-step D: [  1.98047649e-01   6.10421297e-01   1.91513355e-01   1.76992534e-05] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [-0.86857845  2.31471935 -1.25638847 -0.22504301]\n",
      "Last-step D: [  1.97340563e-01   6.12543579e-01   1.90101417e-01   1.44400598e-05] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [-0.88824433  2.35326411 -1.27526617 -0.22504419]\n",
      "Last-step D: [  1.96658840e-01   6.14552403e-01   1.88776969e-01   1.17878357e-05] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [-0.90784457  2.39161881 -1.29401966 -0.22504515]\n",
      "Last-step D: [  1.96002360e-01   6.16453060e-01   1.87534952e-01   9.62807186e-06] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [-0.92738165  2.42979374 -1.31265672 -0.22504594]\n",
      "Last-step D: [  1.95370863e-01   6.18250656e-01   1.86370612e-01   7.86817277e-06] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [-0.94685805  2.46779873 -1.33118467 -0.22504658]\n",
      "Last-step D: [  1.94763976e-01   6.19950110e-01   1.85279481e-01   6.43319937e-06] \n",
      "\n",
      "Final reward weights:  [-0.94685805  2.46779873 -1.33118467 -0.22504658]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.94685805,  2.46779873, -1.33118467, -0.22504658])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(1)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [-0.00377623  0.01186242  0.00531444  0.01626107  0.00190277  0.01659344\n",
      " -0.02781351  0.00451964]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.09622377  0.01089866  0.00246761 -0.00511251  0.00033109  0.01239497\n",
      " -0.04602453 -0.04631504]\n",
      "Last-step D: [  2.84631898e-11   9.63752955e-03   2.84682718e-02   2.13735737e-01\n",
      "   1.57167701e-02   4.19846282e-02   1.82110186e-01   5.08346878e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.19622377  0.00903644 -0.0025203  -0.03350289 -0.00283464  0.00471324\n",
      " -0.07005079 -0.07620081]\n",
      "Last-step D: [  4.73212143e-10   1.86222289e-02   4.98791020e-02   2.83903840e-01\n",
      "   3.16572902e-02   7.68173244e-02   2.40262581e-01   2.98857633e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.29622377  0.00613175 -0.00932433 -0.06129897 -0.00802126 -0.00629689\n",
      " -0.09337547 -0.09917457]\n",
      "Last-step D: [  6.63541424e-09   2.90468996e-02   6.80403060e-02   2.77960788e-01\n",
      "   5.18662699e-02   1.10101337e-01   2.33246751e-01   2.29737641e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.39622376  0.00207707 -0.01758125 -0.08681177 -0.01555713 -0.0201264\n",
      " -0.11463623 -0.11872402]\n",
      "Last-step D: [  8.84884299e-08   4.05468131e-02   8.25692104e-02   2.55128068e-01\n",
      "   7.53586568e-02   1.38295035e-01   2.12607636e-01   1.95494492e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.49622364 -0.00311958 -0.02683509 -0.10993808 -0.02549509 -0.03589781\n",
      " -0.13381532 -0.13625866]\n",
      "Last-step D: [  1.13877410e-06   5.19664941e-02   9.25383622e-02   2.31263049e-01\n",
      "   9.93795663e-02   1.57714112e-01   1.91790896e-01   1.75346381e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.59622221 -0.00938222 -0.03670592 -0.13101348 -0.03768794 -0.0527874\n",
      " -0.15123186 -0.15254937]\n",
      "Last-step D: [  1.43334413e-05   6.26264028e-02   9.87083068e-02   2.10754006e-01\n",
      "   1.21928525e-01   1.68895891e-01   1.74165429e-01   1.62907106e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.69620444 -0.01664934 -0.04695044 -0.1503566  -0.05195046 -0.07020419\n",
      " -0.16715631 -0.16807307]\n",
      "Last-step D: [  1.77711555e-04   7.26712221e-02   1.02445260e-01   1.93431211e-01\n",
      "   1.42625210e-01   1.74167968e-01   1.59244438e-01   1.55236978e-01] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.79598884 -0.02500885 -0.0574234  -0.1680216  -0.06825968 -0.08769302\n",
      " -0.18160075 -0.18311752]\n",
      "Last-step D: [ 0.00215595  0.08359507  0.10472956  0.17664998  0.16309218  0.1748883\n",
      "  0.14444442  0.15044453] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.89358641 -0.03486077 -0.06784175 -0.18337868 -0.08664974 -0.10441409\n",
      " -0.19396939 -0.19760796]\n",
      "Last-step D: [ 0.02402433  0.09851918  0.1041835   0.15357084  0.18390066  0.16721067\n",
      "  0.12368646  0.14490436] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.97533241 -0.04543579 -0.07666085 -0.19434499 -0.10420859 -0.11756259\n",
      " -0.20255866 -0.20969692]\n",
      "Last-step D: [ 0.18254006  0.10575021  0.08819096  0.10966309  0.17558844  0.13148501\n",
      "  0.08589263  0.1208896 ] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 1.02125183 -0.05239991 -0.08168342 -0.1999143  -0.11450043 -0.12456129\n",
      " -0.20681915 -0.2165093 ]\n",
      "Last-step D: [ 0.54080571  0.06964122  0.05022575  0.05569312  0.1029184   0.06998697\n",
      "  0.04260495  0.06812387] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 1.04686369 -0.05644742 -0.08452046 -0.20296882 -0.12019588 -0.12841081\n",
      " -0.20913935 -0.22031693]\n",
      "Last-step D: [ 0.74388147  0.04047509  0.02837038  0.03054521  0.05695449  0.0384952\n",
      "  0.02320196  0.03807621] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 1.06402837 -0.0591892  -0.08643645 -0.20501822 -0.12397371 -0.13098293\n",
      " -0.21069326 -0.22287057]\n",
      "Last-step D: [ 0.82835322  0.02741779  0.01915988  0.02049399  0.03777836  0.02572121\n",
      "  0.01553913  0.02553643] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 1.07683213 -0.06124175 -0.08787302 -0.20655268 -0.12676825 -0.13290031\n",
      " -0.21185614 -0.22477595]\n",
      "Last-step D: [ 0.87196238  0.02052548  0.01436578  0.01534458  0.02794533  0.01917383\n",
      "  0.01162883  0.01905379] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 1.0870065  -0.06287477 -0.08901891 -0.20777687 -0.12897391 -0.13442401\n",
      " -0.21278382 -0.22629019]\n",
      "Last-step D: [ 0.89825632  0.01633023  0.01145887  0.01224186  0.0220566   0.01523696\n",
      "  0.00927674  0.01514241] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 1.0954309  -0.06422724 -0.08997052 -0.20879427 -0.13078997 -0.135686\n",
      " -0.21355486 -0.22754401]\n",
      "Last-step D: [ 0.91575596  0.01352476  0.00951606  0.01017399  0.01816068  0.0126199\n",
      "  0.00771045  0.01253821] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 1.10260989 -0.06537954 -0.0907834  -0.20966419 -0.1323302  -0.13676183\n",
      " -0.21421425 -0.22861245]\n",
      "Last-step D: [ 0.92821008  0.01152294  0.00812879  0.00869926  0.01540231  0.0107583\n",
      "  0.00659391  0.0106844 ] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 1.10885875 -0.06638211 -0.09149238 -0.21042373 -0.13366536 -0.13769865\n",
      " -0.21479008 -0.22954242]\n",
      "Last-step D: [ 0.93751145  0.01002568  0.00708989  0.00759534  0.01335151  0.00936817\n",
      "  0.00575829  0.00929968] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 1.11438724 -0.06726862 -0.09212073 -0.21109757 -0.13484231 -0.1385278\n",
      " -0.21530105 -0.23036513]\n",
      "Last-step D: [ 0.94471511  0.00886514  0.00628343  0.00673841  0.01176956  0.00829152\n",
      "  0.00510969  0.00822714] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 1.1193418  -0.06806264 -0.09268469 -0.21170298 -0.13589368 -0.13927116\n",
      " -0.21576023 -0.23110239]\n",
      "Last-step D: [ 0.95045435  0.00794014  0.00563964  0.00605417  0.01051369  0.00743362\n",
      "  0.0045918   0.00737258] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 1.12382862 -0.06878125 -0.0931961  -0.21225252 -0.13684303 -0.13994459\n",
      " -0.21617711 -0.23177   ]\n",
      "Last-step D: [ 0.95513178  0.00718616  0.00511405  0.00549532  0.00949346  0.00673432\n",
      "  0.00416881  0.0066761 ] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 1.12792709 -0.06943727 -0.0936638  -0.21275556 -0.13770791 -0.14055995\n",
      " -0.2165588  -0.23237978]\n",
      "Last-step D: [ 0.95901532  0.00656015  0.00467699  0.00503039  0.00864886  0.00615358\n",
      "  0.00381688  0.00609783] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 1.13169809 -0.0700405  -0.09409459 -0.21321931 -0.13850177 -0.14112633\n",
      " -0.21691075 -0.2329408 ]\n",
      "Last-step D: [ 0.96229004  0.00603233  0.00430794  0.0046376   0.00793858  0.00566376\n",
      "  0.00351953  0.0056102 ] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 1.1351893  -0.07059865 -0.09449381 -0.21364945 -0.13923509 -0.14165084\n",
      " -0.21723725 -0.23346016]\n",
      "Last-step D: [ 0.96508785  0.00558148  0.00399225  0.0043014   0.00733324  0.00524518\n",
      "  0.003265    0.0051936 ] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 1.13843878 -0.07111785 -0.09486573 -0.2140505  -0.13991624 -0.14213919\n",
      " -0.21754172 -0.23394353]\n",
      "Last-step D: [ 0.96750523  0.00519202  0.00371917  0.00401042  0.0068114   0.00488343\n",
      "  0.00304468  0.00483364] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 1.14147734 -0.07160308 -0.0952138  -0.21442611 -0.14055194 -0.14259596\n",
      " -0.21782694 -0.23439549]\n",
      "Last-step D: [ 0.96961438  0.00485232  0.00348066  0.00375612  0.00635708  0.00456772\n",
      "  0.00285212  0.00451959] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 1.1443303  -0.07205843 -0.09554086 -0.21477931 -0.14114775 -0.14302494\n",
      " -0.21809518 -0.23481981]\n",
      "Last-step D: [ 0.97147036  0.00455349  0.00327058  0.00353201  0.00595808  0.00428985\n",
      "  0.0026824   0.00424324] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 1.14701871 -0.07248729 -0.09584927 -0.21511261 -0.14170825 -0.14342929\n",
      " -0.21834834 -0.23521963]\n",
      "Last-step D: [ 0.97311591  0.00428862  0.00308415  0.00333302  0.00560498  0.00404342\n",
      "  0.00253168  0.00399823] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 1.14956024 -0.07289252 -0.09614103 -0.21542813 -0.14223728 -0.14381163\n",
      " -0.21858804 -0.23559759]\n",
      "Last-step D: [ 0.97458468  0.00405229  0.00291761  0.00315515  0.00529036  0.00382342\n",
      "  0.00239695  0.00377954] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 1.15196989 -0.07327653 -0.09641783 -0.21572765 -0.14273812 -0.14417421\n",
      " -0.21881562 -0.2359559 ]\n",
      "Last-step D: [ 0.97590355  0.00384014  0.00276795  0.00299523  0.00500833  0.00362583\n",
      "  0.00227579  0.00358318] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 1.15426047 -0.0736414  -0.0966811  -0.21601272 -0.14321353 -0.14451895\n",
      " -0.21903224 -0.23629649]\n",
      "Last-step D: [ 0.97709421  0.00364869  0.00263274  0.00285067  0.00475411  0.00344741\n",
      "  0.00216627  0.00340591] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 1.15644303 -0.07398891 -0.0969321  -0.21628465 -0.14366591 -0.1448475\n",
      " -0.21923892 -0.236621  ]\n",
      "Last-step D: [ 0.97817438  0.00347506  0.00250999  0.00271936  0.00452382  0.00328551\n",
      "  0.00206677  0.0032451 ] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 1.15852716 -0.0743206  -0.09717191 -0.21654461 -0.14409734 -0.1451613\n",
      " -0.21943652 -0.23693086]\n",
      "Last-step D: [ 0.97915868  0.00331689  0.00239807  0.00259957  0.00431428  0.00313795\n",
      "  0.00197599  0.00309857] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 1.16052124 -0.07463782 -0.09740147 -0.2167936  -0.14450962 -0.14546159\n",
      " -0.2196258  -0.23722731]\n",
      "Last-step D: [ 0.98005925  0.00317223  0.0022956   0.00248985  0.00412281  0.00300292\n",
      "  0.00189283  0.0029645 ] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 1.16243261 -0.07494176 -0.09762161 -0.21703249 -0.14490434 -0.14574948\n",
      " -0.21980744 -0.23751145]\n",
      "Last-step D: [ 0.98088628  0.00303942  0.00220145  0.00238898  0.00394721  0.00287889\n",
      "  0.00181638  0.00284139] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 1.16426777 -0.07523347 -0.09783308 -0.21726209 -0.1452829  -0.14602594\n",
      " -0.21998203 -0.23778424]\n",
      "Last-step D: [ 0.98164838  0.00291709  0.00211464  0.00229594  0.0037856   0.00276457\n",
      "  0.00174584  0.00272794] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 1.16603249 -0.07551388 -0.09803651 -0.21748307 -0.14564654 -0.14629182\n",
      " -0.22015008 -0.23804655]\n",
      "Last-step D: [ 0.98235285  0.00280404  0.00203435  0.00220984  0.00363638  0.00265889\n",
      "  0.00168057  0.00262308] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 1.16773189 -0.0757838  -0.0982325  -0.21769607 -0.14599636 -0.14654791\n",
      " -0.22031208 -0.23829914]\n",
      "Last-step D: [ 0.98300596  0.00269926  0.00195987  0.00212995  0.0034982   0.00256089\n",
      "  0.00162     0.00252587] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 1.16937058 -0.07604399 -0.09842156 -0.21790163 -0.14633335 -0.14679489\n",
      " -0.22046845 -0.23854269]\n",
      "Last-step D: [ 0.9836131   0.00260189  0.00189061  0.00205561  0.00336989  0.00246977\n",
      "  0.00156363  0.0024355 ] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 1.17095269 -0.07629511 -0.09860416 -0.21810025 -0.14665839 -0.14703338\n",
      " -0.22061955 -0.23877782]\n",
      "Last-step D: [ 0.98417893  0.00251117  0.00182602  0.00198627  0.00325043  0.00238484\n",
      "  0.00151104  0.00235128] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 1.17248194 -0.07653775 -0.09878073 -0.2182924  -0.14697229 -0.14726392\n",
      " -0.22076574 -0.23900508]\n",
      "Last-step D: [ 0.98470751  0.00242645  0.00176567  0.00192144  0.00313896  0.00230549\n",
      "  0.00146187  0.00227261] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 1.1739617  -0.07677247 -0.09895164 -0.21847847 -0.14727576 -0.14748704\n",
      " -0.22090732 -0.23922498]\n",
      "Last-step D: [ 0.98520237  0.00234716  0.00170914  0.00186069  0.00303469  0.00223119\n",
      "  0.0014158   0.00219897] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 1.17539504 -0.07699975 -0.09911725 -0.21865883 -0.14756945 -0.14770319\n",
      " -0.22104457 -0.23943796]\n",
      "Last-step D: [ 0.98566662  0.00227279  0.00165608  0.00180365  0.00293697  0.00216148\n",
      "  0.00137253  0.00212987] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 1.17678474 -0.07722004 -0.09927787 -0.21883383 -0.14785397 -0.14791279\n",
      " -0.22117775 -0.23964446]\n",
      "Last-step D: [ 0.98610301  0.00220291  0.00160619  0.00175     0.00284519  0.00209594\n",
      "  0.00133183  0.00206493] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 1.17813334 -0.07743375 -0.09943379 -0.21900377 -0.14812986 -0.14811621\n",
      " -0.2213071  -0.23984483]\n",
      "Last-step D: [ 0.98651395  0.00213712  0.00155919  0.00169943  0.00275885  0.00203421\n",
      "  0.00129347  0.00200378] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 1.17944318 -0.07764126 -0.09958527 -0.21916894 -0.1483976  -0.1483138\n",
      " -0.22143282 -0.24003944]\n",
      "Last-step D: [ 0.9869016   0.00207508  0.00151483  0.0016517   0.00267747  0.00197598\n",
      "  0.00125725  0.00194609] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 1.1807164  -0.07784291 -0.09973256 -0.2193296  -0.14865767 -0.1485059\n",
      " -0.22155513 -0.2402286 ]\n",
      "Last-step D: [ 0.98726787  0.00201647  0.00147291  0.00160657  0.00260064  0.00192095\n",
      "  0.001223    0.00189159] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 1.18195495 -0.07803901 -0.09987589 -0.21948598 -0.14891047 -0.14869279\n",
      " -0.22167418 -0.2404126 ]\n",
      "Last-step D: [ 0.98761447  0.00196103  0.00143323  0.00156383  0.00252799  0.00186886\n",
      "  0.00119057  0.00184002] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 1.18316066 -0.07822986 -0.10001545 -0.21963831 -0.14915639 -0.14887474\n",
      " -0.22179016 -0.24059172]\n",
      "Last-step D: [ 0.98794294  0.0019085   0.00139561  0.0015233   0.0024592   0.0018195\n",
      "  0.00115981  0.00179114] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 1.18433519 -0.07841573 -0.10015144 -0.21978679 -0.14939579 -0.149052\n",
      " -0.22190322 -0.24076619]\n",
      "Last-step D: [ 0.98825465  0.00185866  0.00135989  0.00148481  0.00239397  0.00177265\n",
      "  0.0011306   0.00174476] \n",
      "\n",
      "Final reward weights:  [ 1.18433519 -0.07841573 -0.10015144 -0.21978679 -0.14939579 -0.149052\n",
      " -0.22190322 -0.24076619]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.18433519, -0.07841573, -0.10015144, -0.21978679, -0.14939579,\n",
       "       -0.149052  , -0.22190322, -0.24076619])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_chain(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_chain()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec\n",
    "\n",
    "test_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [-0.01267519  0.00045912 -0.02355767 -0.00719792 -0.01142489 -0.00243801\n",
      "  0.00319497]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.08732481  0.00045912 -0.02355767 -0.00719792 -0.0435295  -0.03579856\n",
      " -0.03133988]\n",
      "Last-step D: [  2.60226368e-11   2.60998043e-11   0.00000000e+00   2.66676670e-11\n",
      "   3.21046053e-01   3.33605459e-01   3.45348488e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.18732481  0.00045912 -0.02355767 -0.00719792 -0.07583487 -0.06916126\n",
      " -0.0656718 ]\n",
      "Last-step D: [  4.89261026e-10   4.00211111e-10   0.00000000e+00   4.06940694e-10\n",
      "   3.23053700e-01   3.33627046e-01   3.43319252e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.28732481  0.00045912 -0.02355767 -0.00719792 -0.10830804 -0.10252512\n",
      " -0.09983477]\n",
      "Last-step D: [  9.19710451e-09   6.02688626e-09   0.00000000e+00   6.10349726e-09\n",
      "   3.24731729e-01   3.33638571e-01   3.41629679e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.38732479  0.00045911 -0.02355767 -0.00719793 -0.14092142 -0.13588943\n",
      " -0.13385704]\n",
      "Last-step D: [  1.72864699e-07   8.85653028e-08   0.00000000e+00   8.93891549e-08\n",
      "   3.26133821e-01   3.33643111e-01   3.40222717e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [  4.87324466e-01   4.58981738e-04  -2.35576728e-02  -7.19806016e-03\n",
      "  -1.73651802e-01  -1.69253559e-01  -1.67761959e-01]\n",
      "Last-step D: [  3.24873215e-06   1.25660746e-06   0.00000000e+00   1.26473536e-06\n",
      "   3.27303801e-01   3.33641282e-01   3.39049147e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [  5.87318363e-01   4.57293453e-04  -2.35576728e-02  -7.19975539e-03\n",
      "  -2.06477288e-01  -2.02614399e-01  -2.01566146e-01]\n",
      "Last-step D: [  6.10286033e-05   1.68828461e-05   0.00000000e+00   1.69522579e-05\n",
      "   3.28254864e-01   3.33608402e-01   3.38041870e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [  6.87204357e-01   4.36752142e-04  -2.35576728e-02  -7.22034090e-03\n",
      "  -2.39337477e-01  -2.35926126e-01  -2.35239097e-01]\n",
      "Last-step D: [  1.14005956e-03   2.05413109e-04   0.00000000e+00   2.05855084e-04\n",
      "   3.28601887e-01   3.33117273e-01   3.36729512e-01] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [  7.85210646e-01   2.42635383e-04  -2.35576728e-02  -7.41455994e-03\n",
      "  -2.71535520e-01  -2.68494251e-01  -2.68090882e-01]\n",
      "Last-step D: [ 0.01993711  0.00194117  0.          0.00194219  0.32198043  0.32568125\n",
      "  0.32851785] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [  8.63662521e-01  -5.36187109e-04  -2.35576728e-02  -8.19277642e-03\n",
      "  -2.96949837e-01  -2.94148025e-01  -2.93917628e-01]\n",
      "Last-step D: [ 0.21548125  0.00778822  0.          0.00778216  0.25414316  0.25653773\n",
      "  0.25826746] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.90037026 -0.00120157 -0.02355767 -0.00885709 -0.30865797 -0.30595069\n",
      " -0.30578487]\n",
      "Last-step D: [ 0.63292258  0.00665382  0.          0.00664317  0.11708137  0.11802667\n",
      "  0.11867238] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.92050265 -0.0016121  -0.02355767 -0.00926684 -0.31505214 -0.31239345\n",
      " -0.31226004]\n",
      "Last-step D: [ 0.79867613  0.00410536  0.          0.00409749  0.06394162  0.06442763\n",
      "  0.06475177] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 0.93420995 -0.00190251 -0.02355767 -0.00955664 -0.31939943 -0.3167728\n",
      " -0.31666051]\n",
      "Last-step D: [ 0.86292696  0.00290402  0.          0.00289799  0.04347293  0.04379347\n",
      "  0.04400462] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.94457104 -0.00212614 -0.02355767 -0.00977979 -0.3226832  -0.32008031\n",
      " -0.31998352]\n",
      "Last-step D: [ 0.89638911  0.00223634  0.          0.00223146  0.03283776  0.03307513\n",
      "  0.03323019] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.95288645 -0.00230761 -0.02355767 -0.00996084 -0.3253176  -0.32273347\n",
      " -0.32264886]\n",
      "Last-step D: [ 0.91684593  0.00181464  0.          0.00181054  0.02634391  0.02653157\n",
      "  0.0266534 ] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.9598239  -0.0024601  -0.02355767 -0.01011298 -0.32751489 -0.32494624\n",
      " -0.32487162]\n",
      "Last-step D: [ 0.93062551  0.00152495  0.          0.00152141  0.02197291  0.02212763\n",
      "  0.02222759] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.96577095 -0.0025915  -0.02355767 -0.01024407 -0.32939817 -0.32684265\n",
      " -0.32677649]\n",
      "Last-step D: [ 0.94052945  0.00131398  0.          0.00131086  0.01883283  0.01896418\n",
      "  0.01904869] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.97097231 -0.00270686 -0.02355767 -0.01035915 -0.33104511 -0.32850099\n",
      " -0.32844213]\n",
      "Last-step D: [ 0.94798645  0.00115363  0.          0.00115084  0.01646938  0.01658332\n",
      "  0.01665637] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.97559223 -0.00280963 -0.02355767 -0.01046167 -0.33250781 -0.32997374\n",
      " -0.32992131]\n",
      "Last-step D: [ 0.95380078  0.00102771  0.          0.00102519  0.01462704  0.01472753\n",
      "  0.01479175] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.97974627 -0.00290226 -0.02355767 -0.01055407 -0.33382293 -0.33129783\n",
      " -0.33125112]\n",
      "Last-step D: [  9.58459565e-01   9.26256202e-04   0.00000000e+00   9.23955939e-04\n",
      "   1.31511413e-02   1.32409262e-02   1.32981552e-02] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 0.98351878 -0.00298654 -0.02355767 -0.01063814 -0.33501719 -0.3325002\n",
      " -0.33245865]\n",
      "Last-step D: [  9.62274930e-01   8.42802356e-04   0.00000000e+00   8.40684859e-04\n",
      "   1.19426217e-02   1.20237035e-02   1.20752579e-02] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 0.98697317 -0.00306383 -0.02355767 -0.01071524 -0.3361107  -0.3336011\n",
      " -0.33356423]\n",
      "Last-step D: [  9.65456065e-01   7.72968286e-04   0.00000000e+00   7.71005990e-04\n",
      "   1.09351200e-02   1.10089889e-02   1.10558521e-02] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 0.99015834 -0.0031352  -0.02355767 -0.01078642 -0.33711895 -0.33461613\n",
      " -0.33458356]\n",
      "Last-step D: [  9.68148379e-01   7.13686041e-04   0.00000000e+00   7.11857222e-04\n",
      "   1.00825200e-02   1.01503174e-02   1.01932405e-02] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 0.99311273 -0.00320148 -0.02355767 -0.01085253 -0.33805413 -0.33555757\n",
      " -0.33552895]\n",
      "Last-step D: [  9.70456066e-01   6.62742622e-04   0.00000000e+00   6.61029853e-04\n",
      "   9.35178488e-03   9.41440363e-03   9.45397310e-03] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 0.99586716 -0.00326333 -0.02355767 -0.01091422 -0.338926   -0.33643525\n",
      " -0.3364103 ]\n",
      "Last-step D: [  9.72455726e-01   6.18501999e-04   0.00000000e+00   6.16891084e-04\n",
      "   8.71863187e-03   8.77678350e-03   8.81346552e-03] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 0.99844666 -0.0033213  -0.02355767 -0.01097204 -0.33974248 -0.33725716\n",
      " -0.33723562]\n",
      "Last-step D: [  9.74204924e-01   5.79728857e-04   0.00000000e+00   5.78208073e-04\n",
      "   8.16481577e-03   8.21907607e-03   8.25324721e-03] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 1.00087189 -0.00337585 -0.02355767 -0.01102644 -0.34051011 -0.33802988\n",
      " -0.33801154]\n",
      "Last-step D: [  9.75747750e-01   5.45473161e-04   0.00000000e+00   5.44032717e-04\n",
      "   7.67636426e-03   7.72720558e-03   7.75917425e-03] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 1.00316004 -0.00342735 -0.02355767 -0.0110778  -0.34123435 -0.3387589\n",
      " -0.33874357]\n",
      "Last-step D: [  9.77118540e-01   5.14992423e-04   0.00000000e+00   5.13624052e-04\n",
      "   7.24239757e-03   7.29021210e-03   7.32023394e-03] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 1.00532559 -0.00347612 -0.02355767 -0.01112644 -0.34191979 -0.33944884\n",
      " -0.33943634]\n",
      "Last-step D: [  9.78344428e-01   4.87698034e-04   0.00000000e+00   4.86394694e-04\n",
      "   6.85431872e-03   6.89943554e-03   6.92772472e-03] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 1.00738088 -0.00352243 -0.02355767 -0.01117263 -0.34257031 -0.34010364\n",
      " -0.34009381]\n",
      "Last-step D: [  9.79447140e-01   4.63117426e-04   0.00000000e+00   4.61873067e-04\n",
      "   6.50524529e-03   6.54794316e-03   6.57468084e-03] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 1.00933645 -0.00356652 -0.02355767 -0.0112166  -0.34318927 -0.34072665\n",
      " -0.34071935]\n",
      "Last-step D: [  9.80444274e-01   4.40866859e-04   0.00000000e+00   4.39676244e-04\n",
      "   6.18960282e-03   6.23011990e-03   6.25546062e-03] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 1.01120143 -0.00360858 -0.02355767 -0.01125855 -0.34377955 -0.34132079\n",
      " -0.3413159 ]\n",
      "Last-step D: [  9.81350234e-01   4.20631514e-04   0.00000000e+00   4.19490082e-04\n",
      "   5.90282852e-03   5.94136983e-03   5.96544648e-03] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 1.01298374 -0.00364879 -0.02355767 -0.01129865 -0.34434367 -0.34188858\n",
      " -0.34188598]\n",
      "Last-step D: [  9.82176925e-01   4.02150710e-04   0.00000000e+00   4.01054459e-04\n",
      "   5.64115207e-03   5.67789532e-03   5.70082295e-03] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 1.01469031 -0.00368731 -0.02355767 -0.01133707 -0.34488381 -0.34243223\n",
      " -0.34243182]\n",
      "Last-step D: [  9.82934269e-01   3.85206762e-04   0.00000000e+00   3.84152166e-04\n",
      "   5.40143087e-03   5.43653111e-03   5.45840996e-03] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 1.01632725 -0.00372428 -0.02355767 -0.01137393 -0.34540191 -0.34295369\n",
      " -0.34295537]\n",
      "Last-step D: [  9.83630604e-01   3.69616481e-04   0.00000000e+00   3.68600412e-04\n",
      "   5.18102486e-03   5.21461814e-03   5.23553607e-03] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 1.01789995 -0.0037598  -0.02355767 -0.01140935 -0.34589968 -0.34345468\n",
      " -0.34345837]\n",
      "Last-step D: [  9.84272984e-01   3.55224608e-04   0.00000000e+00   3.54244282e-04\n",
      "   4.97770001e-03   5.00990632e-03   5.02994077e-03] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 1.01941321 -0.00379399 -0.02355767 -0.01144345 -0.34637864 -0.34393673\n",
      " -0.34394234]\n",
      "Last-step D: [  9.84867419e-01   3.41898699e-04   0.00000000e+00   3.40951627e-04\n",
      "   4.78955327e-03   4.82047900e-03   4.83969852e-03] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 1.0208713  -0.00382694 -0.02355767 -0.01147631 -0.34684013 -0.3444012\n",
      " -0.34440865]\n",
      "Last-step D: [  9.85419060e-01   3.29525087e-04   0.00000000e+00   3.28609031e-04\n",
      "   4.61495353e-03   4.64469342e-03   4.66315900e-03] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 1.02227807 -0.00385874 -0.02355767 -0.01150802 -0.34728538 -0.34484931\n",
      " -0.34485854]\n",
      "Last-step D: [  9.85932348e-01   3.18005670e-04   0.00000000e+00   3.17118614e-04\n",
      "   4.45249481e-03   4.48113357e-03   4.49889971e-03] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 1.02363695 -0.00388947 -0.02355767 -0.01153866 -0.34771548 -0.34528217\n",
      " -0.34529311]\n",
      "Last-step D: [  9.86411130e-01   3.07255344e-04   0.00000000e+00   3.06395464e-04\n",
      "   4.30095874e-03   4.32857242e-03   4.34568801e-03] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 1.02495108 -0.00391919 -0.02355767 -0.01156829 -0.34813141 -0.34570076\n",
      " -0.34571336]\n",
      "Last-step D: [  9.86858758e-01   2.97199919e-04   0.00000000e+00   2.96365558e-04\n",
      "   4.15928434e-03   4.18594147e-03   4.20245050e-03] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 1.02622326 -0.00394796 -0.02355767 -0.01159699 -0.34853406 -0.34610599\n",
      " -0.34612018]\n",
      "Last-step D: [  9.87278164e-01   2.87774427e-04   0.00000000e+00   2.86964078e-04\n",
      "   4.02654342e-03   4.05230596e-03   4.06824818e-03] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 1.02745607 -0.00397586 -0.02355767 -0.0116248  -0.34892425 -0.34649868\n",
      " -0.34651441]\n",
      "Last-step D: [  9.87671923e-01   2.78921735e-04   0.00000000e+00   2.78134021e-04\n",
      "   3.90192046e-03   3.92684460e-03   3.94225604e-03] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 1.02865184 -0.00400292 -0.02355767 -0.01165179 -0.34930272 -0.34687956\n",
      " -0.34689678]\n",
      "Last-step D: [  9.88042308e-01   2.70591400e-04   0.00000000e+00   2.69825061e-04\n",
      "   3.78469606e-03   3.80883293e-03   3.82374633e-03] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 1.02981271 -0.00402919 -0.02355767 -0.01167799 -0.34967015 -0.34724933\n",
      " -0.34726799]\n",
      "Last-step D: [  9.88391331e-01   2.62738719e-04   0.00000000e+00   2.61992597e-04\n",
      "   3.67423319e-03   3.69762942e-03   3.71207463e-03] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 1.03094063 -0.00405472 -0.02355767 -0.01170345 -0.35002714 -0.34760859\n",
      " -0.34762866]\n",
      "Last-step D: [  9.88720781e-01   2.55323937e-04   0.00000000e+00   2.54596967e-04\n",
      "   3.56996574e-03   3.59266399e-03   3.60666830e-03] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45\n",
      "Reward vector: [ 1.0320374  -0.00407955 -0.02355767 -0.01172821 -0.35037428 -0.34795793\n",
      " -0.34797936]\n",
      "Last-step D: [  9.89032252e-01   2.48311579e-04   0.00000000e+00   2.47602779e-04\n",
      "   3.47138895e-03   3.49342834e-03   3.50701675e-03] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 1.03310469 -0.00410372 -0.02355767 -0.0117523  -0.35071209 -0.34829788\n",
      " -0.34832062]\n",
      "Last-step D: [  9.89327169e-01   2.41669895e-04   0.00000000e+00   2.40978356e-04\n",
      "   3.37805131e-03   3.39946780e-03   3.41266328e-03] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 1.034144   -0.00412726 -0.02355767 -0.01177577 -0.35104104 -0.34862892\n",
      " -0.34865294]\n",
      "Last-step D: [  9.89606814e-01   2.35370379e-04   0.00000000e+00   2.34695260e-04\n",
      "   3.28954773e-03   3.31037445e-03   3.32319814e-03] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 1.03515677 -0.0041502  -0.02355767 -0.01179865 -0.35136159 -0.3489515\n",
      " -0.34897677]\n",
      "Last-step D: [  9.89872337e-01   2.29387368e-04   0.00000000e+00   2.28727889e-04\n",
      "   3.20551376e-03   3.22578129e-03   3.23825269e-03] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 1.03614429 -0.00417257 -0.02355767 -0.01182095 -0.35167416 -0.34926603\n",
      " -0.34929252]\n",
      "Last-step D: [  9.90124777e-01   2.23697700e-04   0.00000000e+00   2.23053135e-04\n",
      "   3.12562058e-03   3.14535721e-03   3.15749437e-03] \n",
      "\n",
      "Initial reward vector: [ 0.00393675  0.01193374  0.00140278 -0.00836068  0.0078268   0.01042156\n",
      "  0.0050601 ]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.00393675  0.01193374  0.10140278 -0.00836068 -0.02569195 -0.02299987\n",
      " -0.02799972]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   2.76984030e-11   0.00000000e+00\n",
      "   3.35187532e-01   3.34214346e-01   3.30598122e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.00393675  0.01193374  0.20140278 -0.00836068 -0.05917409 -0.05641848\n",
      " -0.06109898]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   5.20469798e-10   0.00000000e+00\n",
      "   3.34821324e-01   3.34186035e-01   3.30992641e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.00393675  0.01193374  0.30140278 -0.00836068 -0.09262587 -0.08983431\n",
      " -0.09423137]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   9.77987813e-09   0.00000000e+00\n",
      "   3.34517793e-01   3.34158330e-01   3.31323867e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.00393675  0.01193374  0.40140276 -0.00836068 -0.12605251 -0.12324743\n",
      " -0.12739158]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   1.83767593e-07   0.00000000e+00\n",
      "   3.34266449e-01   3.34131210e-01   3.31602157e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.00393675  0.01193374  0.50140242 -0.00836068 -0.1594583  -0.15665773\n",
      " -0.16057514]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   3.45298591e-06   0.00000000e+00\n",
      "   3.34057933e-01   3.34103031e-01   3.31835584e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.00393675  0.01193374  0.60139593 -0.00836068 -0.19284543 -0.19006233\n",
      " -0.19377693]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   6.48594718e-05   0.00000000e+00\n",
      "   3.33871236e-01   3.34046018e-01   3.32017886e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.00393675  0.01193374  0.70127474 -0.00836068 -0.22618669 -0.22341293\n",
      " -0.22696389]\n",
      "Last-step D: [ 0.          0.          0.0012119   0.          0.33341259  0.33350592\n",
      "  0.33186959] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.00393675  0.01193374  0.79915356 -0.00836068 -0.25890921 -0.25597236\n",
      " -0.25956075]\n",
      "Last-step D: [ 0.          0.          0.02121181  0.          0.32722525  0.32559435\n",
      "  0.32596859] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.00393675  0.01193374  0.87642022 -0.00836068 -0.2849423  -0.28125499\n",
      " -0.28551169]\n",
      "Last-step D: [ 0.          0.          0.22733337  0.          0.26033088  0.25282633\n",
      "  0.25950942] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.00393675  0.01193374  0.91163001 -0.00836068 -0.29689525 -0.29259125\n",
      " -0.29743227]\n",
      "Last-step D: [ 0.          0.          0.64790211  0.          0.11952949  0.1133626\n",
      "  0.1192058 ] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.00393675  0.01193374  0.9309783  -0.00836068 -0.30347563 -0.2987955\n",
      " -0.30399593]\n",
      "Last-step D: [ 0.          0.          0.80651709  0.          0.06580386  0.06204243\n",
      "  0.06563662] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 0.00393675  0.01193374  0.9441861  -0.00836068 -0.30797025 -0.30302511\n",
      " -0.3084795 ]\n",
      "Last-step D: [ 0.          0.          0.86792208  0.          0.04494614  0.04229612\n",
      "  0.04483566] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.00393675  0.01193374  0.95418684 -0.00836068 -0.3113744  -0.30622573\n",
      " -0.31187547]\n",
      "Last-step D: [ 0.          0.          0.8999926   0.          0.03404152  0.03200617\n",
      "  0.03395971] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.00393675  0.01193374  0.96222262 -0.00836068 -0.31411009 -0.3087966\n",
      " -0.31460469]\n",
      "Last-step D: [ 0.          0.          0.9196422   0.          0.02735686  0.02570873\n",
      "  0.02729221] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.00393675  0.01193374  0.96893262 -0.00836068 -0.31639462 -0.31094287\n",
      " -0.31688389]\n",
      "Last-step D: [ 0.          0.          0.93289992  0.          0.02284532  0.02146271\n",
      "  0.02279205] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.00393675  0.01193374  0.97468854 -0.00836068 -0.31835442 -0.31278372\n",
      " -0.31883917]\n",
      "Last-step D: [ 0.          0.          0.9424408   0.          0.01959798  0.01840845\n",
      "  0.01955278] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.00393675  0.01193374  0.97972539 -0.00836068 -0.32006944 -0.31439444\n",
      " -0.32055027]\n",
      "Last-step D: [ 0.          0.          0.94963156  0.          0.01715019  0.01610723\n",
      "  0.01711101] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.00393675  0.01193374  0.98420111 -0.00836068 -0.32159343 -0.31582563\n",
      " -0.32207081]\n",
      "Last-step D: [ 0.          0.          0.95524281  0.          0.0152399   0.01431191\n",
      "  0.01520538] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.00393675  0.01193374  0.98822692 -0.00836068 -0.32296424 -0.3171129\n",
      " -0.32343854]\n",
      "Last-step D: [ 0.          0.          0.95974184  0.          0.01370815  0.01287269\n",
      "  0.01367732] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 0.00393675  0.01193374  0.99188408 -0.00836068 -0.32420953 -0.31828225\n",
      " -0.32468105]\n",
      "Last-step D: [ 0.          0.          0.96342843  0.          0.01245293  0.01169352\n",
      "  0.01242511] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 0.00393675  0.01193374  0.99523371 -0.00836068 -0.32535012 -0.31935325\n",
      " -0.32581911]\n",
      "Last-step D: [ 0.          0.          0.96650365  0.          0.01140583  0.01071\n",
      "  0.01138051] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 0.00393675  0.01193374  0.99832297 -0.00836068 -0.32640204 -0.32034099\n",
      " -0.32686871]\n",
      "Last-step D: [ 0.          0.          0.9691074   0.          0.01051924  0.00987734\n",
      "  0.01049601] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 0.00393675  0.01193374  1.00118898 -0.00836068 -0.32737794 -0.32125733\n",
      " -0.32784246]\n",
      "Last-step D: [ 0.          0.          0.97133998  0.          0.00975902  0.00916343\n",
      "  0.00973758] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 0.00393675  0.01193374  1.00386146 -0.00836068 -0.32828795 -0.32211179\n",
      " -0.32875048]\n",
      "Last-step D: [ 0.          0.          0.97327516  0.          0.00910005  0.00854464\n",
      "  0.00908015] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 0.00393675  0.01193374  1.00636462 -0.00836068 -0.32914029 -0.32291212\n",
      " -0.32960097]\n",
      "Last-step D: [ 0.          0.          0.97496844  0.          0.00852344  0.00800323\n",
      "  0.00850488] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 0.00393675  0.01193374  1.00871838 -0.00836068 -0.32994177 -0.32366468\n",
      " -0.3304007 ]\n",
      "Last-step D: [ 0.          0.          0.97646232  0.          0.00801473  0.0075256\n",
      "  0.00799735] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 0.00393675  0.01193374  1.01093939 -0.00836068 -0.33069803 -0.32437479\n",
      " -0.33115533]\n",
      "Last-step D: [ 0.          0.          0.97778992  0.          0.00756264  0.00710114\n",
      "  0.0075463 ] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 0.00393675  0.01193374  1.01304165 -0.00836068 -0.33141385 -0.32504694\n",
      " -0.33186962]\n",
      "Last-step D: [ 0.          0.          0.97897743  0.          0.00715825  0.00672148\n",
      "  0.00714284] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 0.00393675  0.01193374  1.01503707 -0.00836068 -0.3320933  -0.32568493\n",
      " -0.3325476 ]\n",
      "Last-step D: [ 0.          0.          0.98004583  0.          0.00679442  0.00637991\n",
      "  0.00677985] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 0.00393675  0.01193374  1.01693586 -0.00836068 -0.33273983 -0.32629203\n",
      " -0.33319276]\n",
      "Last-step D: [ 0.          0.          0.98101209  0.          0.00646537  0.006071\n",
      "  0.00645154] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 0.00393675  0.01193374  1.01874684 -0.00836068 -0.33335647 -0.32687106\n",
      " -0.33380808]\n",
      "Last-step D: [ 0.          0.          0.98189014  0.          0.00616635  0.00579029\n",
      "  0.00615321] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02047769 -0.00836068 -0.33394581 -0.32742447\n",
      " -0.33439617]\n",
      "Last-step D: [ 0.          0.          0.98269149  0.          0.00589346  0.00553411\n",
      "  0.00588094] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02213512 -0.00836068 -0.33451016 -0.32795441\n",
      " -0.33495932]\n",
      "Last-step D: [ 0.          0.          0.98342571  0.          0.00564343  0.00529939\n",
      "  0.00563147] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02372504 -0.00836068 -0.33505151 -0.32846276\n",
      " -0.33549952]\n",
      "Last-step D: [ 0.          0.          0.98410087  0.          0.00541351  0.00508355\n",
      "  0.00540207] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02525266 -0.00836068 -0.33557165 -0.3289512\n",
      " -0.33601857]\n",
      "Last-step D: [ 0.          0.          0.98472379  0.          0.00520138  0.00488442\n",
      "  0.00519042] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02672263 -0.00836068 -0.33607215 -0.32942122\n",
      " -0.33651802]\n",
      "Last-step D: [ 0.          0.          0.98530028  0.          0.00500506  0.00470013\n",
      "  0.00499454] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 0.00393675  0.01193374  1.0281391  -0.00836068 -0.33655444 -0.32987413\n",
      " -0.33699929]\n",
      "Last-step D: [ 0.          0.          0.98583532  0.          0.00482286  0.00452908\n",
      "  0.00481274] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 0.00393675  0.01193374  1.02950578 -0.00836068 -0.33701977 -0.33031112\n",
      " -0.33746365]\n",
      "Last-step D: [ 0.          0.          0.98633321  0.          0.0046533   0.00436992\n",
      "  0.00464357] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03082601 -0.00836068 -0.33746928 -0.33073326\n",
      " -0.33791222]\n",
      "Last-step D: [ 0.          0.          0.98679767  0.          0.00449514  0.00422144\n",
      "  0.00448575] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03210281 -0.00836068 -0.33790401 -0.33114152\n",
      " -0.33834604]\n",
      "Last-step D: [ 0.          0.          0.98723195  0.          0.00434725  0.00408262\n",
      "  0.00433819] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03333893 -0.00836068 -0.33832487 -0.33153678\n",
      " -0.33876604]\n",
      "Last-step D: [ 0.          0.          0.98763888  0.          0.00420867  0.00395253\n",
      "  0.00419992] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03453683 -0.00836068 -0.33873273 -0.33191982\n",
      " -0.33917305]\n",
      "Last-step D: [ 0.          0.          0.98802095  0.          0.00407856  0.00383039\n",
      "  0.0040701 ] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03569879 -0.00836068 -0.33912835 -0.33229136\n",
      " -0.33956784]\n",
      "Last-step D: [ 0.          0.          0.98838037  0.          0.00395616  0.00371549\n",
      "  0.00394797] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03682689 -0.00836068 -0.33951243 -0.33265209\n",
      " -0.33995113]\n",
      "Last-step D: [ 0.          0.          0.98871909  0.          0.00384082  0.00360721\n",
      "  0.00383288] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 0.00393675  0.01193374  1.037923   -0.00836068 -0.33988562 -0.33300259\n",
      " -0.34032355]\n",
      "Last-step D: [ 0.          0.          0.98903883  0.          0.00373194  0.003505\n",
      "  0.00372423] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 0.00393675  0.01193374  1.03898889 -0.00836068 -0.34024852 -0.33334342\n",
      " -0.3406857 ]\n",
      "Last-step D: [ 0.          0.          0.98934114  0.          0.00362899  0.00340836\n",
      "  0.00362151] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 0.00393675  0.01193374  1.04002615 -0.00836068 -0.34060167 -0.33367511\n",
      " -0.34103813]\n",
      "Last-step D: [ 0.          0.          0.9896274   0.          0.00353151  0.00331685\n",
      "  0.00352425] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 0.00393675  0.01193374  1.04103626 -0.00836068 -0.34094558 -0.33399811\n",
      " -0.34138133]\n",
      "Last-step D: [ 0.          0.          0.98989885  0.          0.00343907  0.00323007\n",
      "  0.00343201] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 0.00393675  0.01193374  1.0420206  -0.00836068 -0.34128071 -0.33431288\n",
      " -0.34171577]\n",
      "Last-step D: [ 0.          0.          0.99015661  0.          0.00335129  0.00314767\n",
      "  0.00334443] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 0.00393675  0.01193374  1.04298044 -0.00836068 -0.34160749 -0.33461982\n",
      " -0.34204189]\n",
      "Last-step D: [ 0.          0.          0.99040167  0.          0.00326784  0.00306933\n",
      "  0.00326115] \n",
      "\n",
      "Final reward weights, at s_0:  [ 1.03614429 -0.00417257 -0.02355767 -0.01182095 -0.35167416 -0.34926603\n",
      " -0.34929252]\n",
      "Final reward weights, at s_2:  [ 0.00393675  0.01193374  1.04298044 -0.00836068 -0.34160749 -0.33461982\n",
      " -0.34204189]\n"
     ]
    }
   ],
   "source": [
    "def test_water(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_water()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec_1 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[2]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[2]=1\n",
    "    \n",
    "    r_vec_2 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    print('Final reward weights, at s_0: ', r_vec_1)\n",
    "    print('Final reward weights, at s_2: ', r_vec_2)\n",
    "\n",
    "test_water()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
