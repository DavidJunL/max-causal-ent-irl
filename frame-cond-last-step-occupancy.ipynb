{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import FrozenLakeEnv\n",
    "from mdps import MDP, MDPOneTimeR, MDP_toy_irreversibility\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  0.   0.5]\n",
      "[ 0.25  0.    0.25  0.5 ]\n",
      "[ 0.     0.25   0.125  0.625]\n",
      "[ 0.125   0.0625  0.1875  0.625 ]\n",
      "[ 0.03125  0.15625  0.125    0.6875 ]\n",
      "[ 0.078125  0.078125  0.140625  0.703125]\n",
      "[ 0.0390625  0.109375   0.109375   0.7421875]\n",
      "[ 0.0546875   0.07421875  0.109375    0.76171875]\n",
      "[ 0.03710938  0.08203125  0.09179688  0.7890625 ]\n",
      "[ 0.04101562  0.06445312  0.08691406  0.80761719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04101562,  0.06445312,  0.08691406,  0.80761719])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        D = np.zeros_like(P_0)\n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:\n",
    "                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "\n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "mdp = MDP_toy_irreversibility(FrozenLakeEnv(is_slippery=False))    \n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vector=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vector is None:\n",
    "        r_vector = np.random.rand(mdp.nS)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vector, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vector = -(current_state - D)\n",
    "\n",
    "            # Gradient descent\n",
    "            r_vector = r_vector - learning_rate * dL_dr_vector\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vector))\n",
    "            print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(temperature_irl=1,\n",
    "         horizon=43, #number of timesteps we assume the expert has been acting previously\n",
    "         learning_rate=0.1,\n",
    "         epochs = 40):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    mdp = MDP_toy_irreversibility(FrozenLakeEnv(is_slippery=False))    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vector = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vector)\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Reward vector: [ 0.64519534  0.7053788   0.58886242  0.47221287]\n",
      "Policy: [[ 0.80847953  0.19152047]\n",
      " [ 0.64054801  0.35945199]\n",
      " [ 0.52257979  0.47742021]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.03618164  0.09810568  0.1390096   0.72670308] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.73457     0.67417213  0.54498524  0.45792205]\n",
      "Policy: [[ 0.97533973  0.02466027]\n",
      " [ 0.66055696  0.33944304]\n",
      " [ 0.5271372   0.4728628 ]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.10625337  0.31206667  0.43877174  0.14290822] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.8230329   0.64368936  0.50547685  0.43945032]\n",
      "Policy: [[ 0.97078955  0.02921045]\n",
      " [ 0.62308068  0.37691932]\n",
      " [ 0.51513048  0.48486952]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.11537107  0.30482772  0.39508389  0.18471731] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.91004588  0.61230524  0.46797836  0.42131995]\n",
      "Policy: [[ 0.97403146  0.02596854]\n",
      " [ 0.58788111  0.41211889]\n",
      " [ 0.50386802  0.49613198]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.12987017  0.31384122  0.37498494  0.18130368] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.99535476  0.57955232  0.43200142  0.40474093]\n",
      "Policy: [[ 0.97848974  0.02151026]\n",
      " [ 0.55310666  0.44689334]\n",
      " [ 0.49270445  0.50729555]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.14691121  0.32752917  0.35976937  0.16579026] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 1.07884333  0.54536121  0.39758211  0.38986279]\n",
      "Policy: [[ 0.98242661  0.01757339]\n",
      " [ 0.51858773  0.48141227]\n",
      " [ 0.48153661  0.51846339]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.16511432  0.34191111  0.34419315  0.14878142] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 1.16045034  0.50974915  0.36481327  0.37663668]\n",
      "Policy: [[ 0.9856868   0.0143132 ]\n",
      " [ 0.48470608  0.51529392]\n",
      " [ 0.47044917  0.52955083]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.1839299   0.35612064  0.32768839  0.13226107] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 1.24015795  0.47276538  0.33377011  0.36495599]\n",
      "Policy: [[ 0.98833753  0.01166247]\n",
      " [ 0.45187206  0.54812794]\n",
      " [ 0.45955339  0.54044661]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.20292387  0.36983768  0.31043155  0.1168069 ] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 1.31799043  0.4344744   0.30449663  0.35468798]\n",
      "Policy: [[ 0.99047378  0.00952622]\n",
      " [ 0.42042733  0.57957267]\n",
      " [ 0.44895702  0.55104298]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.22167524  0.38290982  0.29273481  0.10268013] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 1.39401261  0.39494425  0.27700456  0.34568801]\n",
      "Policy: [[ 0.99218668  0.00781332]\n",
      " [ 0.39062077  0.60937923]\n",
      " [ 0.43875534  0.56124466]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.2397782   0.39530143  0.27492072  0.08999965] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 1.46832685  0.35423646  0.25127682  0.33780929]\n",
      "Policy: [[ 0.99355649  0.00644351]\n",
      " [ 0.36260603  0.63739397]\n",
      " [ 0.42902764  0.57097236]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.25685759  0.40707789  0.25727736  0.07878716] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 1.54106845  0.31239817  0.2272729   0.33090991]\n",
      "Policy: [[ 0.9946511   0.0053489 ]\n",
      " [ 0.33645078  0.66354922]\n",
      " [ 0.4198363   0.5801637 ]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.27258391  0.41838299  0.24003928  0.06899382] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 1.61239991  0.26945737  0.20493458  0.32485757]\n",
      "Policy: [[ 0.99552653  0.00447347]\n",
      " [ 0.31215208  0.68784792]\n",
      " [ 0.41122753  0.58877247]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.28668545  0.42940794  0.22338318  0.06052344] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 1.68250423  0.22542153  0.18419135  0.31953232]\n",
      "Policy: [[ 0.99622811  0.00377189]\n",
      " [ 0.28965415  0.71034585]\n",
      " [ 0.40323269  0.59676731]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.29895683  0.44035837  0.20743229  0.05325251] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 1.75157794  0.18027891  0.16496485  0.31482773]\n",
      "Policy: [[ 0.99679207  0.00320793]\n",
      " [ 0.26886529  0.73113471]\n",
      " [ 0.39586996  0.60413004]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.30926289  0.45142624  0.19226498  0.04704589] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 1.81982422  0.13400186  0.14717243  0.31065092]\n",
      "Policy: [[ 0.99724716  0.00275284]\n",
      " [ 0.24967244  0.75032756]\n",
      " [ 0.38914597  0.61085403]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.31753722  0.46277048  0.17792423  0.04176807] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 1.88744664  0.08655111  0.1307298   0.30692187]\n",
      "Policy: [[ 0.99761601  0.00238399]\n",
      " [ 0.23195265  0.76804735]\n",
      " [ 0.38305745  0.61694255]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32377573  0.47450748  0.16442629  0.03729049] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 1.95464398  0.03788015  0.11555301  0.30357229]\n",
      "Policy: [[ 0.99791642  0.00208358]\n",
      " [ 0.21558148  0.78441852]\n",
      " [ 0.37759288  0.62240712]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32802667  0.48670968  0.15176788  0.03349577] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 2.02160609 -0.01206083  0.10155983  0.30054433]\n",
      "Policy: [[ 0.99816239  0.00183761]\n",
      " [ 0.20043866  0.79956134]\n",
      " [ 0.37273411  0.62726589]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.33037882  0.49940979  0.13993179  0.0302796 ] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 2.08851113 -0.06332163  0.08867071  0.29778922]\n",
      "Policy: [[ 0.99836488  0.00163512]\n",
      " [ 0.18641156  0.81358844]\n",
      " [ 0.36845781  0.63154219]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.33094961  0.512608    0.12889128  0.02755111] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 2.15552369 -0.11594963  0.07680937  0.29526599]\n",
      "Policy: [[ 0.99853253  0.00146747]\n",
      " [ 0.17339706  0.82660294]\n",
      " [ 0.36473691  0.63526309]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32987441  0.52627992  0.11861334  0.02523233] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 2.22279392 -0.16998803  0.06590325  0.29294029]\n",
      "Policy: [[ 0.99867215  0.00132785]\n",
      " [ 0.16130224  0.83869776]\n",
      " [ 0.36154173  0.63845827]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32729771  0.54038407  0.1090612   0.02325702] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 2.29045728 -0.22547484  0.05588364  0.29078335]\n",
      "Policy: [[ 0.9987891   0.0012109 ]\n",
      " [ 0.15004432  0.84995568]\n",
      " [ 0.35884102  0.64115898]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32336643  0.5548681   0.10019612  0.02156935] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 2.35863476 -0.2824422   0.04668577  0.28877111]\n",
      "Policy: [[ 0.99888767  0.00111233]\n",
      " [ 0.13955011  0.86044989]\n",
      " [ 0.35660272  0.64339728]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.31822521  0.5696736   0.09197874  0.02012245] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 2.42743343 -0.34091615  0.03824876  0.2868834 ]\n",
      "Policy: [[ 0.99897124  0.00102876]\n",
      " [ 0.12975523  0.87024477]\n",
      " [ 0.35479458  0.64520542]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.31201332  0.58473952  0.08437003  0.01887712] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 2.49694712 -0.4009166   0.03051557  0.28510334]\n",
      "Policy: [[  9.99042544e-01   9.57456228e-04]\n",
      " [  1.20603148e-01   8.79396852e-01]\n",
      " [  3.53384593e-01   6.46615407e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.30486303  0.60000444  0.07733196  0.01780058] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 2.56725725 -0.46245739  0.02343278  0.2834168 ]\n",
      "Policy: [[  9.99103767e-01   8.96233087e-04]\n",
      " [  1.12044292e-01   8.87955708e-01]\n",
      " [  3.52341305e-01   6.47658695e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.29689878  0.61540796  0.07082785  0.01686542] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 2.63843354 -0.52554655  0.01695051  0.28181193]\n",
      "Policy: [[  9.99156680e-01   8.43320082e-04]\n",
      " [  1.04035093e-01   8.95964907e-01]\n",
      " [  3.51634053e-01   6.48365947e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.2882371   0.63089151  0.06482268  0.01604871] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 2.71053484 -0.59018641  0.01102219  0.28027881]\n",
      "Policy: [[  9.99202721e-01   7.97278646e-04]\n",
      " [  9.65371586e-02   9.03462841e-01]\n",
      " [  3.51233094e-01   6.48766906e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.27898691  0.64639868  0.05928321  0.0153312 ] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 2.78360985 -0.65637395  0.0056044   0.27880913]\n",
      "Policy: [[  9.99243066e-01   7.56933893e-04]\n",
      " [  8.95164948e-02   9.10483505e-01]\n",
      " [  3.51109698e-01   6.48890302e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.26924997  0.66187534  0.05417794  0.01469675] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [  2.85769770e+00  -7.24100911e-01   6.56678856e-04   2.77395956e-01]\n",
      "Policy: [[  9.99278678e-01   7.21321861e-04]\n",
      " [  8.29428290e-02   9.17057171e-01]\n",
      " [  3.51236209e-01   6.48763791e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.25912143  0.67726963  0.0494772   0.01413174] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 2.93282867 -0.7933541  -0.00385862  0.27603348]\n",
      "Policy: [[  9.99310352e-01   6.89648413e-04]\n",
      " [  7.67890103e-02   9.23210990e-01]\n",
      " [  3.51586081e-01   6.48413919e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.24869037  0.69253192  0.04515299  0.01362473] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 3.00902464 -0.86411558 -0.00797651  0.27471688]\n",
      "Policy: [[  9.99338743e-01   6.61257113e-04]\n",
      " [  7.10304936e-02   9.28969506e-01]\n",
      " [  3.52133911e-01   6.47866089e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.23804023  0.7076148   0.04117895  0.01316602] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 3.08629973 -0.93636289 -0.01172954  0.27344213]\n",
      "Policy: [[  9.99364396e-01   6.35604000e-04]\n",
      " [  6.56448977e-02   9.34355102e-01]\n",
      " [  3.52855459e-01   6.47144541e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.22724914  0.72247311  0.03753026  0.01274749] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 3.16466072 -1.0100693  -0.0151479   0.27220591]\n",
      "Policy: [[  9.99387762e-01   6.12237674e-04]\n",
      " [  6.06116317e-02   9.39388368e-01]\n",
      " [  3.53727680e-01   6.46272320e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.21639011  0.73706407  0.03418358  0.01236225] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 3.2441076  -1.08520404 -0.01825959  0.27100545]\n",
      "Policy: [[  9.99409217e-01   5.90783444e-04]\n",
      " [  5.59115833e-02   9.44088417e-01]\n",
      " [  3.54728745e-01   6.45271255e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.20553116  0.7513474   0.0311169   0.01200454] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 3.32463407 -1.1617326  -0.02109054  0.2698385 ]\n",
      "Policy: [[  9.99429069e-01   5.70930617e-04]\n",
      " [  5.15268613e-02   9.48473139e-01]\n",
      " [  3.55838075e-01   6.44161925e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.19473534  0.76528559  0.02830953  0.01166954] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 3.40622801 -1.23961701 -0.02366474  0.26870317]\n",
      "Policy: [[  9.99447578e-01   5.52422176e-04]\n",
      " [  4.74405862e-02   9.52559414e-01]\n",
      " [  3.57036370e-01   6.42963630e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.1840606   0.77884414  0.025742    0.01135326] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 3.48887203 -1.3188162  -0.02600434  0.26759794]\n",
      "Policy: [[  9.99464954e-01   5.35046320e-04]\n",
      " [  4.36367243e-02   9.56363276e-01]\n",
      " [  3.58305638e-01   6.41694362e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.17355977  0.79199184  0.023396    0.01105239] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 3.57254399 -1.3992863  -0.02812977  0.26652151]\n",
      "Policy: [[  9.99481371e-01   5.18629419e-04]\n",
      " [  4.00999576e-02   9.59900042e-01]\n",
      " [  3.59629221e-01   6.40370779e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.16328039  0.80470103  0.02125433  0.01076425] \n",
      "\n",
      "Final reward weights:  [ 3.57254399 -1.3992863  -0.02812977  0.26652151]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.57254399, -1.3992863 , -0.02812977,  0.26652151])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
