{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdps import MDP_toy_irreversibility\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        D = np.zeros_like(P_0)\n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "mdp = MDP_toy_irreversibility()    \n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vector=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vector is None:\n",
    "        r_vector = .1*np.random.randn(mdp.nS)\n",
    "        print('Initial reward vector {}'.format(r_vector))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vector, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vector = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vector = r_vector - learning_rate * dL_dr_vector\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vector))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=0.1,\n",
    "         epochs = 100):\n",
    "\n",
    "    mdp = MDP_toy_irreversibility()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[1]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[1]=1\n",
    "    \n",
    "    r_vector = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vector)\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
