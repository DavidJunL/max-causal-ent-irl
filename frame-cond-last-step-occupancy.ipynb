{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdps import MDP_toy_irreversibility, MDP_chain, MDP_water\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 0.  0.5]\n",
      "[0.25 0.   0.25 0.5 ]\n",
      "[0.    0.25  0.125 0.625]\n",
      "[0.125  0.0625 0.1875 0.625 ]\n",
      "[0.03125 0.15625 0.125   0.6875 ]\n",
      "[0.078125 0.078125 0.140625 0.703125]\n",
      "[0.0390625 0.109375  0.109375  0.7421875]\n",
      "[0.0546875  0.07421875 0.109375   0.76171875]\n",
      "[0.03710938 0.08203125 0.09179688 0.7890625 ]\n",
      "[0.04101562 0.06445312 0.08691406 0.80761719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04101562, 0.06445312, 0.08691406, 0.80761719])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(P_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "\n",
    "mdp = MDP_toy_irreversibility()    \n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "# A small example demonstrating what last-step OM looks like for a uniformly random policy.\n",
    "# We can see that most of the probability mass is on the agent ending up in irreversible s_3.\n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.nS)\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vec, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vec = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vec))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_toy_irreversibility()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[1]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[1]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [ 0.00248065 -0.01077407 -0.0185889   0.00981771]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.00115007  0.08702804 -0.02117976 -0.08406297]\n",
      "Last-step D: [0.01330578 0.02197886 0.02590856 0.9388068 ] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [-0.0059881   0.16926953 -0.04504878 -0.13529727]\n",
      "Last-step D: [0.07138169 0.17758509 0.23869023 0.51234299] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [-0.01661662  0.24003307 -0.08358427 -0.1568968 ]\n",
      "Last-step D: [0.10628522 0.29236466 0.38535486 0.21599526] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [-0.02878332  0.30700523 -0.12454678 -0.17073976]\n",
      "Last-step D: [0.12166695 0.33027834 0.4096251  0.13842961] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [-0.04217871  0.3716211  -0.16569555 -0.18081146]\n",
      "Last-step D: [0.13395392 0.35384132 0.41148778 0.10071699] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [-0.05666755  0.43443098 -0.20629196 -0.1885361 ]\n",
      "Last-step D: [0.14488835 0.37190119 0.40596406 0.0772464 ] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [-0.0721509   0.49573231 -0.24603075 -0.19461528]\n",
      "Last-step D: [0.15483351 0.38698674 0.39738792 0.06079183] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [-0.08853889  0.55571758 -0.28477877 -0.19946454]\n",
      "Last-step D: [0.16387991 0.40014734 0.38748021 0.04849253] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [-0.10574403  0.61452412 -0.32248408 -0.20336063]\n",
      "Last-step D: [0.17205144 0.41193458 0.37705306 0.03896091] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [-0.12368012  0.67225521 -0.35913717 -0.20650255]\n",
      "Last-step D: [0.17936084 0.42268913 0.36653085 0.03141917] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [-0.14226279  0.72899079 -0.39475204 -0.20904059]\n",
      "Last-step D: [0.18582675 0.4326441  0.35614874 0.0253804 ] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [-0.16141061  0.78479397 -0.4293561  -0.21109188]\n",
      "Last-step D: [0.19147821 0.44196827 0.34604061 0.02051291] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [-0.18104606  0.83971528 -0.46298435 -0.21274949]\n",
      "Last-step D: [0.19635451 0.45078687 0.33628252 0.0165761 ] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [-0.2010964   0.89379597 -0.49567596 -0.21408823]\n",
      "Last-step D: [0.20050333 0.45919313 0.32691609 0.01338745] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [-0.22149423  0.9470704  -0.52747213 -0.21516866]\n",
      "Last-step D: [0.20397834 0.46725571 0.31796172 0.01080424] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [-0.24217791  0.99956798 -0.55841479 -0.21603991]\n",
      "Last-step D: [0.2068368  0.47502412 0.30942651 0.00871257] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [-0.26309165  1.05131469 -0.5885457  -0.21674196]\n",
      "Last-step D: [0.20913743 0.48253294 0.30130914 0.00702049] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [-0.28418551  1.10233415 -0.61790598 -0.21730728]\n",
      "Last-step D: [0.21093863 0.48980537 0.29360282 0.00565318] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [-0.30541522  1.15264855 -0.64653571 -0.21776224]\n",
      "Last-step D: [0.2122971  0.49685601 0.28629728 0.0045496 ] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [-0.3267419   1.20227922 -0.67447371 -0.21812823]\n",
      "Last-step D: [0.21326677 0.50369332 0.27938002 0.0036599 ] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [-0.34813171  1.25124708 -0.70175742 -0.21842256]\n",
      "Last-step D: [0.21389811 0.51032145 0.2728371  0.00294334] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [-0.36955548  1.2995729  -0.7284228  -0.21865924]\n",
      "Last-step D: [0.21423769 0.51674179 0.26665377 0.00236675] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [-0.39098828  1.34727748 -0.75450428 -0.21884955]\n",
      "Last-step D: [0.21432796 0.52295414 0.26081479 0.0019031 ] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [-0.412409    1.39438173 -0.78003475 -0.2190026 ]\n",
      "Last-step D: [0.21420722 0.52895756 0.25530473 0.00153048] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [-0.43379997  1.44090663 -0.80504557 -0.21912571]\n",
      "Last-step D: [0.21390968 0.534751   0.25010819 0.00123113] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [-0.45514654  1.48687325 -0.82956656 -0.21922478]\n",
      "Last-step D: [0.21346567 0.54033375 0.24520991 0.00099068] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [-0.47643672  1.53230268 -0.85362605 -0.21930453]\n",
      "Last-step D: [0.21290185 0.54570569 0.24059492 0.00079754] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [-0.49766087  1.57721594 -0.87725092 -0.21936877]\n",
      "Last-step D: [0.21224153 0.55086745 0.23624862 0.0006424 ] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [-0.51881137  1.62163389 -0.9004666  -0.21942055]\n",
      "Last-step D: [2.11504927e-01 5.55820489e-01 2.32156841e-01 5.17743139e-04] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [-0.53988232  1.66557718 -0.92329719 -0.2194623 ]\n",
      "Last-step D: [2.10709501e-01 5.60567063e-01 2.28305889e-01 4.17546987e-04] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [-0.56086934  1.70906616 -0.94576545 -0.219496  ]\n",
      "Last-step D: [2.09870220e-01 5.65110212e-01 2.24682593e-01 3.36974405e-04] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [-0.58176932  1.75212079 -0.96789288 -0.21952321]\n",
      "Last-step D: [2.08999851e-01 5.69453675e-01 2.21274327e-01 2.72146903e-04] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [-0.60258024  1.79476061 -0.98969978 -0.21954521]\n",
      "Last-step D: [2.08109218e-01 5.73601804e-01 2.18069023e-01 2.19955648e-04] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [-0.62330099  1.83700467 -1.0112053  -0.219563  ]\n",
      "Last-step D: [2.07207444e-01 5.77559472e-01 2.15055175e-01 1.77909363e-04] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [-0.64393121  1.87887147 -1.03242748 -0.2195774 ]\n",
      "Last-step D: [2.06302168e-01 5.81331976e-01 2.12221844e-01 1.44011781e-04] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [-0.66447118  1.92037897 -1.05338335 -0.21958907]\n",
      "Last-step D: [2.05399740e-01 5.84924949e-01 2.09558648e-01 1.16663120e-04] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [-0.68492172  1.96154455 -1.07408892 -0.21959852]\n",
      "Last-step D: [2.04505393e-01 5.88344273e-01 2.07055753e-01 9.45810106e-05] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [-0.70528406  2.00238495 -1.09455931 -0.2196062 ]\n",
      "Last-step D: [2.03623403e-01 5.91596002e-01 2.04703858e-01 7.67371307e-05] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [-0.72555978  2.04291632 -1.11480873 -0.21961243]\n",
      "Last-step D: [2.02757222e-01 5.94686296e-01 2.02494176e-01 6.23064718e-05] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [-0.74575074  2.08315418 -1.13485057 -0.21961749]\n",
      "Last-step D: [2.01909597e-01 5.97621357e-01 2.00418419e-01 5.06267574e-05] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [-0.76585901  2.12311344 -1.15469745 -0.21962161]\n",
      "Last-step D: [2.01082677e-01 6.00407385e-01 1.98468772e-01 4.11659958e-05] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [-0.78588682  2.16280839 -1.17436123 -0.21962496]\n",
      "Last-step D: [2.00278098e-01 6.03050528e-01 1.96637877e-01 3.34965432e-05] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [-0.80583653  2.2022527  -1.19385312 -0.21962769]\n",
      "Last-step D: [1.99497067e-01 6.05556851e-01 1.94918808e-01 2.72743734e-05] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [-0.82571057  2.24145947 -1.21318362 -0.21962991]\n",
      "Last-step D: [1.98740428e-01 6.07932304e-01 1.93305045e-01 2.22225057e-05] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [-0.84551144  2.2804412  -1.23236267 -0.21963172]\n",
      "Last-step D: [1.98008718e-01 6.10182702e-01 1.91790462e-01 1.81177542e-05] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [-0.86524166  2.31920983 -1.2513996  -0.2196332 ]\n",
      "Last-step D: [1.97302221e-01 6.12313704e-01 1.90369295e-01 1.47801266e-05] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [-0.88490376  2.35777675 -1.27030321 -0.2196344 ]\n",
      "Last-step D: [1.96621008e-01 6.14330800e-01 1.89036127e-01 1.20643414e-05] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [-0.90450026  2.39615282 -1.2890818  -0.21963539]\n",
      "Last-step D: [1.95964976e-01 6.16239303e-01 1.87785868e-01 9.85303218e-06] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [-0.92403365  2.43434839 -1.30774317 -0.21963619]\n",
      "Last-step D: [1.95333876e-01 6.18044341e-01 1.86613731e-01 8.05130307e-06] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [-0.94350638  2.4723733  -1.32629469 -0.21963685]\n",
      "Last-step D: [1.94727346e-01 6.19750853e-01 1.85515219e-01 6.58236089e-06] \n",
      "\n",
      "Final reward weights:  [-0.94350638  2.4723733  -1.32629469 -0.21963685]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.94350638,  2.4723733 , -1.32629469, -0.21963685])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(1)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [-0.00590599 -0.019307   -0.01034276 -0.00554016  0.00600828 -0.00247221\n",
      "  0.00621924  0.00322552]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.09409401 -0.0200763  -0.01243408 -0.02566105  0.0047319  -0.00527401\n",
      " -0.01309795 -0.05039759]\n",
      "Last-step D: [2.71175181e-11 7.69306400e-03 2.09131983e-02 2.01208916e-01\n",
      " 1.27637579e-02 2.80180139e-02 1.93171922e-01 5.36231128e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.19409401 -0.02166139 -0.01636518 -0.05397684  0.00198036 -0.01080411\n",
      " -0.03996869 -0.08141323]\n",
      "Last-step D: [4.65543103e-10 1.58508833e-02 3.93110096e-02 2.83157927e-01\n",
      " 2.75154272e-02 5.53009699e-02 2.68707393e-01 3.10156390e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.29409401 -0.02426685 -0.022049   -0.08224317 -0.00280719 -0.01932765\n",
      " -0.06636148 -0.10515374]\n",
      "Last-step D: [6.74073093e-09 2.60546002e-02 5.68382069e-02 2.82663274e-01\n",
      " 4.78755222e-02 8.52353990e-02 2.63927878e-01 2.37405113e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.394094   -0.02805371 -0.0293047  -0.1083326  -0.01009826 -0.03079014\n",
      " -0.09036374 -0.12526591]\n",
      "Last-step D: [9.24044311e-08 3.78685875e-02 7.25570209e-02 2.60894335e-01\n",
      " 7.29106476e-02 1.14624918e-01 2.40022692e-01 2.01121707e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.49409388 -0.0330254  -0.03774925 -0.13188718 -0.0199884  -0.04458784\n",
      " -0.11178314 -0.14318774]\n",
      "Last-step D: [1.20910574e-06 4.97168551e-02 8.44454564e-02 2.35545724e-01\n",
      " 9.89014261e-02 1.37977036e-01 2.14193996e-01 1.79218297e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.59409235 -0.03908608 -0.0469944  -0.1531842  -0.03228585 -0.05995024\n",
      " -0.13097522 -0.15973142]\n",
      "Last-step D: [1.53514268e-05 6.06068269e-02 9.24514704e-02 2.12970228e-01\n",
      " 1.22974510e-01 1.53624004e-01 1.91920780e-01 1.65436830e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.69407322 -0.0461503  -0.05676341 -0.17257715 -0.04674063 -0.07623217\n",
      " -0.14830705 -0.17541758]\n",
      "Last-step D: [1.91281344e-04 7.06422424e-02 9.76901690e-02 1.93929503e-01\n",
      " 1.44547789e-01 1.62819249e-01 1.73318231e-01 1.56861535e-01] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.79384032 -0.05429222 -0.06688013 -0.19016899 -0.063286   -0.09290123\n",
      " -0.16386136 -0.19056545]\n",
      "Last-step D: [0.002329   0.08141922 0.10116712 0.17591841 0.16545371 0.16669064\n",
      " 0.15554311 0.15147877] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.89124442 -0.06390544 -0.07703054 -0.20535291 -0.08190668 -0.10903245\n",
      " -0.1770335  -0.20509796]\n",
      "Last-step D: [0.02595899 0.09613216 0.10150415 0.15183919 0.18620681 0.16131218\n",
      " 0.13172142 0.14532509] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.97186552 -0.0741364  -0.0855945  -0.21604447 -0.09942785 -0.1216937\n",
      " -0.1860197  -0.21706397]\n",
      "Last-step D: [0.193789   0.10230963 0.08563964 0.10691556 0.17521164 0.12661249\n",
      " 0.08986197 0.11966007] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 1.01651412 -0.08075852 -0.09042783 -0.2214161  -0.1095096  -0.12838893\n",
      " -0.19042186 -0.22370635]\n",
      "Last-step D: [0.55351398 0.06622114 0.04833325 0.05371636 0.10081755 0.0669523\n",
      " 0.04402165 0.06642377] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 1.04158341 -0.08462708 -0.09317995 -0.22438518 -0.11512075 -0.13210712\n",
      " -0.19283663 -0.22744177]\n",
      "Last-step D: [0.74930708 0.03868557 0.02752125 0.02969081 0.05611145 0.03718196\n",
      " 0.02414769 0.03735417] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 1.05847427 -0.08726123 -0.09504943 -0.22638767 -0.11886202 -0.13460747\n",
      " -0.19446179 -0.22995972]\n",
      "Last-step D: [0.83109145 0.02634154 0.01869477 0.02002491 0.03741274 0.02500346\n",
      " 0.01625159 0.02517955] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 1.07111365 -0.08923954 -0.09645595 -0.2278914  -0.12163813 -0.13647832\n",
      " -0.19568126 -0.23184412]\n",
      "Last-step D: [0.87360614 0.01978311 0.01406515 0.01503724 0.02776113 0.01870852\n",
      " 0.01219472 0.01884398] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 1.08117787 -0.09081683 -0.09758034 -0.22909324 -0.12383366 -0.13796863\n",
      " -0.19665567 -0.23334457]\n",
      "Last-step D: [0.89935786 0.01577285 0.01124392 0.01201843 0.02195525 0.01490309\n",
      " 0.00974409 0.01500451] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 1.08952286 -0.09212508 -0.09851554 -0.23009329 -0.1256439  -0.13920506\n",
      " -0.19746643 -0.23458862]\n",
      "Last-step D: [0.91655005 0.01308259 0.00935199 0.0100005  0.01810243 0.01236424\n",
      " 0.00810765 0.01244055] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 1.09664159 -0.09324094 -0.09931532 -0.23094913 -0.12718076 -0.14026041\n",
      " -0.19816032 -0.23564977]\n",
      "Last-step D: [0.92881275 0.01115856 0.00799777 0.00855838 0.01536862 0.01055359\n",
      " 0.00693884 0.0106115 ] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 1.10284293 -0.09421264 -0.10001349 -0.23169685 -0.12851404 -0.1411803\n",
      " -0.19876661 -0.23657408]\n",
      "Last-step D: [0.93798654 0.00971697 0.00698176 0.00747721 0.01333273 0.00919885\n",
      " 0.00606287 0.00924307] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 1.10833286 -0.09507244 -0.10063269 -0.23236055 -0.12969006 -0.1419951\n",
      " -0.19930483 -0.23739226]\n",
      "Last-step D: [0.94510071 0.00859805 0.00619195 0.00663696 0.01176027 0.00814801\n",
      " 0.00538222 0.00818184] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 1.1132554  -0.09584296 -0.10118876 -0.23295709 -0.13074113 -0.14272607\n",
      " -0.19978866 -0.23812581]\n",
      "Last-step D: [0.95077461 0.00770523 0.00556072 0.0059654  0.01051066 0.00730965\n",
      " 0.00483828 0.00733545] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 1.11771513 -0.09654065 -0.10169325 -0.23349874 -0.1316906  -0.14338862\n",
      " -0.20022803 -0.23879032]\n",
      "Last-step D: [0.95540277 0.00697682 0.0050449  0.0054165  0.00949466 0.00662557\n",
      " 0.00439372 0.00664507] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 1.12179031 -0.0971778  -0.10215481 -0.23399469 -0.13255589 -0.14399432\n",
      " -0.20063039 -0.23939746]\n",
      "Last-step D: [0.95924816 0.00637157 0.00461563 0.00495957 0.00865298 0.00605698\n",
      " 0.00402364 0.00607147] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 1.12554104 -0.0977639  -0.1025801  -0.23445202 -0.13335037 -0.14455203\n",
      " -0.20100147 -0.23995621]\n",
      "Last-step D: [0.96249271 0.00586094 0.00425291 0.00457332 0.00794473 0.00557707\n",
      " 0.00371081 0.00558751] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 1.12901442 -0.09830635 -0.10297435 -0.23487628 -0.13408445 -0.1450687\n",
      " -0.20134576 -0.2404736 ]\n",
      "Last-step D: [0.96526619 0.00542451 0.00394245 0.00424259 0.00734081 0.00516669\n",
      " 0.00344292 0.00517383] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 1.13224805 -0.09881108 -0.10334172 -0.23527191 -0.13476645 -0.14554988\n",
      " -0.20166686 -0.24095522]\n",
      "Last-step D: [0.96766365 0.00504733 0.00367377 0.00395622 0.00681997 0.00481183\n",
      " 0.00321097 0.00481625] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 1.13527243 -0.0992829  -0.10368562 -0.23564249 -0.13540308 -0.14600008\n",
      " -0.20196768 -0.24140564]\n",
      "Last-step D: [0.96975626 0.0047182  0.00343899 0.00370589 0.00636634 0.004502\n",
      " 0.00300818 0.00450416] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 1.1381126  -0.09972576 -0.10400883 -0.23599101 -0.13599986 -0.146423\n",
      " -0.20225062 -0.24182858]\n",
      "Last-step D: [0.97159834 0.00442855 0.00323211 0.00348519 0.00596781 0.00422916\n",
      " 0.0028294  0.00422944] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 1.14078939 -0.10014293 -0.10431368 -0.23631993 -0.13656136 -0.14682171\n",
      " -0.20251768 -0.24222716]\n",
      "Last-step D: [0.97323208 0.00417173 0.00304846 0.00328919 0.00561501 0.00398712\n",
      " 0.00267061 0.0039858 ] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 1.14332031 -0.10053718 -0.10460212 -0.23663133 -0.13709142 -0.1471988\n",
      " -0.20277054 -0.24260399]\n",
      "Last-step D: [0.97469073 0.00394251 0.00288435 0.00311396 0.00530059 0.00377094\n",
      " 0.00252863 0.00376829] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 1.14572023 -0.10091085 -0.1048758  -0.23692696 -0.13759329 -0.14755647\n",
      " -0.20301064 -0.24296129]\n",
      "Last-step D: [0.97600085 0.0037367  0.00273683 0.00295637 0.00501865 0.00357673\n",
      " 0.00240094 0.00357294] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 1.14800184 -0.10126594 -0.10513615 -0.23720835 -0.13806973 -0.14789661\n",
      " -0.20323918 -0.24330094]\n",
      "Last-step D: [0.97718389 0.0035509  0.00260352 0.00281389 0.00476447 0.00340131\n",
      " 0.00228549 0.00339654] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 1.1501761  -0.10160418 -0.1053844  -0.2374768  -0.13852315 -0.14822081\n",
      " -0.20345724 -0.24362459]\n",
      "Last-step D: [0.97825737 0.00338237 0.00248247 0.00268446 0.00453417 0.00324209\n",
      " 0.0021806  0.00323648] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 1.15225253 -0.10192706 -0.1056216  -0.23773344 -0.13895561 -0.14853051\n",
      " -0.20366573 -0.24393365]\n",
      "Last-step D: [0.97923577 0.00322881 0.00237206 0.00256636 0.00432456 0.00309693\n",
      " 0.00208488 0.00309061] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 1.15423942 -0.10223589 -0.1058487  -0.23797925 -0.13936891 -0.14882691\n",
      " -0.20386545 -0.24422936]\n",
      "Last-step D: [0.98013111 0.00308834 0.00227096 0.00245817 0.00413302 0.00296407\n",
      " 0.0019972  0.00295714] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 1.15614407 -0.10253183 -0.1060665  -0.23821512 -0.13976464 -0.14911112\n",
      " -0.20405711 -0.24451282]\n",
      "Last-step D: [0.98095346 0.00295935 0.00217805 0.0023587  0.00395731 0.00284201\n",
      " 0.00191657 0.00283454] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 1.15797293 -0.10281588 -0.10627574 -0.23844182 -0.1401442  -0.14938406\n",
      " -0.20424133 -0.24478497]\n",
      "Last-step D: [0.98171137 0.00284051 0.00209237 0.00226694 0.00379558 0.00272949\n",
      " 0.00184218 0.00272156] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 1.15973173 -0.10308895 -0.10647705 -0.23866002 -0.14050882 -0.14964661\n",
      " -0.20441866 -0.24504668]\n",
      "Last-step D: [0.98241207 0.00273068 0.00201311 0.00218202 0.00364623 0.00262544\n",
      " 0.00177334 0.00261712] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 1.16142555 -0.10335184 -0.10667101 -0.23887034 -0.14085961 -0.1498995\n",
      " -0.2045896  -0.24529871]\n",
      "Last-step D: [0.98306177 0.00262887 0.00193959 0.00210321 0.00350791 0.00252894\n",
      " 0.00170944 0.00252028] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38\n",
      "Reward vector: [ 1.16305897 -0.10360526 -0.10685813 -0.23907333 -0.14119756 -0.15014342\n",
      " -0.2047546  -0.24554174]\n",
      "Last-step D: [0.98366581 0.00253424 0.00187119 0.00202987 0.00337946 0.0024392\n",
      " 0.00164998 0.00243025] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 1.16463608 -0.10384987 -0.10703887 -0.23926947 -0.14152354 -0.15037898\n",
      " -0.20491405 -0.24577637]\n",
      "Last-step D: [0.98422883 0.00244607 0.00180741 0.00196145 0.00325985 0.00235555\n",
      " 0.0015945  0.00234633] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 1.1661606  -0.10408624 -0.10721365 -0.23945922 -0.14183836 -0.15060671\n",
      " -0.20506832 -0.24600316]\n",
      "Last-step D: [0.98475482 0.00236371 0.0017478  0.00189749 0.00314823 0.00227738\n",
      " 0.00154263 0.00226794] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 1.16763587 -0.1043149  -0.10738285 -0.23964297 -0.14214275 -0.15082713\n",
      " -0.20521772 -0.24622262]\n",
      "Last-step D: [0.98524732 0.00228663 0.00169196 0.00183754 0.00304381 0.00220417\n",
      " 0.00149402 0.00219455] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 1.16906493 -0.10453633 -0.1075468  -0.2398211  -0.14243734 -0.15104068\n",
      " -0.20536255 -0.24643519]\n",
      "Last-step D: [0.98570941 0.00221432 0.00163954 0.00178126 0.00294594 0.00213547\n",
      " 0.00144837 0.00212569] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 1.17045055 -0.10475097 -0.10770583 -0.23999393 -0.14272274 -0.15124777\n",
      " -0.2055031  -0.24664128]\n",
      "Last-step D: [0.98614379 0.00214637 0.00159025 0.00172831 0.00285402 0.00207088\n",
      " 0.00140542 0.00206096] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 1.17179526 -0.10495921 -0.10786021 -0.24016177 -0.1429995  -0.15144877\n",
      " -0.20563959 -0.24684128]\n",
      "Last-step D: [0.98655289 0.00208239 0.0015438  0.00167841 0.00276753 0.00201004\n",
      " 0.00136494 0.002     ] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 1.17310138 -0.10516141 -0.1080102  -0.2403249  -0.1432681  -0.15164404\n",
      " -0.20577226 -0.24703553]\n",
      "Last-step D: [0.98693883 0.00202204 0.00149997 0.00163129 0.002686   0.00195264\n",
      " 0.00132673 0.00194249] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 1.17437103 -0.10535792 -0.10815606 -0.24048358 -0.143529   -0.15183387\n",
      " -0.20590132 -0.24722435]\n",
      "Last-step D: [0.98730351 0.00196504 0.00145854 0.00158675 0.00260903 0.00189838\n",
      " 0.00129059 0.00188816] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 1.17560616 -0.10554903 -0.10829799 -0.24063803 -0.14378262 -0.15201858\n",
      " -0.20602696 -0.24740802]\n",
      "Last-step D: [0.98764863 0.00191111 0.00141932 0.00154456 0.00253625 0.00184703\n",
      " 0.00125636 0.00183674] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 1.17680859 -0.10573503 -0.1084362  -0.24078849 -0.14402936 -0.15219841\n",
      " -0.20614935 -0.24758682]\n",
      "Last-step D: [0.98797573 0.00186    0.00138213 0.00150455 0.00246732 0.00179836\n",
      " 0.0012239  0.00178801] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 1.17797997 -0.10591618 -0.10857088 -0.24093514 -0.14426955 -0.15237363\n",
      " -0.20626866 -0.247761  ]\n",
      "Last-step D: [0.98828616 0.00181151 0.00134682 0.00146656 0.00240196 0.00175215\n",
      " 0.00119308 0.00174176] \n",
      "\n",
      "Final reward weights:  [ 1.17797997 -0.10591618 -0.10857088 -0.24093514 -0.14426955 -0.15237363\n",
      " -0.20626866 -0.247761  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.17797997, -0.10591618, -0.10857088, -0.24093514, -0.14426955,\n",
       "       -0.15237363, -0.20626866, -0.247761  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_chain(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_chain()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec\n",
    "\n",
    "test_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [ 0.01597626  0.00777117  0.00579403 -0.01892682  0.00215899  0.01553243\n",
      "  0.00700513]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.11597626  0.00777117  0.00579403 -0.01892682 -0.03065443 -0.01804359\n",
      " -0.02660543]\n",
      "Last-step D: [3.77662011e-11 3.76499052e-11 0.00000000e+00 3.70110779e-11\n",
      " 3.28134180e-01 3.35760228e-01 3.36105591e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.21597626  0.00777117  0.00579403 -0.01892682 -0.06353721 -0.05161182\n",
      " -0.06015441]\n",
      "Last-step D: [7.09692079e-10 5.74962358e-10 0.00000000e+00 5.64317903e-10\n",
      " 3.28827777e-01 3.35682346e-01 3.35489875e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.31597626  0.00777117  0.00579403 -0.01892682 -0.09647855 -0.08517243\n",
      " -0.09365246]\n",
      "Last-step D: [1.33360635e-08 8.62109166e-09 0.00000000e+00 8.45036081e-09\n",
      " 3.29413445e-01 3.35606074e-01 3.34980451e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.41597623  0.00777115  0.00579403 -0.01892683 -0.12946941 -0.11872558\n",
      " -0.1271084 ]\n",
      "Last-step D: [2.50598394e-07 1.26048339e-07 0.00000000e+00 1.23416271e-07\n",
      " 3.29908559e-01 3.35531517e-01 3.34559424e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.51597576  0.00777098  0.00579403 -0.018927   -0.162502   -0.15227124\n",
      " -0.16052933]\n",
      "Last-step D: [4.70880774e-06 1.77664433e-06 0.00000000e+00 1.73794706e-06\n",
      " 3.30325949e-01 3.35456539e-01 3.34209288e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.61596692  0.00776861  0.00579403 -0.01892931 -0.19556633 -0.1858056\n",
      " -0.19391712]\n",
      "Last-step D: [8.84329151e-05 2.36314198e-05 0.00000000e+00 2.30989576e-05\n",
      " 3.30643222e-01 3.35343673e-01 3.33877942e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.7158021   0.00774042  0.00579403 -0.01895685 -0.22859433 -0.21926374\n",
      " -0.22721044]\n",
      "Last-step D: [1.64813933e-03 2.81911812e-04 0.00000000e+00 2.75383537e-04\n",
      " 3.30280024e-01 3.34581337e-01 3.32933204e-01] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.81298811  0.00748952  0.00579403 -0.01920181 -0.26063407 -0.2516796\n",
      " -0.25944499]\n",
      "Last-step D: [0.02813991 0.00250898 0.         0.00244959 0.32039746 0.32415865\n",
      " 0.32234542] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.88616889  0.00665282  0.00579403 -0.0200184  -0.28436862 -0.27565515\n",
      " -0.28326237]\n",
      "Last-step D: [0.26819222 0.00836706 0.         0.00816592 0.23734551 0.23975546\n",
      " 0.23817383] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.91947943  0.00602551  0.00579403 -0.02063054 -0.29501934 -0.28640376\n",
      " -0.29393415]\n",
      "Last-step D: [0.66689457 0.00627306 0.         0.00612137 0.10650716 0.10748606\n",
      " 0.10671778] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.93845827  0.00563134  0.00579403 -0.02101517 -0.30106482 -0.29250313\n",
      " -0.29998934]\n",
      "Last-step D: [0.81021161 0.00394169 0.         0.00384628 0.06045476 0.06099378\n",
      " 0.06055189] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 0.95160956  0.00534871  0.00579403 -0.02129096 -0.30524814 -0.29672324\n",
      " -0.30417877]\n",
      "Last-step D: [0.86848715 0.00282631 0.         0.00275787 0.04183321 0.04220108\n",
      " 0.04189437] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.96164533  0.0051293   0.00579403 -0.02150506 -0.30843815 -0.29994107\n",
      " -0.30737319]\n",
      "Last-step D: [0.8996423  0.00219415 0.         0.00214101 0.03190009 0.03217825\n",
      " 0.0319442 ] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.96974746  0.00495032  0.00579403 -0.0216797  -0.3110124  -0.30253764\n",
      " -0.30995088]\n",
      "Last-step D: [0.91897864 0.00178977 0.         0.00174641 0.02574258 0.02596573\n",
      " 0.02577687] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.97653432  0.00479937  0.00579403 -0.021827   -0.31316814 -0.30471198\n",
      " -0.31210941]\n",
      "Last-step D: [0.93213144 0.00150956 0.         0.00147299 0.02155735 0.02174337\n",
      " 0.02158528] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.98236934  0.00466894  0.00579403 -0.02195426 -0.31502115 -0.30658093\n",
      " -0.31396477]\n",
      "Last-step D: [0.9416498  0.00130423 0.         0.00127264 0.01853017 0.01868949\n",
      " 0.01855366] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.98748406  0.0045542   0.00579403 -0.02206622 -0.31664518 -0.30821887\n",
      " -0.31559082]\n",
      "Last-step D: [0.94885284 0.00114744 0.         0.00111964 0.01624021 0.01637943\n",
      " 0.01626045] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.99203495  0.00445181  0.00579403 -0.02216613 -0.31809    -0.30967605\n",
      " -0.31703742]\n",
      "Last-step D: [0.95449107 0.00102387 0.         0.00099906 0.01444824 0.01457178\n",
      " 0.01446599] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.99613268  0.00435941  0.00579403 -0.02225629 -0.31939083 -0.31098797\n",
      " -0.31833983]\n",
      "Last-step D: [9.59022729e-01 9.24017222e-04 0.00000000e+00 9.01627077e-04\n",
      " 1.30082889e-02 1.31192648e-02 1.30240727e-02] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 0.99985834  0.00427524  0.00579403 -0.02233842 -0.32057346 -0.31218067\n",
      " -0.31952387]\n",
      "Last-step D: [9.62743332e-01 8.41684638e-04 0.00000000e+00 8.21288804e-04\n",
      " 1.18262677e-02 1.19269605e-02 1.18404659e-02] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 1.00327315  0.00419798  0.00579403 -0.02241382 -0.32165734 -0.31327376\n",
      " -0.32060904]\n",
      "Last-step D: [9.65851933e-01 7.72652311e-04 0.00000000e+00 7.53928707e-04\n",
      " 1.08388238e-02 1.09309460e-02 1.08517158e-02] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 1.0064244   0.00412658  0.00579403 -0.02248348 -0.32265751 -0.31428243\n",
      " -0.3216104 ]\n",
      "Last-step D: [9.68487487e-01 7.13952439e-04 0.00000000e+00 6.96650838e-04\n",
      " 1.00017464e-02 1.00866183e-02 1.00135446e-02] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 1.00934941  0.00406024  0.00579403 -0.02254822 -0.32358584 -0.31521862\n",
      " -0.32253981]\n",
      "Last-step D: [9.70749918e-01 6.63437028e-04 0.00000000e+00 6.47359210e-04\n",
      " 9.28325192e-03 9.36191212e-03 9.29412156e-03] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 1.01207812  0.00399829  0.00579403 -0.02260867 -0.32445183 -0.31609194\n",
      " -0.32340681]\n",
      "Last-step D: [9.72712909e-01 6.19513513e-04 0.00000000e+00 6.04499821e-04\n",
      " 8.65990853e-03 8.73318872e-03 8.66998027e-03] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 1.01463492  0.00394019  0.00579403 -0.02266536 -0.32526324 -0.3169102\n",
      " -0.32421915]\n",
      "Last-step D: [9.74431978e-01 5.80976392e-04 0.00000000e+00 5.66896360e-04\n",
      " 8.11406459e-03 8.18264091e-03 8.12344371e-03] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 1.01703995  0.0038855   0.00579403 -0.02271872 -0.32602645 -0.31767986\n",
      " -0.32498325]\n",
      "Last-step D: [9.75949744e-01 5.46896534e-04 0.00000000e+00 5.33642197e-04\n",
      " 7.63217188e-03 7.69660131e-03 7.64094437e-03] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 1.01931     0.00383384  0.00579403 -0.02276912 -0.32674682 -0.3184063\n",
      " -0.32570444]\n",
      "Last-step D: [9.77299463e-01 5.16546387e-04 0.00000000e+00 5.04027399e-04\n",
      " 7.20365995e-03 7.26440669e-03 7.21189690e-03] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 1.02145925  0.00378491  0.00579403 -0.02281687 -0.32742883 -0.31909406\n",
      " -0.32638723]\n",
      "Last-step D: [9.78507463e-01 4.89348201e-04 0.00000000e+00 4.77488210e-04\n",
      " 6.82016156e-03 6.87761648e-03 6.82792241e-03] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 1.02349977  0.00373842  0.00579403 -0.02286223 -0.32807633 -0.31974701\n",
      " -0.32703546]\n",
      "Last-step D: [9.79594858e-01 4.64837416e-04 0.00000000e+00 4.53571320e-04\n",
      " 6.47496775e-03 6.52946300e-03 6.48230268e-03] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 1.02544189  0.00369416  0.00579403 -0.02290542 -0.32869259 -0.32036845\n",
      " -0.32765242]\n",
      "Last-step D: [9.80578773e-01 4.42636275e-04 0.00000000e+00 4.31908123e-04\n",
      " 6.16263689e-03 6.21445710e-03 6.16958865e-03] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 1.02729457  0.00365192  0.00579403 -0.02294664 -0.32928047 -0.32096126\n",
      " -0.32824095]\n",
      "Last-step D: [9.81473246e-01 4.22434487e-04 0.00000000e+00 4.12195842e-04\n",
      " 5.87870931e-03 5.92810019e-03 5.88531459e-03] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 1.02906558  0.00361152  0.00579403 -0.02298606 -0.32984242 -0.32152793\n",
      " -0.32880353]\n",
      "Last-step D: [9.82289889e-01 4.03974826e-04 0.00000000e+00 3.94183483e-04\n",
      " 5.61949561e-03 5.66667072e-03 5.62578614e-03] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 1.03076174  0.00357282  0.00579403 -0.02302382 -0.33038061 -0.32207064\n",
      " -0.32934232]\n",
      "Last-step D: [9.83038395e-01 3.87042274e-04 0.00000000e+00 3.77661236e-04\n",
      " 5.38191748e-03 5.42706358e-03 5.38792087e-03] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 1.03238905  0.00353567  0.00579403 -0.02306007 -0.33089695 -0.3225913\n",
      " -0.32985923]\n",
      "Last-step D: [9.83726911e-01 3.71455715e-04 0.00000000e+00 3.62452371e-04\n",
      " 5.16338634e-03 5.20666769e-03 5.16912677e-03] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 1.03395281  0.00349996  0.00579403 -0.02309491 -0.33139312 -0.32309163\n",
      " -0.33035595]\n",
      "Last-step D: [9.84362342e-01 3.57061514e-04 0.00000000e+00 3.48406977e-04\n",
      " 4.96170981e-03 5.00327162e-03 4.96720857e-03] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 1.03545775  0.00346559  0.00579403 -0.02312845 -0.33187062 -0.32357313\n",
      " -0.33083398]\n",
      "Last-step D: [9.84950571e-01 3.43728507e-04 0.00000000e+00 3.35397066e-04\n",
      " 4.77501880e-03 4.81499008e-03 4.78029472e-03] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 1.03690809  0.00343246  0.00579403 -0.02316078 -0.33233079 -0.32403715\n",
      " -0.33129466]\n",
      "Last-step D: [9.85496647e-01 3.31344042e-04 0.00000000e+00 3.23312714e-04\n",
      " 4.60171014e-03 4.64020598e-03 4.60677995e-03] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 1.0383076   0.00340048  0.00579403 -0.02319199 -0.33277483 -0.3244849\n",
      " -0.33173919]\n",
      "Last-step D: [9.86004925e-01 3.19810828e-04 0.00000000e+00 3.12058988e-04\n",
      " 4.44040092e-03 4.47752446e-03 4.44527959e-03] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 1.03965968  0.00336957  0.00579403 -0.02322214 -0.33320382 -0.32491748\n",
      " -0.33216865]\n",
      "Last-step D: [9.86479181e-01 3.09044407e-04 0.00000000e+00 3.01553477e-04\n",
      " 4.28989203e-03 4.32573601e-03 4.29459294e-03] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 1.04096741  0.00333967  0.00579403 -0.02325131 -0.33361873 -0.32533586\n",
      " -0.33258402]\n",
      "Last-step D: [9.86922705e-01 2.98971113e-04 0.00000000e+00 2.91724298e-04\n",
      " 4.14913862e-03 4.18378675e-03 4.15367382e-03] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 1.04223357  0.00331072  0.00579403 -0.02327956 -0.33402046 -0.32574093\n",
      " -0.33298618]\n",
      "Last-step D: [9.87338378e-01 2.89526397e-04 0.00000000e+00 2.82508466e-04\n",
      " 4.01722607e-03 4.05075418e-03 4.02160646e-03] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 1.0434607   0.00328266  0.00579403 -0.02330695 -0.33440979 -0.32613351\n",
      " -0.33337594]\n",
      "Last-step D: [9.87728732e-01 2.80653466e-04 0.00000000e+00 2.73850566e-04\n",
      " 3.89335040e-03 3.92582736e-03 3.89758584e-03] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 1.0446511   0.00325543  0.00579403 -0.02333352 -0.33478747 -0.32651434\n",
      " -0.33375403]\n",
      "Last-step D: [9.88096002e-01 2.72302154e-04 0.00000000e+00 2.65701644e-04\n",
      " 3.77680203e-03 3.80829057e-03 3.78090146e-03] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 1.04580688  0.00322898  0.00579403 -0.02335932 -0.33515417 -0.32688409\n",
      " -0.33412112]\n",
      "Last-step D: [9.88442168e-01 2.64427981e-04 0.00000000e+00 2.58018300e-04\n",
      " 3.66695230e-03 3.69750978e-03 3.67092390e-03] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 1.04692998  0.00320328  0.00579403 -0.0233844  -0.33551049 -0.32724339\n",
      " -0.33447783]\n",
      "Last-step D: [9.88768990e-01 2.56991372e-04 0.00000000e+00 2.50761918e-04\n",
      " 3.56324234e-03 3.59292126e-03 3.56709356e-03] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 1.04802218  0.00317829  0.00579403 -0.02340879 -0.33585701 -0.32759279\n",
      " -0.33482472]\n",
      "Last-step D: [9.89078038e-01 2.49957003e-04 0.00000000e+00 2.43898028e-04\n",
      " 3.46517360e-03 3.49402219e-03 3.46891128e-03] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 1.04908511  0.00315396  0.00579403 -0.02343253 -0.33619424 -0.32793282\n",
      " -0.33516231]\n",
      "Last-step D: [9.89370718e-01 2.43293240e-04 0.00000000e+00 2.37395765e-04\n",
      " 3.37229998e-03 3.40036262e-03 3.37593039e-03] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 1.05012028  0.00313026  0.00579403 -0.02345565 -0.33652266 -0.32826398\n",
      " -0.33549109]\n",
      "Last-step D: [9.89648291e-01 2.36971677e-04 0.00000000e+00 2.31227409e-04\n",
      " 3.28422109e-03 3.31153873e-03 3.28775000e-03] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 1.05112909  0.00310717  0.00579403 -0.02347819 -0.33684272 -0.3285867\n",
      " -0.33581149]\n",
      "Last-step D: [9.89911892e-01 2.30966730e-04 0.00000000e+00 2.25367997e-04\n",
      " 3.20057657e-03 3.22718704e-03 3.20400930e-03] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 1.05211283  0.00308464  0.00579403 -0.02350017 -0.33715482 -0.3289014\n",
      " -0.33612393]\n",
      "Last-step D: [9.90162546e-01 2.25255298e-04 0.00000000e+00 2.19794986e-04\n",
      " 3.12104118e-03 3.14697955e-03 3.12438267e-03] \n",
      "\n",
      "Initial reward vector: [ 0.01047936  0.00156276  0.0142492  -0.00288426 -0.01043665  0.00401116\n",
      "  0.01449982]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.01047936  0.00156276  0.1142492  -0.00288426 -0.04169206 -0.02933694\n",
      " -0.02089668]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 4.08590698e-11 0.00000000e+00\n",
      " 3.12554048e-01 3.33480965e-01 3.53964987e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.01047936  0.00156276  0.2142492  -0.00288426 -0.07328647 -0.06269344\n",
      " -0.05594577]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 7.69078036e-10 0.00000000e+00\n",
      " 3.15944107e-01 3.33564992e-01 3.50490900e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.01047936  0.00156276  0.3142492  -0.00288426 -0.10516474 -0.09605536\n",
      " -0.09070558]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 1.44685120e-08 0.00000000e+00\n",
      " 3.18782649e-01 3.33619190e-01 3.47598146e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.01047936  0.00156276  0.41424917 -0.00288426 -0.1372805  -0.12942062\n",
      " -0.12522453]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 2.72093163e-07 0.00000000e+00\n",
      " 3.21157630e-01 3.33652590e-01 3.45189507e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.01047936  0.00156276  0.51424866 -0.00288426 -0.16959479 -0.16278753\n",
      " -0.15954282]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 5.11550539e-06 0.00000000e+00\n",
      " 3.23142921e-01 3.33669140e-01 3.43182823e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.01047936  0.00156276  0.61423905 -0.00288426 -0.20207303 -0.19615082\n",
      " -0.19369168]\n",
      "Last-step D: [0.00000000e+00 0.00000000e+00 9.61100949e-05 0.00000000e+00\n",
      " 3.24782375e-01 3.33632890e-01 3.41488625e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.01047936  0.00156276  0.71405985 -0.00288426 -0.23464433 -0.22943963\n",
      " -0.22765237]\n",
      "Last-step D: [0.         0.         0.001792   0.         0.325713   0.33288809\n",
      " 0.33960691] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.01047936  0.00156276  0.81100358 -0.00288426 -0.26646911 -0.2616136\n",
      " -0.26059734]\n",
      "Last-step D: [0.         0.         0.03056276 0.         0.3182478  0.32173972\n",
      " 0.32944972] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.01047936  0.00156276  0.88246542 -0.00288426 -0.29021217 -0.28490495\n",
      " -0.28502477]\n",
      "Last-step D: [0.         0.         0.28538156 0.         0.23743064 0.23291352\n",
      " 0.24427427] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.01047936  0.00156276  0.91425708 -0.00288426 -0.30086655 -0.2951214\n",
      " -0.29594561]\n",
      "Last-step D: [0.         0.         0.68208337 0.         0.1065438  0.10216444\n",
      " 0.10920839] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.01047936  0.00156276  0.93245853 -0.00288426 -0.30698033 -0.30095096\n",
      " -0.30220372]\n",
      "Last-step D: [0.         0.         0.8179855  0.         0.06113775 0.05829562\n",
      " 0.06258113] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "Reward vector: [ 0.01047936  0.00156276  0.9451146  -0.00288426 -0.31123533 -0.3049997\n",
      " -0.30655604]\n",
      "Last-step D: [0.         0.         0.87343935 0.         0.04255005 0.04048746\n",
      " 0.04352315] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.01047936  0.00156276  0.95479329 -0.00288426 -0.31449099 -0.30809423\n",
      " -0.30988454]\n",
      "Last-step D: [0.         0.         0.9032131  0.         0.03255654 0.03094529\n",
      " 0.03328507] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.01047936  0.00156276  0.96261856 -0.00288426 -0.31712406 -0.31059539\n",
      " -0.31257558]\n",
      "Last-step D: [0.         0.         0.9217473  0.         0.02633078 0.02501154\n",
      " 0.02691038] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.01047936  0.00156276  0.96918042 -0.00288426 -0.31933254 -0.31269231\n",
      " -0.31483203]\n",
      "Last-step D: [0.         0.         0.93438142 0.         0.02208478 0.02096928\n",
      " 0.02256452] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.01047936  0.00156276  0.97482651 -0.00288426 -0.32123314 -0.31449637\n",
      " -0.31677347]\n",
      "Last-step D: [0.         0.         0.94353905 0.         0.01900603 0.01804056\n",
      " 0.01941436] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.01047936  0.00156276  0.97977874 -0.00288426 -0.32290041 -0.31607859\n",
      " -0.31847622]\n",
      "Last-step D: [0.         0.         0.95047771 0.         0.01667265 0.01582215\n",
      " 0.01702748] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.01047936  0.00156276  0.9841873  -0.00288426 -0.32438481 -0.31748702\n",
      " -0.31999195]\n",
      "Last-step D: [0.         0.         0.9559144  0.         0.01484398 0.01408433\n",
      " 0.01515729] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.01047936  0.00156276  0.98815854 -0.00288426 -0.32572208 -0.31875569\n",
      " -0.32135724]\n",
      "Last-step D: [0.         0.         0.96028763 0.         0.01337276 0.01268666\n",
      " 0.01365295] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 0.01047936  0.00156276  0.99177048 -0.00288426 -0.32693847 -0.31990954\n",
      " -0.32259895]\n",
      "Last-step D: [0.         0.         0.96388061 0.         0.01216385 0.01153851\n",
      " 0.01241703] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 0.01047936  0.00156276  0.99508205 -0.00288426 -0.32805378 -0.32096741\n",
      " -0.32373733]\n",
      "Last-step D: [0.         0.         0.96688431 0.         0.01115309 0.01057875\n",
      " 0.01138384] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 0.01047936  0.00156276  0.99813883 -0.00288426 -0.32908334 -0.32194388\n",
      " -0.32478808]\n",
      "Last-step D: [0.         0.         0.96943219 0.         0.01029563 0.00976471\n",
      " 0.01050747] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 0.01047936  0.00156276  1.00097679 -0.00288426 -0.33003926 -0.32285045\n",
      " -0.32576356]\n",
      "Last-step D: [0.         0.         0.97162031 0.         0.00955917 0.00906565\n",
      " 0.00975486] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 0.01047936  0.00156276  1.00362484 -0.00288426 -0.33093125 -0.32369634\n",
      " -0.32667373]\n",
      "Last-step D: [0.         0.         0.97351955 0.         0.00891989 0.00845892\n",
      " 0.00910164] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 0.01047936  0.00156276  1.00610651 -0.00288426 -0.33176723 -0.32448908\n",
      " -0.32752667]\n",
      "Last-step D: [0.         0.         0.97518335 0.         0.00835982 0.00792743\n",
      " 0.00852941] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 0.01047936  0.00156276  1.00844123 -0.00288426 -0.33255374 -0.32523489\n",
      " -0.32832907]\n",
      "Last-step D: [0.         0.         0.97665276 0.         0.00786515 0.00745804\n",
      " 0.00802405] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 0.01047936  0.00156276  1.01064525 -0.00288426 -0.33329625 -0.32593894\n",
      " -0.32908653]\n",
      "Last-step D: [0.         0.         0.97795983 0.         0.0074251  0.00704053\n",
      " 0.00757454] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 0.01047936  0.00156276  1.01273225 -0.00288426 -0.33399937 -0.32660562\n",
      " -0.32980374]\n",
      "Last-step D: [0.         0.         0.97912994 0.         0.00703115 0.00666677\n",
      " 0.00717214] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 0.01047936  0.00156276  1.01471391 -0.00288426 -0.33466701 -0.32723864\n",
      " -0.33048473]\n",
      "Last-step D: [0.         0.         0.98018347 0.         0.00667642 0.00633026\n",
      " 0.00680985] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 0.01047936  0.00156276  1.01660021 -0.00288426 -0.33530255 -0.32784122\n",
      " -0.33113292]\n",
      "Last-step D: [0.         0.         0.98113693 0.         0.00635538 0.00602572\n",
      " 0.00648197] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 0.01047936  0.00156276  1.01839982 -0.00288426 -0.33590889 -0.3284161\n",
      " -0.33175131]\n",
      "Last-step D: [0.         0.         0.98200388 0.         0.00606345 0.00574881\n",
      " 0.00618385] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02012027 -0.00288426 -0.33648858 -0.32896569\n",
      " -0.33234247]\n",
      "Last-step D: [0.         0.         0.98279554 0.         0.00579687 0.00549596\n",
      " 0.00591164] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02176815 -0.00288426 -0.33704383 -0.32949211\n",
      " -0.33290868]\n",
      "Last-step D: [0.         0.         0.98352125 0.         0.00555248 0.00526417\n",
      " 0.0056621 ] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02334926 -0.00288426 -0.33757659 -0.3299972\n",
      " -0.33345194]\n",
      "Last-step D: [0.         0.         0.9841889  0.         0.00532764 0.00505092\n",
      " 0.00543253] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02486874 -0.00288426 -0.3380886  -0.33048261\n",
      " -0.333974  ]\n",
      "Last-step D: [0.         0.         0.98480516 0.         0.0051201  0.0048541\n",
      " 0.00522065] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02633117 -0.00288426 -0.3385814  -0.3309498\n",
      " -0.33447645]\n",
      "Last-step D: [0.         0.         0.98537571 0.         0.00492795 0.00467187\n",
      " 0.00502448] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02774062 -0.00288426 -0.33905635 -0.33140007\n",
      " -0.33496068]\n",
      "Last-step D: [0.         0.         0.98590544 0.         0.00474954 0.00450268\n",
      " 0.00484235] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 0.01047936  0.00156276  1.02910077 -0.00288426 -0.3395147  -0.33183458\n",
      " -0.33542796]\n",
      "Last-step D: [0.         0.         0.98639856 0.         0.00458345 0.00434518\n",
      " 0.00467281] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 0.01047936  0.00156276  1.0304149  -0.00288426 -0.33995754 -0.3322544\n",
      " -0.33587942]\n",
      "Last-step D: [0.         0.         0.98685872 0.         0.00442846 0.00419821\n",
      " 0.00451461] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03168599 -0.00288426 -0.34038589 -0.33266048\n",
      " -0.33631609]\n",
      "Last-step D: [0.         0.         0.98728911 0.         0.0042835  0.00406075\n",
      " 0.00436664] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03291674 -0.00288426 -0.34080065 -0.33305367\n",
      " -0.33673889]\n",
      "Last-step D: [0.         0.         0.9876925  0.         0.00414763 0.00393191\n",
      " 0.00422796] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 0.01047936  0.00156276  1.0341096  -0.00288426 -0.34120266 -0.33343476\n",
      " -0.33714866]\n",
      "Last-step D: [0.         0.         0.98807136 0.         0.00402001 0.00381091\n",
      " 0.00409772] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03526682 -0.00288426 -0.34159265 -0.33380447\n",
      " -0.33754617]\n",
      "Last-step D: [0.         0.         0.98842784 0.         0.00389993 0.00369705\n",
      " 0.00397517] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03639043 -0.00288426 -0.34197132 -0.33416344\n",
      " -0.33793214]\n",
      "Last-step D: [0.         0.         0.98876387 0.         0.00378674 0.00358973\n",
      " 0.00385966] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03748231 -0.00288426 -0.34233931 -0.33451228\n",
      " -0.3383072 ]\n",
      "Last-step D: [0.         0.         0.98908114 0.         0.00367987 0.0034884\n",
      " 0.0037506 ] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 0.01047936  0.00156276  1.0385442  -0.00288426 -0.34269719 -0.33485154\n",
      " -0.33867195]\n",
      "Last-step D: [0.         0.         0.98938118 0.         0.0035788  0.00339257\n",
      " 0.00364746] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 0.01047936  0.00156276  1.03957766 -0.00288426 -0.3430455  -0.33518172\n",
      " -0.33902692]\n",
      "Last-step D: [0.         0.         0.98966535 0.         0.00348307 0.00330181\n",
      " 0.00354978] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 0.01047936  0.00156276  1.04058418 -0.00288426 -0.34338472 -0.33550329\n",
      " -0.33937264]\n",
      "Last-step D: [0.         0.         0.98993486 0.         0.00339228 0.00321572\n",
      " 0.00345714] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 0.01047936  0.00156276  1.04156509 -0.00288426 -0.34371533 -0.33581669\n",
      " -0.33970955]\n",
      "Last-step D: [0.         0.         0.99019083 0.         0.00330605 0.00313397\n",
      " 0.00336915] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 0.01047936  0.00156276  1.04252167 -0.00288426 -0.34403773 -0.33612231\n",
      " -0.3400381 ]\n",
      "Last-step D: [0.         0.         0.99043424 0.         0.00322404 0.00305623\n",
      " 0.00328549] \n",
      "\n",
      "Final reward weights, at s_0:  [ 1.05211283  0.00308464  0.00579403 -0.02350017 -0.33715482 -0.3289014\n",
      " -0.33612393]\n",
      "Final reward weights, at s_2:  [ 0.01047936  0.00156276  1.04252167 -0.00288426 -0.34403773 -0.33612231\n",
      " -0.3400381 ]\n"
     ]
    }
   ],
   "source": [
    "def test_water(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_water()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec_1 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[2]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[2]=1\n",
    "    \n",
    "    r_vec_2 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    print('Final reward weights, at s_0: ', r_vec_1)\n",
    "    print('Final reward weights, at s_2: ', r_vec_2)\n",
    "\n",
    "test_water()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Max Causal Entropy (Python 3)",
   "language": "python",
   "name": "max-causal-ent-p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
