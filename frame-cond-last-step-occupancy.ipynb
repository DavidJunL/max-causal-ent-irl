{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import FrozenLakeEnv\n",
    "from mdps import MDP, MDPOneTimeR, MDP_toy_irreversibility\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  0.   0.5]\n",
      "[ 0.25  0.    0.25  0.5 ]\n",
      "[ 0.     0.25   0.125  0.625]\n",
      "[ 0.125   0.0625  0.1875  0.625 ]\n",
      "[ 0.03125  0.15625  0.125    0.6875 ]\n",
      "[ 0.078125  0.078125  0.140625  0.703125]\n",
      "[ 0.0390625  0.109375   0.109375   0.7421875]\n",
      "[ 0.0546875   0.07421875  0.109375    0.76171875]\n",
      "[ 0.03710938  0.08203125  0.09179688  0.7890625 ]\n",
      "[ 0.04101562  0.06445312  0.08691406  0.80761719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04101562,  0.06445312,  0.08691406,  0.80761719])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        D = np.zeros_like(P_0)\n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:\n",
    "                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "\n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "mdp = MDP_toy_irreversibility(FrozenLakeEnv(is_slippery=False))    \n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vector=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vector is None:\n",
    "        r_vector = np.random.rand(mdp.nS)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vector, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vector = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vector = r_vector - learning_rate * dL_dr_vector\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vector))\n",
    "            print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(temperature_irl=1,\n",
    "         horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         learning_rate=0.1,\n",
    "         epochs = 100):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    mdp = MDP_toy_irreversibility(FrozenLakeEnv(is_slippery=False))    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vector = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vector)\n",
    "    return r_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Reward vector: [ 0.64400728  0.70314654  0.58614588  0.47834972]\n",
      "Policy: [[ 0.74284619  0.25715381]\n",
      " [ 0.61641022  0.38358978]\n",
      " [ 0.51587926  0.48412074]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.04806221  0.12042822  0.16617495  0.66533462] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.73510278  0.67916069  0.55333659  0.44404937]\n",
      "Policy: [[ 0.89351094  0.10648906]\n",
      " [ 0.63535964  0.36464036]\n",
      " [ 0.52018442  0.47981558]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.08904507  0.23985854  0.32809293  0.34300347] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.82418979  0.65152885  0.5177901   0.41814069]\n",
      "Policy: [[ 0.92787014  0.07212986]\n",
      " [ 0.61158928  0.38841072]\n",
      " [ 0.51230502  0.48769498]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.10912983  0.27631845  0.35546486  0.25908686] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.91150979  0.62184287  0.48225249  0.39604428]\n",
      "Policy: [[ 0.94396407  0.05603593]\n",
      " [ 0.58113397  0.41886603]\n",
      " [ 0.50242868  0.49757132]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.12680004  0.29685978  0.35537612  0.22096405] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.99703129  0.59057029  0.447561    0.37648685]\n",
      "Policy: [[ 0.9545322   0.0454678 ]\n",
      " [ 0.54873934  0.45126066]\n",
      " [ 0.49189357  0.50810643]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.14478501  0.31272575  0.3469149   0.19557434] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 1.08066168  0.55793899  0.41407239  0.35897637]\n",
      "Policy: [[ 0.96248973  0.03751027]\n",
      " [ 0.51585506  0.48414494]\n",
      " [ 0.4810727   0.5189273 ]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.16369607  0.32631306  0.33488607  0.1751048 ] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 1.16229151  0.52412876  0.38196474  0.34326442]\n",
      "Policy: [[ 0.96882676  0.03117324]\n",
      " [ 0.48325461  0.51674539]\n",
      " [ 0.47013699  0.52986301]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.18370173  0.33810228  0.32107654  0.15711946] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 1.24180853  0.48932879  0.35132708  0.32918503]\n",
      "Policy: [[ 0.97398629  0.02601371]\n",
      " [ 0.45147538  0.54852462]\n",
      " [ 0.45918881  0.54081119]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.20482978  0.34799968  0.30637662  0.14079392] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 1.31910364  0.45375355  0.32219264  0.31659961]\n",
      "Policy: [[ 0.9782221   0.0217779 ]\n",
      " [ 0.4209223   0.5790777 ]\n",
      " [ 0.44829658  0.55170342]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.22704895  0.35575247  0.29134441  0.12585417] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 1.39407459  0.41764143  0.29455495  0.30537846]\n",
      "Policy: [[ 0.98171018  0.01828982]\n",
      " [ 0.39189519  0.60810481]\n",
      " [ 0.43750672  0.56249328]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.25029041  0.3611212   0.2763769   0.11221149] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 1.46662969  0.38124541  0.26837848  0.29539586]\n",
      "Policy: [[ 0.98458674  0.01541326]\n",
      " [ 0.36459895  0.63540105]\n",
      " [ 0.42685024  0.57314976]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.27444908  0.36396014  0.26176469  0.09982609] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 1.53669161  0.34482056  0.24360717  0.28653009]\n",
      "Policy: [[ 0.98696227  0.01303773]\n",
      " [ 0.33915286  0.66084714]\n",
      " [ 0.41634816  0.58365184]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.29938074  0.36424855  0.2477131   0.08865761] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 1.60420132  0.30861153  0.22017156  0.27866502]\n",
      "Policy: [[ 0.98892764  0.01107236]\n",
      " [ 0.3156024   0.6843976 ]\n",
      " [ 0.40601623  0.59398377]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.32490288  0.36209031  0.23435608  0.07865074] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 1.66912114  0.27284214  0.19799459  0.27169155]\n",
      "Policy: [[ 0.99055751  0.00944249]\n",
      " [ 0.29393316  0.70606684]\n",
      " [ 0.3958685   0.6041315 ]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.35080182  0.35769381  0.22176964  0.06973473] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 1.73143661  0.23770798  0.17699606  0.26550878]\n",
      "Policy: [[ 0.991913    0.008087  ]\n",
      " [ 0.27408519  0.72591481]\n",
      " [ 0.38591944  0.61408056]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.37684528  0.35134169  0.20998533  0.0618277 ] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 1.79115687  0.20337214  0.15709581  0.26002461]\n",
      "Policy: [[ 0.99304391  0.00695609]\n",
      " [ 0.25596654  0.74403346]\n",
      " [ 0.3761847   0.6238153 ]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.40279742  0.34335837  0.19900249  0.05484172] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 1.84831353  0.16996407  0.13821595  0.25515588]\n",
      "Policy: [[ 0.99399071  0.00600929]\n",
      " [ 0.23946497  0.76053503]\n",
      " [ 0.36668073  0.63331927]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.42843344  0.33408065  0.18879864  0.04868727] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 1.9029584   0.13758063  0.1202822   0.25082821]\n",
      "Policy: [[ 0.99478625  0.00521375]\n",
      " [ 0.22445745  0.77554255]\n",
      " [ 0.35742387  0.64257613]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.45355132  0.32383447  0.17933752  0.04327669] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 1.95516043  0.10628875  0.10322469  0.24697556]\n",
      "Policy: [[ 0.99545722  0.00454278]\n",
      " [ 0.21081749  0.78918251]\n",
      " [ 0.34842925  0.65157075]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.47797967  0.31291876  0.17057503  0.03852655] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 2.00500227  0.07612917  0.08697835  0.24353964]\n",
      "Policy: [[ 0.99602528  0.00397472]\n",
      " [ 0.19842039  0.80157961]\n",
      " [ 0.33970983  0.66029017]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.50158161  0.30159579  0.1624634   0.0343592 ] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 2.05257676  0.04712045  0.07148295  0.24046927]\n",
      "Policy: [[ 0.99650808  0.00349192]\n",
      " [ 0.18714683  0.81285317]\n",
      " [ 0.33127569  0.66872431]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.52425511  0.2900872   0.15495403  0.03070366] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 2.09798367  0.01926306  0.05668303  0.23771968]\n",
      "Policy: [[ 0.99692001  0.00307999]\n",
      " [ 0.17688502  0.82311498]\n",
      " [ 0.32313367  0.67686633]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.54593091  0.27857395  0.14799925  0.0274959 ] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 2.14132679 -0.00745684  0.04252768  0.2352518 ]\n",
      "Policy: [[ 0.99727285  0.00272715]\n",
      " [ 0.16753192  0.83246808]\n",
      " [ 0.31528737  0.68471263]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.56656873  0.26719893  0.14155351  0.02467883] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 2.18271151 -0.03306395  0.02897028  0.2330316 ]\n",
      "Policy: [[ 0.99757624  0.00242376]\n",
      " [ 0.15899368  0.84100632]\n",
      " [ 0.30773724  0.69226276]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.58615285  0.25607118  0.13557396  0.02220201] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 2.22224277 -0.05759102  0.0159682   0.23102948]\n",
      "Policy: [[ 0.99783811  0.00216189]\n",
      " [ 0.15118555  0.84881445]\n",
      " [ 0.30048099  0.69951901]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.60468738  0.24527066  0.13002078  0.02002117] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 2.26002358 -0.08107634  0.00348247  0.22921971]\n",
      "Policy: [[ 0.998065    0.001935  ]\n",
      " [ 0.1440315   0.8559685 ]\n",
      " [ 0.29351393  0.70648607]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.62219185  0.23485318  0.12485729  0.01809768] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 2.29615386 -0.10356183 -0.00852251  0.22757992]\n",
      "Policy: [[ 0.9982623   0.0017377 ]\n",
      " [ 0.13746366  0.86253634]\n",
      " [ 0.28682944  0.71317056]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.63869727  0.22485492  0.12004986  0.01639795] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 2.33072959 -0.12509148 -0.0200793   0.22609062]\n",
      "Policy: [[ 0.9984345   0.0015655 ]\n",
      " [ 0.13142161  0.86857839]\n",
      " [ 0.28041942  0.71958058]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.65424271  0.21529653  0.11556784  0.01489291] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 2.36384233 -0.14571015 -0.03121763  0.22473488]\n",
      "Policy: [[ 0.99858532  0.00141468]\n",
      " [ 0.12585172  0.87414828]\n",
      " [ 0.27427463  0.72572537]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.66887261  0.20618664  0.11138335  0.01355741] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 2.39557888 -0.16546262 -0.04196474  0.22349791]\n",
      "Policy: [[ 0.99871787  0.00128213]\n",
      " [ 0.1207064   0.8792936 ]\n",
      " [ 0.26838507  0.73161493]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.68263446  0.19752474  0.10747108  0.01236972] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 2.42602116 -0.18439298 -0.05234555  0.2223668 ]\n",
      "Policy: [[ 0.99883476  0.00116524]\n",
      " [ 0.11594349  0.88405651]\n",
      " [ 0.26274026  0.73725974]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.69557717  0.18930362  0.1038081   0.0113111 ] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 2.45524619 -0.20254411 -0.06238292  0.22133026]\n",
      "Policy: [[ 0.99893816  0.00106184]\n",
      " [ 0.11152557  0.88847443]\n",
      " [ 0.25732948  0.74267052]\n",
      " [ 0.5         0.5       ]]\n",
      "Last-step D: [ 0.70774973  0.18151123  0.10037365  0.01036538] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 2.48332616 -0.21995733 -0.07209781  0.2203784 ]\n",
      "Policy: [[  9.99029924e-01   9.70075717e-04]\n",
      " [  1.07419498e-01   8.92580502e-01]\n",
      " [  2.52141986e-01   7.47858014e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.71920028  0.1741322   0.09714892  0.0095186 ] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 2.51032862 -0.23667223 -0.0815095   0.21950254]\n",
      "Policy: [[  9.99111603e-01   8.88396876e-04]\n",
      " [  1.03595821e-01   8.96404179e-01]\n",
      " [  2.47167112e-01   7.52832888e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.7299754   0.16714902  0.0941169   0.00875868] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 2.53631665 -0.25272652 -0.09063571  0.21869502]\n",
      "Policy: [[  9.99184517e-01   8.15482551e-04]\n",
      " [  1.00028376e-01   8.99971624e-01]\n",
      " [  2.42394423e-01   7.57605577e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.74011975  0.1605429   0.09126216  0.00807519] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 2.56134908 -0.26815597 -0.09949279  0.21794911]\n",
      "Policy: [[  9.99249790e-01   7.50209724e-04]\n",
      " [  9.66938779e-02   9.03306122e-01]\n",
      " [  2.37813788e-01   7.62186212e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.7496757   0.15429448  0.08857075  0.00745908] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 2.58548075 -0.28299439 -0.10809579  0.21725886]\n",
      "Policy: [[  9.99308380e-01   6.91619808e-04]\n",
      " [  9.35715743e-02   9.06428426e-01]\n",
      " [  2.33415436e-01   7.66584564e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.75868323  0.14838428  0.08602998  0.00690251] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 2.60876277 -0.29727371 -0.11645863  0.21661899]\n",
      "Policy: [[  9.99361108e-01   6.38891981e-04]\n",
      " [  9.06429389e-02   9.09357061e-01]\n",
      " [  2.29189997e-01   7.70810003e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.76717985  0.14279311  0.08362838  0.00639866] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 2.6312427  -0.31102394 -0.12459417  0.21602483]\n",
      "Policy: [[  9.99408679e-01   5.91321194e-04]\n",
      " [  8.78914050e-02   9.12108595e-01]\n",
      " [  2.25128524e-01   7.74871476e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.77520063  0.1375023   0.08135548  0.00594159] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 2.65296488 -0.32427332 -0.13251435  0.21547222]\n",
      "Policy: [[  9.99451700e-01   5.48299997e-04]\n",
      " [  8.53021319e-02   9.14697868e-01]\n",
      " [  2.21222509e-01   7.78777491e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.78277823  0.13249384  0.0792018   0.00552613] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 2.67397058 -0.33704837 -0.14023022  0.21495745]\n",
      "Policy: [[  9.99490697e-01   5.09303457e-04]\n",
      " [  8.28618005e-02   9.17138200e-01]\n",
      " [  2.17463875e-01   7.82536125e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.78994301  0.12775055  0.07715869  0.00514775] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 2.69429827 -0.34937398 -0.14775205  0.2144772 ]\n",
      "Policy: [[  9.99526123e-01   4.73876629e-04]\n",
      " [  8.05584355e-02   9.19441565e-01]\n",
      " [  2.13844984e-01   7.86155016e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.79672315  0.12325611  0.07521825  0.00480249] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 2.71398379 -0.36127349 -0.15508938  0.21402851]\n",
      "Policy: [[  9.99558376e-01   4.41624096e-04]\n",
      " [  7.83812499e-02   9.21618750e-01]\n",
      " [  2.10358613e-01   7.89641387e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.80314474  0.1189951   0.0733733   0.00448686] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 2.7330606  -0.37276879 -0.1622511   0.21360873]\n",
      "Policy: [[  9.99587799e-01   4.12201229e-04]\n",
      " [  7.63205090e-02   9.23679491e-01]\n",
      " [  2.06997952e-01   7.93002048e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.80923194  0.11495299  0.07161725  0.00419782] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 2.75155989 -0.38388041 -0.16924551  0.21321546]\n",
      "Policy: [[  9.99614693e-01   3.85306847e-04]\n",
      " [  7.43674115e-02   9.25632588e-01]\n",
      " [  2.03756578e-01   7.96243422e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.81500711  0.11111614  0.06994409  0.00393265] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 2.76951079 -0.39462759 -0.17608034  0.21284657]\n",
      "Policy: [[  9.99639323e-01   3.60677051e-04]\n",
      " [  7.25139860e-02   9.27486014e-01]\n",
      " [  2.00628447e-01   7.99371553e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.82049091  0.1074718   0.06834831  0.00368898] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 2.78694055 -0.40502839 -0.18276283  0.2125001 ]\n",
      "Policy: [[  9.99661920e-01   3.38080016e-04]\n",
      " [  7.07529990e-02   9.29247001e-01]\n",
      " [  1.97607864e-01   8.02392136e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.82570245  0.104008    0.06682484  0.00346471] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 2.80387461 -0.41509975 -0.18929973  0.2121743 ]\n",
      "Policy: [[  9.99682688e-01   3.17311585e-04]\n",
      " [  6.90778758e-02   9.30922124e-01]\n",
      " [  1.94689475e-01   8.05310525e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.83065938  0.10071361  0.06536904  0.00325796] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 2.8203368  -0.42485757 -0.1956974   0.21186759]\n",
      "Policy: [[  9.99701808e-01   2.98191538e-04]\n",
      " [  6.74826296e-02   9.32517370e-01]\n",
      " [  1.91868241e-01   8.08131759e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.83537806  0.09757821  0.06397664  0.00306708] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 2.83634944 -0.43431678 -0.20196177  0.21157853]\n",
      "Policy: [[  9.99719440e-01   2.80560409e-04]\n",
      " [  6.59617998e-02   9.34038200e-01]\n",
      " [  1.89139422e-01   8.10860578e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.8398736   0.09459211  0.0626437   0.00289059] \n",
      "\n",
      "Epoch 50\n",
      "Reward vector: [ 2.85193345 -0.44349141 -0.20809843  0.21130581]\n",
      "Policy: [[  9.99735723e-01   2.64276777e-04]\n",
      " [  6.45103976e-02   9.35489602e-01]\n",
      " [  1.86498558e-01   8.13501442e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.84415997  0.09174626  0.06136659  0.00272717] \n",
      "\n",
      "Epoch 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward vector: [ 2.86710843 -0.45239463 -0.21411262  0.21104825]\n",
      "Policy: [[  9.99750785e-01   2.49214948e-04]\n",
      " [  6.31238577e-02   9.36876142e-01]\n",
      " [  1.83941454e-01   8.16058546e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.84825013  0.08903224  0.06014197  0.00257565] \n",
      "\n",
      "Epoch 52\n",
      "Reward vector: [ 2.88189283 -0.46103885 -0.2200093   0.21080475]\n",
      "Policy: [[  9.99764737e-01   2.35262968e-04]\n",
      " [  6.17979957e-02   9.38202004e-01]\n",
      " [  1.81464160e-01   8.18535840e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.85215608  0.08644222  0.05896673  0.00243498] \n",
      "\n",
      "Epoch 53\n",
      "Reward vector: [ 2.89630393 -0.46943574 -0.2257931   0.21057433]\n",
      "Policy: [[  9.99777679e-01   2.22320912e-04]\n",
      " [  6.05289705e-02   9.39471030e-01]\n",
      " [  1.79062959e-01   8.20937041e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.85588892  0.08396887  0.05783801  0.0023042 ] \n",
      "\n",
      "Epoch 54\n",
      "Reward vector: [ 2.91035804 -0.47759628 -0.23146841  0.21035609]\n",
      "Policy: [[  9.99789701e-01   2.10299415e-04]\n",
      " [  5.93132506e-02   9.40686749e-01]\n",
      " [  1.76734349e-01   8.23265651e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.85945898  0.08160539  0.05675316  0.00218247] \n",
      "\n",
      "Epoch 55\n",
      "Reward vector: [ 2.92407045 -0.48553082 -0.23703939  0.21014918]\n",
      "Policy: [[  9.99800882e-01   1.99118400e-04]\n",
      " [  5.81475845e-02   9.41852416e-01]\n",
      " [  1.74475032e-01   8.25524968e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.86287582  0.07934544  0.05570971  0.00206903] \n",
      "\n",
      "Epoch 56\n",
      "Reward vector: [ 2.93745562 -0.49324913 -0.24250992  0.20995286]\n",
      "Policy: [[  9.99811294e-01   1.88705974e-04]\n",
      " [  5.70289743e-02   9.42971026e-01]\n",
      " [  1.72281899e-01   8.27718101e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.86614834  0.07718309  0.05470538  0.00196319] \n",
      "\n",
      "Epoch 57\n",
      "Reward vector: [ 2.95052714 -0.50076041 -0.24788373  0.20976643]\n",
      "Policy: [[  9.99821003e-01   1.78997478e-04]\n",
      " [  5.59546520e-02   9.44045348e-01]\n",
      " [  1.70152016e-01   8.29847984e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.8692848   0.07511284  0.05373804  0.00186433] \n",
      "\n",
      "Epoch 58\n",
      "Reward vector: [ 2.96329785 -0.50807337 -0.2531643   0.20958924]\n",
      "Policy: [[  9.99830065e-01   1.69934651e-04]\n",
      " [  5.49220581e-02   9.45077942e-01]\n",
      " [  1.68082616e-01   8.31917384e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.87229289  0.07312952  0.05280571  0.00177188] \n",
      "\n",
      "Epoch 59\n",
      "Reward vector: [ 2.97577987 -0.5151962  -0.25835495  0.20942071]\n",
      "Policy: [[  9.99838535e-01   1.61464910e-04]\n",
      " [  5.39288232e-02   9.46071177e-01]\n",
      " [  1.66071089e-01   8.33928911e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.87517976  0.07122835  0.05190655  0.00168534] \n",
      "\n",
      "Epoch 60\n",
      "Reward vector: [ 2.98798467 -0.52213668 -0.26345884  0.20926028]\n",
      "Policy: [[  9.99846459e-01   1.53540715e-04]\n",
      " [  5.29727508e-02   9.47027249e-01]\n",
      " [  1.64114967e-01   8.35885033e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.87795209  0.06940482  0.05103884  0.00160425] \n",
      "\n",
      "Epoch 61\n",
      "Reward vector: [ 2.99992306 -0.52890216 -0.26847893  0.20910746]\n",
      "Policy: [[  9.99853881e-01   1.46119015e-04]\n",
      " [  5.20518024e-02   9.47948198e-01]\n",
      " [  1.62211919e-01   8.37788081e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.8806161   0.06765474  0.05020097  0.00152819] \n",
      "\n",
      "Epoch 62\n",
      "Reward vector: [ 3.0116053  -0.53549958 -0.27341808  0.20896179]\n",
      "Policy: [[  9.99860839e-01   1.39160766e-04]\n",
      " [  5.11640832e-02   9.48835917e-01]\n",
      " [  1.60359740e-01   8.39640260e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.88317757  0.0659742   0.04939145  0.00145678] \n",
      "\n",
      "Epoch 63\n",
      "Reward vector: [ 3.0230411  -0.54193553 -0.27827896  0.20882282]\n",
      "Policy: [[  9.99867369e-01   1.32630500e-04]\n",
      " [  5.03078308e-02   9.49692169e-01]\n",
      " [  1.58556343e-01   8.41443657e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.88564195  0.06435952  0.04860887  0.00138966] \n",
      "\n",
      "Epoch 64\n",
      "Reward vector: [ 3.03423968 -0.54821626 -0.28306416  0.20869017]\n",
      "Policy: [[  9.99873504e-01   1.26495952e-04]\n",
      " [  4.94814033e-02   9.50518597e-01]\n",
      " [  1.56799756e-01   8.43200244e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.88801428  0.06280726  0.04785192  0.00132653] \n",
      "\n",
      "Epoch 65\n",
      "Reward vector: [ 3.04520974 -0.55434768 -0.28777609  0.20856346]\n",
      "Policy: [[  9.99879272e-01   1.20727725e-04]\n",
      " [  4.86832695e-02   9.51316730e-01]\n",
      " [  1.55088106e-01   8.44911894e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.89029931  0.06131422  0.04711937  0.0012671 ] \n",
      "\n",
      "Epoch 66\n",
      "Reward vector: [ 3.0559596  -0.56033541 -0.2924171   0.20844235]\n",
      "Policy: [[  9.99884701e-01   1.15299004e-04]\n",
      " [  4.79120003e-02   9.52088000e-01]\n",
      " [  1.53419621e-01   8.46580379e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.89250148  0.05987737  0.04641006  0.00121109] \n",
      "\n",
      "Epoch 67\n",
      "Reward vector: [ 3.0664971  -0.5661848  -0.29698939  0.20832652]\n",
      "Policy: [[  9.99889815e-01   1.10185287e-04]\n",
      " [  4.71662598e-02   9.52833740e-01]\n",
      " [  1.51792619e-01   8.48207381e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.89462493  0.05849389  0.04572291  0.00115827] \n",
      "\n",
      "Epoch 68\n",
      "Reward vector: [ 3.07682975 -0.57190092 -0.30149508  0.20821568]\n",
      "Policy: [[  9.99894636e-01   1.05364165e-04]\n",
      " [  4.64447986e-02   9.53555201e-01]\n",
      " [  1.50205507e-01   8.49794493e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.89667357  0.05716112  0.04505689  0.00110842] \n",
      "\n",
      "Epoch 69\n",
      "Reward vector: [ 3.08696464 -0.57748857 -0.30593619  0.20810955]\n",
      "Policy: [[  9.99899185e-01   1.00815109e-04]\n",
      " [  4.57464465e-02   9.54253554e-01]\n",
      " [  1.48656768e-01   8.51343232e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.89865104  0.05587657  0.04441106  0.00106133] \n",
      "\n",
      "Epoch 70\n",
      "Reward vector: [ 3.09690856 -0.58295236 -0.31031464  0.20800787]\n",
      "Policy: [[  9.99903481e-01   9.65192924e-05]\n",
      " [  4.50701066e-02   9.54929893e-01]\n",
      " [  1.47144964e-01   8.52855036e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.90056079  0.05463789  0.04378451  0.00101681] \n",
      "\n",
      "Epoch 71\n",
      "Reward vector: [ 3.10666796 -0.58829665 -0.31463228  0.2079104 ]\n",
      "Policy: [[  9.99907541e-01   9.24594298e-05]\n",
      " [  4.44147499e-02   9.55585250e-01]\n",
      " [  1.45668727e-01   8.54331273e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.90240604  0.05344289  0.04317638  0.0009747 ] \n",
      "\n",
      "Epoch 72\n",
      "Reward vector: [ 3.11624898 -0.5935256  -0.31889086  0.20781691]\n",
      "Policy: [[  9.99911380e-01   8.86196291e-05]\n",
      " [  4.37794099e-02   9.56220590e-01]\n",
      " [  1.44226757e-01   8.55773243e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [ 0.90418981  0.05228948  0.04258588  0.00093482] \n",
      "\n",
      "Epoch 73\n",
      "Reward vector: [ 3.12565748 -0.59864317 -0.32309209  0.20772721]\n",
      "Policy: [[  9.99915015e-01   8.49852641e-05]\n",
      " [  4.31631780e-02   9.56836822e-01]\n",
      " [  1.42817815e-01   8.57182185e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.05914969e-01   5.11757248e-02   4.20122543e-02   8.97051677e-04] \n",
      "\n",
      "Epoch 74\n",
      "Reward vector: [ 3.13489906 -0.60365315 -0.32723757  0.20764108]\n",
      "Policy: [[  9.99918457e-01   8.15428581e-05]\n",
      " [  4.25651997e-02   9.57434800e-01]\n",
      " [  1.41440723e-01   8.58559277e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.07584199e-01   5.00997763e-02   4.14547837e-02   8.61241004e-04] \n",
      "\n",
      "Epoch 75\n",
      "Reward vector: [ 3.14397906 -0.60855914 -0.33132885  0.20755836]\n",
      "Policy: [[  9.99921720e-01   7.82799798e-05]\n",
      " [  4.19846699e-02   9.58015330e-01]\n",
      " [  1.40094359e-01   8.59905641e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.09200033e-01   4.90599032e-02   4.09127957e-02   8.27267876e-04] \n",
      "\n",
      "Epoch 76\n",
      "Reward vector: [ 3.15290257 -0.61336459 -0.33536741  0.20747886]\n",
      "Policy: [[  9.99924815e-01   7.51851502e-05]\n",
      " [  4.14208304e-02   9.58579170e-01]\n",
      " [  1.38777652e-01   8.61222348e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.10764860e-01   4.80544704e-02   4.03856524e-02   7.95016876e-04] \n",
      "\n",
      "Epoch 77\n",
      "Reward vector: [ 3.16167448 -0.61807278 -0.33935469  0.20740242]\n",
      "Policy: [[  9.99927752e-01   7.22477585e-05]\n",
      " [  4.08729656e-02   9.59127034e-01]\n",
      " [  1.37489582e-01   8.62510418e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.12280934e-01   4.70819345e-02   3.98727500e-02   7.64381144e-04] \n",
      "\n",
      "Epoch 78\n",
      "Reward vector: [ 3.17029944 -0.62268686 -0.34329204  0.20732889]\n",
      "Policy: [[  9.99930542e-01   6.94579863e-05]\n",
      " [  4.03404003e-02   9.59659600e-01]\n",
      " [  1.36229174e-01   8.63770826e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.13750384e-01   4.61408380e-02   3.93735167e-02   7.35261655e-04] \n",
      "\n",
      "Epoch 79\n",
      "Reward vector: [ 3.17878192 -0.62720984 -0.34718078  0.20725814]\n",
      "Policy: [[  9.99933193e-01   6.68067399e-05]\n",
      " [  3.98224965e-02   9.60177504e-01]\n",
      " [  1.34995501e-01   8.65004499e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.15175219e-01   4.52298032e-02   3.88874108e-02   7.07566555e-04] \n",
      "\n",
      "Epoch 80\n",
      "Reward vector: [ 3.18712618 -0.6316446  -0.35102217  0.20719001]\n",
      "Policy: [[  9.99935714e-01   6.42855883e-05]\n",
      " [  3.93186512e-02   9.60681349e-01]\n",
      " [  1.33787673e-01   8.66212327e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.16557343e-01   4.43475278e-02   3.84139184e-02   6.81210564e-04] \n",
      "\n",
      "Epoch 81\n",
      "Reward vector: [ 3.19533633 -0.63599387 -0.35481743  0.2071244 ]\n",
      "Policy: [[  9.99938113e-01   6.18867076e-05]\n",
      " [  3.88282936e-02   9.61171706e-01]\n",
      " [  1.32604843e-01   8.67395157e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.17898554e-01   4.34927795e-02   3.79525522e-02   6.56114435e-04] \n",
      "\n",
      "Epoch 82\n",
      "Reward vector: [ 3.20341627 -0.64026031 -0.35856771  0.20706118]\n",
      "Policy: [[  9.99940397e-01   5.96028309e-05]\n",
      " [  3.83508837e-02   9.61649116e-01]\n",
      " [  1.31446198e-01   8.68553802e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.19200555e-01   4.26643915e-02   3.75028491e-02   6.32204463e-04] \n",
      "\n",
      "Epoch 83\n",
      "Reward vector: [ 3.21136978 -0.64444644 -0.36227415  0.20700024]\n",
      "Policy: [[  9.99942573e-01   5.74272023e-05]\n",
      " [  3.78859095e-02   9.62114091e-01]\n",
      " [  1.30310964e-01   8.69689036e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.20464960e-01   4.18612590e-02   3.70643694e-02   6.09412041e-04] \n",
      "\n",
      "Epoch 84\n",
      "Reward vector: [ 3.21920045 -0.64855467 -0.36593782  0.20694147]\n",
      "Policy: [[  9.99944646e-01   5.53535355e-05]\n",
      " [  3.74328857e-02   9.62567114e-01]\n",
      " [  1.29198398e-01   8.70801602e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.21693297e-01   4.10823346e-02   3.66366951e-02   5.87673249e-04] \n",
      "\n",
      "Epoch 85\n",
      "Reward vector: [ 3.22691175 -0.65258734 -0.36955976  0.20688478]\n",
      "Policy: [[  9.99946624e-01   5.33759765e-05]\n",
      " [  3.69913519e-02   9.63008648e-01]\n",
      " [  1.28107789e-01   8.71892211e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.22887018e-01   4.03266253e-02   3.62194284e-02   5.66928491e-04] \n",
      "\n",
      "Epoch 86\n",
      "Reward vector: [ 3.234507   -0.65654665 -0.37314098  0.20683007]\n",
      "Policy: [[  9.99948511e-01   5.14890690e-05]\n",
      " [  3.65608710e-02   9.63439129e-01]\n",
      " [  1.27038456e-01   8.72961544e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.24047498e-01   3.95931889e-02   3.58121907e-02   5.47122156e-04] \n",
      "\n",
      "Epoch 87\n",
      "Reward vector: [ 3.24198939 -0.66043477 -0.37668244  0.20677725]\n",
      "Policy: [[  9.99950312e-01   4.96877231e-05]\n",
      " [  3.61410277e-02   9.63858972e-01]\n",
      " [  1.25989746e-01   8.74010254e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.25176045e-01   3.88811309e-02   3.54146217e-02   5.28202308e-04] \n",
      "\n",
      "Epoch 88\n",
      "Reward vector: [ 3.249362   -0.66425373 -0.38018508  0.20672624]\n",
      "Policy: [[  9.99952033e-01   4.79671875e-05]\n",
      " [  3.57314272e-02   9.64268573e-01]\n",
      " [  1.24961034e-01   8.75038966e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.26273900e-01   3.81896020e-02   3.50263779e-02   5.10120413e-04] \n",
      "\n",
      "Epoch 89\n",
      "Reward vector: [ 3.25662778 -0.66800551 -0.38364979  0.20667695]\n",
      "Policy: [[  9.99953677e-01   4.63230230e-05]\n",
      " [  3.53316942e-02   9.64668306e-01]\n",
      " [  1.23951719e-01   8.76048281e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.27342243e-01   3.75177947e-02   3.46471317e-02   4.92831075e-04] \n",
      "\n",
      "Epoch 90\n",
      "Reward vector: [ 3.26378956 -0.671692   -0.38707745  0.20662932]\n",
      "Policy: [[  9.99955249e-01   4.47510791e-05]\n",
      " [  3.49414712e-02   9.65058529e-01]\n",
      " [  1.22961226e-01   8.77038774e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.28382196e-01   3.68649417e-02   3.42765708e-02   4.76291809e-04] \n",
      "\n",
      "Epoch 91\n",
      "Reward vector: [ 3.27085007 -0.67531503 -0.39046889  0.20658328]\n",
      "Policy: [[  9.99956753e-01   4.32474723e-05]\n",
      " [  3.45604180e-02   9.65439582e-01]\n",
      " [  1.21989001e-01   8.78010999e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.29394827e-01   3.62303128e-02   3.39143970e-02   4.60462819e-04] \n",
      "\n",
      "Epoch 92\n",
      "Reward vector: [ 3.27781196 -0.67887635 -0.39382492  0.20653875]\n",
      "Policy: [[  9.99958191e-01   4.18085661e-05]\n",
      " [  3.41882103e-02   9.65811790e-01]\n",
      " [  1.21034515e-01   8.78965485e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.30381154e-01   3.56132132e-02   3.35603257e-02   4.45306810e-04] \n",
      "\n",
      "Epoch 93\n",
      "Reward vector: [ 3.28467774 -0.68237765 -0.39714633  0.20649567]\n",
      "Policy: [[  9.99959569e-01   4.04309532e-05]\n",
      " [  3.38245390e-02   9.66175461e-01]\n",
      " [  1.20097256e-01   8.79902744e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.31342145e-01   3.50129812e-02   3.32140849e-02   4.30788797e-04] \n",
      "\n",
      "Epoch 94\n",
      "Reward vector: [ 3.29144987 -0.68582055 -0.40043387  0.20645398]\n",
      "Policy: [[  9.99960889e-01   3.91114386e-05]\n",
      " [  3.34691091e-02   9.66530891e-01]\n",
      " [  1.19176735e-01   8.80823265e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.32278723e-01   3.44289867e-02   3.28754146e-02   4.16875951e-04] \n",
      "\n",
      "Epoch 95\n",
      "Reward vector: [ 3.2981307  -0.68920661 -0.40368828  0.20641363]\n",
      "Policy: [[  9.99962153e-01   3.78470246e-05]\n",
      " [  3.31216393e-02   9.66878361e-01]\n",
      " [  1.18272480e-01   8.81727520e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.33191767e-01   3.38606289e-02   3.25440662e-02   4.03537436e-04] \n",
      "\n",
      "Epoch 96\n",
      "Reward vector: [ 3.30472248 -0.69253735 -0.40691026  0.20637455]\n",
      "Policy: [[  9.99963365e-01   3.66348962e-05]\n",
      " [  3.27818607e-02   9.67218139e-01]\n",
      " [  1.17384037e-01   8.82615963e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.34082119e-01   3.33073350e-02   3.22198019e-02   3.90744276e-04] \n",
      "\n",
      "Epoch 97\n",
      "Reward vector: [ 3.31122743 -0.6958142  -0.4101005   0.20633671]\n",
      "Policy: [[  9.99964528e-01   3.54724090e-05]\n",
      " [  3.24495166e-02   9.67550483e-01]\n",
      " [  1.16510971e-01   8.83489029e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.34950578e-01   3.27685587e-02   3.19023938e-02   3.78469223e-04] \n",
      "\n",
      "Epoch 98\n",
      "Reward vector: [ 3.31764763 -0.69903858 -0.41325966  0.20630004]\n",
      "Policy: [[  9.99965643e-01   3.43570764e-05]\n",
      " [  3.21243615e-02   9.67875639e-01]\n",
      " [  1.15652860e-01   8.84347140e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.35797911e-01   3.22437785e-02   3.15916241e-02   3.66686639e-04] \n",
      "\n",
      "Epoch 99\n",
      "Reward vector: [ 3.32398515 -0.70221183 -0.41638839  0.2062645 ]\n",
      "Policy: [[  9.99966713e-01   3.32865597e-05]\n",
      " [  3.18061607e-02   9.68193839e-01]\n",
      " [  1.14809301e-01   8.85190699e-01]\n",
      " [  5.00000000e-01   5.00000000e-01]]\n",
      "Last-step D: [  9.36624848e-01   3.17324963e-02   3.12872837e-02   3.55372386e-04] \n",
      "\n",
      "Final reward weights:  [ 3.32398515 -0.70221183 -0.41638839  0.2062645 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.32398515, -0.70221183, -0.41638839,  0.2062645 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
