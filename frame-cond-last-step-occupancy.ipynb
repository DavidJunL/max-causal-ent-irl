{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from envs.mdps import MDP_toy_irreversibility, MDP_chain, MDP_water\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  0.   0.5]\n",
      "[ 0.25  0.    0.25  0.5 ]\n",
      "[ 0.     0.25   0.125  0.625]\n",
      "[ 0.125   0.0625  0.1875  0.625 ]\n",
      "[ 0.03125  0.15625  0.125    0.6875 ]\n",
      "[ 0.078125  0.078125  0.140625  0.703125]\n",
      "[ 0.0390625  0.109375   0.109375   0.7421875]\n",
      "[ 0.0546875   0.07421875  0.109375    0.76171875]\n",
      "[ 0.03710938  0.08203125  0.09179688  0.7890625 ]\n",
      "[ 0.04101562  0.06445312  0.08691406  0.80761719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04101562,  0.06445312,  0.08691406,  0.80761719])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(P_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "\n",
    "mdp = MDP_toy_irreversibility()    \n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "# A small example demonstrating what last-step OM looks like for a uniformly random policy.\n",
    "# We can see that most of the probability mass is on the agent ending up in irreversible s_3.\n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.nS)\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vec, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vec = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vec))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_toy_irreversibility()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[1]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[1]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [ 0.00245344 -0.00035729  0.01072747 -0.002642  ]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.00022856  0.09548817  0.00542266 -0.09095778]\n",
      "Last-step D: [ 0.0222488   0.04154534  0.05304808  0.88315779] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [-0.00788774  0.17364386 -0.02523474 -0.13033976]\n",
      "Last-step D: [ 0.08116301  0.21844312  0.30657399  0.39381987] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [-0.01845571  0.2438062  -0.06543299 -0.14973588]\n",
      "Last-step D: [ 0.10567975  0.29837658  0.40198249  0.19396118] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [-0.03042331  0.31070244 -0.10726703 -0.16283048]\n",
      "Last-step D: [ 0.11967597  0.3310376   0.41834048  0.13094596] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [-0.04357858  0.3753991  -0.14904417 -0.17259473]\n",
      "Last-step D: [ 0.13155272  0.35303347  0.4177713   0.09764251] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [-0.05781383  0.43835164 -0.19016225 -0.18019395]\n",
      "Last-step D: [ 0.14235248  0.37047452  0.41118081  0.0759922 ] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [-0.07304258  0.49982246 -0.23036487 -0.18623339]\n",
      "Last-step D: [ 0.15228751  0.3852918   0.40202625  0.06039444] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [-0.08918203  0.55998874 -0.26953998 -0.1910851 ]\n",
      "Last-step D: [ 0.16139453  0.39833725  0.39175111  0.0485171 ] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [-0.10614913  0.61898062 -0.30764587 -0.195004  ]\n",
      "Last-step D: [ 0.16967093  0.41008119  0.38105891  0.03918897] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [-0.12386045  0.67689796 -0.34467843 -0.19817745]\n",
      "Last-step D: [ 0.17711327  0.42082662  0.3703256   0.0317345 ] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [-0.14223335  0.73381913 -0.38065472 -0.20074944]\n",
      "Last-step D: [ 0.18372892  0.43078828  0.35976288  0.02571991] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [-0.1611872   0.78980652 -0.41560398 -0.20283372]\n",
      "Last-step D: [ 0.18953858  0.4401261   0.34949253  0.02084279] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [-0.18064472  0.84491038 -0.44956239 -0.20452165]\n",
      "Last-step D: [ 0.19457522  0.44896134  0.33958418  0.01687926] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [-0.2005329   0.8991718  -0.48256998 -0.2058873 ]\n",
      "Last-step D: [ 0.1988818   0.45738582  0.33007589  0.01365649] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [-0.22078376  0.952625   -0.51466859 -0.20699103]\n",
      "Last-step D: [ 0.20250854  0.46546803  0.32098609  0.01103734] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [-0.24133479  1.00529921 -0.54590067 -0.20788213]\n",
      "Last-step D: [ 0.20551029  0.47325785  0.31232082  0.00891103] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [-0.26212921  1.05722018 -0.57630849 -0.20860085]\n",
      "Last-step D: [ 0.20794426  0.48079036  0.30407817  0.00718722] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [-0.28311601  1.10841126 -0.6059336  -0.20918003]\n",
      "Last-step D: [ 0.20986799  0.48808914  0.29625107  0.0057918 ] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [-0.3042498   1.15889436 -0.63481651 -0.20964642]\n",
      "Last-step D: [ 0.21133787  0.49516909  0.28882916  0.00466389] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [-0.32549059  1.20869049 -0.6629965  -0.21002177]\n",
      "Last-step D: [ 0.21240793  0.5020387   0.2817999   0.00375348] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [-0.3468035   1.25782028 -0.69051144 -0.21032373]\n",
      "Last-step D: [ 0.21312904  0.50870202  0.27514939  0.00301955] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [-0.36815833  1.30630426 -0.71739773 -0.21056658]\n",
      "Last-step D: [ 0.21354836  0.51516019  0.26886291  0.00242853] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [-0.38952924  1.354163   -0.74369026 -0.21076188]\n",
      "Last-step D: [ 0.21370908  0.52141267  0.26292524  0.00195301] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [-0.41089427  1.40141719 -0.76942235 -0.21091895]\n",
      "Last-step D: [ 0.21365026  0.52745809  0.25732097  0.00157067] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [-0.43223496  1.44808769 -0.79462582 -0.21104529]\n",
      "Last-step D: [ 0.21340695  0.53329497  0.25203467  0.00126342] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [-0.45353599  1.49419548 -0.81933093 -0.21114695]\n",
      "Last-step D: [ 0.21301025  0.53892214  0.24705104  0.00101657] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [-0.47478475  1.53976157 -0.84356643 -0.21122877]\n",
      "Last-step D: [ 0.2124876   0.54433908  0.24235505  0.00081827] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [-0.49597105  1.58480697 -0.86735963 -0.21129467]\n",
      "Last-step D: [ 0.21186304  0.549546    0.23793198  0.00065898] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [-0.51708679  1.62935257 -0.89073639 -0.21134777]\n",
      "Last-step D: [  2.11157446e-01   5.54544006e-01   2.33767558e-01   5.30990420e-04] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [-0.53812568  1.67341906 -0.91372118 -0.21139058]\n",
      "Last-step D: [  2.10388874e-01   5.59335054e-01   2.29847945e-01   4.28126794e-04] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [-0.55908297  1.71702687 -0.93633716 -0.21142512]\n",
      "Last-step D: [  2.09572837e-01   5.63921918e-01   2.26159824e-01   3.45420978e-04] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [-0.57995522  1.76019606 -0.9586062  -0.21145301]\n",
      "Last-step D: [  2.08722586e-01   5.68308119e-01   2.22690406e-01   2.78889322e-04] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [-0.60074016  1.80294628 -0.98054895 -0.21147555]\n",
      "Last-step D: [  2.07849374e-01   5.72497834e-01   2.19427455e-01   2.25337739e-04] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [-0.62143643  1.8452967  -1.00218488 -0.21149377]\n",
      "Last-step D: [  2.06962699e-01   5.76495801e-01   2.16359294e-01   1.82206137e-04] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [-0.64204348  1.88726598 -1.02353236 -0.21150851]\n",
      "Last-step D: [  2.06070528e-01   5.80307219e-01   2.13474810e-01   1.47442906e-04] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [-0.66256143  1.92887221 -1.0446087  -0.21152045]\n",
      "Last-step D: [  2.05179496e-01   5.83937650e-01   2.10763450e-01   1.19403876e-04] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [-0.68299094  1.97013292 -1.06543022 -0.21153013]\n",
      "Last-step D: [  2.04295085e-01   5.87392936e-01   2.08215208e-01   9.67711648e-05] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [-0.70333312  2.01106501 -1.08601229 -0.21153798]\n",
      "Last-step D: [  2.03421784e-01   5.90679110e-01   2.05820618e-01   7.84880782e-05] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [-0.72358944  2.05168477 -1.10636936 -0.21154435]\n",
      "Last-step D: [  2.02563228e-01   5.93802331e-01   2.03570734e-01   6.37069748e-05] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [-0.74376168  2.09200789 -1.12651507 -0.21154952]\n",
      "Last-step D: [  2.01722324e-01   5.96768816e-01   2.01457112e-01   5.17475394e-05] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [-0.76385181  2.13204941 -1.14646225 -0.21155373]\n",
      "Last-step D: [  2.00901352e-01   5.99584792e-01   1.99471792e-01   4.20634121e-05] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [-0.78386202  2.17182377 -1.16622298 -0.21155715]\n",
      "Last-step D: [  2.00102068e-01   6.02256443e-01   1.97607274e-01   3.42155098e-05] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [-0.8037946   2.21134478 -1.18580863 -0.21155994]\n",
      "Last-step D: [  1.99325775e-01   6.04789877e-01   1.95856497e-01   2.78507003e-05] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [-0.82365194  2.25062567 -1.20522991 -0.21156221]\n",
      "Last-step D: [  1.98573402e-01   6.07191095e-01   1.94212818e-01   2.26847541e-05] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [-0.84343649  2.28967907 -1.22449691 -0.21156405]\n",
      "Last-step D: [  1.97845558e-01   6.09465963e-01   1.92669990e-01   1.84887141e-05] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [-0.86315075  2.32851705 -1.24361912 -0.21156556]\n",
      "Last-step D: [  1.97142587e-01   6.11620196e-01   1.91222139e-01   1.50779928e-05] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [-0.88279721  2.36715112 -1.2626055  -0.21156679]\n",
      "Last-step D: [  1.96464611e-01   6.13659343e-01   1.89863743e-01   1.23036475e-05] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [-0.90237837  2.40559224 -1.28146446 -0.2115678 ]\n",
      "Last-step D: [  1.95811570e-01   6.15588772e-01   1.88589612e-01   1.00453943e-05] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [-0.92189669  2.44385088 -1.30020394 -0.21156862]\n",
      "Last-step D: [  1.95183255e-01   6.17413670e-01   1.87394870e-01   8.20601100e-06] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [-0.94135463  2.48193697 -1.31883144 -0.21156929]\n",
      "Last-step D: [  1.94579332e-01   6.19139030e-01   1.86274931e-01   6.70684844e-06] \n",
      "\n",
      "Final reward weights:  [-0.94135463  2.48193697 -1.31883144 -0.21156929]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.94135463,  2.48193697, -1.31883144, -0.21156929])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(1)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [-0.00285405 -0.01705156 -0.00223505 -0.00323391 -0.01104429 -0.01027503\n",
      " -0.01083443 -0.01978086]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.09714595 -0.01790568 -0.00484225 -0.02810127 -0.01241198 -0.01381198\n",
      " -0.03382845 -0.06355352]\n",
      "Last-step D: [  3.95941655e-11   8.54117417e-03   2.60720144e-02   2.48673604e-01\n",
      "   1.36769083e-02   3.53695438e-02   2.29940141e-01   4.37726614e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.19714595 -0.01951781 -0.00924642 -0.05767872 -0.01513692 -0.02014128\n",
      " -0.06085215 -0.09188184]\n",
      "Last-step D: [  6.19974171e-10   1.61212829e-02   4.40416605e-02   2.95774456e-01\n",
      "   2.72494023e-02   6.32929893e-02   2.70237050e-01   2.83283159e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.29714594 -0.02212284 -0.01545973 -0.08628872 -0.01979871 -0.02963216\n",
      " -0.08662308 -0.1145299 ]\n",
      "Last-step D: [  8.93327520e-09   2.60503619e-02   6.21330916e-02   2.86100022e-01\n",
      "   4.66178508e-02   9.49087448e-02   2.57709274e-01   2.26480646e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.39714593 -0.02587829 -0.02325967 -0.11243036 -0.02683461 -0.04210114\n",
      " -0.10988058 -0.13407047]\n",
      "Last-step D: [  1.21817170e-07   3.75544359e-02   7.79994551e-02   2.61416406e-01\n",
      "   7.03590206e-02   1.24689812e-01   2.32575028e-01   1.95405721e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.49714577 -0.030803   -0.03222257 -0.13596694 -0.03636643 -0.05681496\n",
      " -0.13062242 -0.15165865]\n",
      "Last-step D: [  1.58775972e-06   4.92471360e-02   8.96289421e-02   2.35365803e-01\n",
      "   9.53181732e-02   1.47138182e-01   2.07418396e-01   1.75881781e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.59714376 -0.03683023 -0.04194104 -0.15724284 -0.04827275 -0.07293858\n",
      " -0.1492338  -0.16799371]\n",
      "Last-step D: [  2.01031656e-05   6.02723302e-02   9.71847253e-02   2.12758953e-01\n",
      "   1.19063218e-01   1.61236255e-01   1.86113811e-01   1.63350604e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.69711879 -0.04390579 -0.05213285 -0.17660858 -0.06237851 -0.08980986\n",
      " -0.16605329 -0.1835391 ]\n",
      "Last-step D: [ 0.00024969  0.0707556   0.10191805  0.19365742  0.14105763  0.16871283\n",
      "  0.16819486  0.15545391] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.79681687 -0.05214376 -0.06261526 -0.19412192 -0.07867731 -0.106882\n",
      " -0.18110942 -0.19857639]\n",
      "Last-step D: [ 0.00301922  0.08237969  0.10482418  0.17513336  0.16298804  0.17072131\n",
      "  0.15056137  0.15037283] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.89353282 -0.06195508 -0.07300336 -0.20905268 -0.09708409 -0.1231125\n",
      " -0.19370038 -0.21293392]\n",
      "Last-step D: [ 0.03284054  0.09811317  0.10388096  0.14930764  0.1840678   0.16230501\n",
      "  0.1259096   0.14357529] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.97089021 -0.07207231 -0.08142598 -0.21920053 -0.11381518 -0.13531808\n",
      " -0.20199493 -0.22437239]\n",
      "Last-step D: [ 0.2264261   0.10117236  0.08422625  0.10147845  0.16731089  0.12205583\n",
      "  0.08294543  0.11438469] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 1.01269119 -0.07836737 -0.0860488  -0.22422688 -0.12315378 -0.14162136\n",
      " -0.20601196 -0.23057023]\n",
      "Last-step D: [ 0.58199021  0.06295052  0.04622813  0.05026352  0.093386    0.06303281\n",
      "  0.04017036  0.06197846] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 1.03667923 -0.08210642 -0.08873353 -0.22707513 -0.12846653 -0.14520046\n",
      " -0.20827388 -0.23413246]\n",
      "Last-step D: [ 0.76011962  0.03739054  0.02684735  0.02848248  0.05312754  0.03579103\n",
      "  0.02261921  0.03562224] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 1.05306212 -0.08468447 -0.09058033 -0.22902305 -0.13205953 -0.1476393\n",
      " -0.20981807 -0.23656656]\n",
      "Last-step D: [ 0.83617107  0.0257805   0.01846803  0.01947922  0.03592993  0.02438838\n",
      "  0.01544186  0.02434101] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 1.06541493 -0.08663484 -0.09197964 -0.23049698 -0.13474718 -0.14947746\n",
      " -0.21098582 -0.2384022 ]\n",
      "Last-step D: [ 0.87647185  0.01950375  0.01399303  0.01473929  0.02687651  0.01838157\n",
      "  0.01167753  0.01835648] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 1.0752977  -0.08819708 -0.09310318 -0.23168056 -0.13688352 -0.1509483\n",
      " -0.21192339 -0.23987086]\n",
      "Last-step D: [ 0.90117234  0.01562238  0.01123548  0.0118358   0.02136344  0.01470836\n",
      "  0.00937565  0.01468654] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 1.08351862 -0.08949698 -0.09404045 -0.23266854 -0.13865106 -0.15217223\n",
      " -0.21270602 -0.24109252]\n",
      "Last-step D: [ 0.9177908   0.01299898  0.00937268  0.00987982  0.01767534  0.01223938\n",
      "  0.00782634  0.01221666] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 1.09054775 -0.09060826 -0.0948437  -0.23351598 -0.14015541 -0.15321917\n",
      " -0.21337739 -0.24213703]\n",
      "Last-step D: [ 0.9297087   0.01111277  0.0080325   0.00847438  0.01504353  0.01046939\n",
      "  0.00671366  0.01044507] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 1.09668173 -0.09157765 -0.09554602 -0.23425763 -0.14146295 -0.15413318\n",
      " -0.21396503 -0.24304846]\n",
      "Last-step D: [ 0.93866026  0.00969394  0.00702322  0.00741651  0.01307537  0.00914003\n",
      "  0.00587638  0.00911428] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 1.10211939 -0.09243659 -0.09616966 -0.23491681 -0.14261798 -0.15494376\n",
      " -0.21448741 -0.24385636]\n",
      "Last-step D: [ 0.9456234   0.00858934  0.00623639  0.00659186  0.01155031  0.00810588\n",
      "  0.00522379  0.00807904] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 1.10700035 -0.09320717 -0.09673027 -0.23550993 -0.1436515  -0.15567166\n",
      " -0.21495751 -0.2445815 ]\n",
      "Last-step D: [ 0.95119031  0.00770582  0.00560611  0.00593119  0.01033526  0.00727895\n",
      "  0.00470099  0.00725136] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 1.11142634 -0.09390553 -0.09723929 -0.23604895 -0.14458603 -0.15633196\n",
      " -0.21538479 -0.24523899]\n",
      "Last-step D: [ 0.95574012  0.00698358  0.00509013  0.00539014  0.00934532  0.00660296\n",
      "  0.00427287  0.00657488] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 1.11547369 -0.09454378 -0.0977053  -0.23654285 -0.14543842 -0.15693598\n",
      " -0.21577638 -0.24584018]\n",
      "Last-step D: [ 0.95952654  0.0063825   0.00466008  0.00493901  0.00852381  0.00604026\n",
      "  0.00391589  0.00601192] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 1.11920112 -0.09513125 -0.09813492 -0.23699856 -0.14622157 -0.15749245\n",
      " -0.21613775 -0.24639381]\n",
      "Last-step D: [ 0.96272567  0.00587469  0.00429624  0.00455717  0.00783154  0.00556471\n",
      "  0.00361372  0.00553627] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 1.12265477 -0.09567526 -0.09853337 -0.23742155 -0.14694562 -0.15800821\n",
      " -0.21647322 -0.24690673]\n",
      "Last-step D: [ 0.9654635   0.00544017  0.00398449  0.00422982  0.00724052  0.00515763\n",
      "  0.00335466  0.00512922] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 1.12587152 -0.09618169 -0.09890481 -0.23781616 -0.14761865 -0.15848874\n",
      " -0.21678623 -0.24738443]\n",
      "Last-step D: [ 0.96783249  0.00506427  0.00371443  0.00394612  0.00673026  0.00480529\n",
      "  0.00313012  0.00477702] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 1.12888132 -0.09665528 -0.09925264 -0.23818595 -0.14824719 -0.15893849\n",
      " -0.2170796  -0.24783136]\n",
      "Last-step D: [ 0.96990204  0.00473596  0.00347827  0.00369789  0.00628542  0.00449742\n",
      "  0.00293365  0.00446934] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 1.1317088  -0.09709997 -0.09957964 -0.23853384 -0.14883662 -0.1593611\n",
      " -0.21735563 -0.24825119]\n",
      "Last-step D: [ 0.97172521  0.00444682  0.00327003  0.00347889  0.00589431  0.00422613\n",
      "  0.0027603   0.0041983 ] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 1.13437447 -0.09751899 -0.09988815 -0.23886226 -0.1493914  -0.15975963\n",
      " -0.21761625 -0.24864697]\n",
      "Last-step D: [ 0.97334327  0.00419028  0.00308506  0.00328426  0.00554783  0.0039853\n",
      "  0.00260623  0.00395776] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 1.13689559 -0.09791511 -0.10018012 -0.23917328 -0.14991529 -0.16013664\n",
      " -0.21786309 -0.24902126]\n",
      "Last-step D: [ 0.97478879  0.00396116  0.00291968  0.00311016  0.00523883  0.00377011\n",
      "  0.00246839  0.00374288] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 1.13928681 -0.09829064 -0.10045721 -0.23946863 -0.15041145 -0.16049431\n",
      " -0.21809752 -0.24937624]\n",
      "Last-step D: [ 0.9760878   0.00375533  0.00277094  0.0029535   0.00496161  0.00357668\n",
      "  0.00234435  0.00354978] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 1.14156067 -0.09864759 -0.10072086 -0.23974981 -0.1508826  -0.1608345\n",
      " -0.21832074 -0.24971377]\n",
      "Last-step D: [ 0.97726138  0.00356944  0.00263647  0.0028118   0.00471153  0.00340189\n",
      "  0.00223215  0.00337534] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 1.143728   -0.09898766 -0.10097229 -0.24001811 -0.15133109 -0.16115881\n",
      " -0.21853376 -0.25003547]\n",
      "Last-step D: [ 0.97832676  0.00340074  0.00251432  0.00268301  0.00448485  0.00324318\n",
      "  0.00213016  0.00321698] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 1.14579818 -0.09931236 -0.10121258 -0.24027465 -0.15175893 -0.16146866\n",
      " -0.21873746 -0.25034273]\n",
      "Last-step D: [ 0.97929815  0.00324697  0.00240287  0.00256545  0.00427845  0.00309844\n",
      "  0.00203706  0.00307261] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 1.14777944 -0.09962298 -0.10144266 -0.24052043 -0.15216791 -0.16176525\n",
      " -0.21893264 -0.25063677]\n",
      "Last-step D: [ 0.98018739  0.00310624  0.00230079  0.00245773  0.00408976  0.00296592\n",
      "  0.00195174  0.00294044] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 1.149679   -0.09992068 -0.10166335 -0.24075629 -0.15255957 -0.16204966\n",
      " -0.21911996 -0.25091868]\n",
      "Last-step D: [ 0.98100442  0.00297699  0.00220694  0.00235864  0.0039166   0.00284413\n",
      "  0.00187326  0.00281901] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35\n",
      "Reward vector: [ 1.15150324 -0.10020647 -0.10187539 -0.24098301 -0.15293528 -0.16232285\n",
      " -0.21930004 -0.25118938]\n",
      "Last-step D: [ 0.98175765  0.00285786  0.00212037  0.00226721  0.00375717  0.00273183\n",
      "  0.00180083  0.00270707] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 1.15325781 -0.10048124 -0.10207941 -0.24120127 -0.15329627 -0.16258564\n",
      " -0.21947342 -0.25144974]\n",
      "Last-step D: [ 0.98245423  0.00274773  0.00204028  0.00218257  0.0036099   0.00262796\n",
      "  0.00173377  0.00260355] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 1.15494778 -0.1007458  -0.10227601 -0.24141167 -0.15364362 -0.1628388\n",
      " -0.21964057 -0.25170049]\n",
      "Last-step D: [ 0.98310028  0.00264562  0.00196595  0.002104    0.00347346  0.00253161\n",
      "  0.00167152  0.00250755] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 1.15657768 -0.10100087 -0.10246569 -0.24161476 -0.15397829 -0.163083\n",
      " -0.21980193 -0.25194232]\n",
      "Last-step D: [ 0.98370107  0.00255069  0.0018968   0.00203087  0.00334672  0.00244199\n",
      "  0.00161358  0.00241827] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 1.15815156 -0.10124709 -0.10264892 -0.24181102 -0.15430116 -0.16331885\n",
      " -0.21995788 -0.25217582]\n",
      "Last-step D: [ 0.98426118  0.00246221  0.00183231  0.00196264  0.00322868  0.00235843\n",
      "  0.00155951  0.00233505] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 1.1596731  -0.10148505 -0.10282612 -0.2420009  -0.15461301 -0.16354688\n",
      " -0.22010878 -0.25240155]\n",
      "Last-step D: [ 0.98478457  0.00237956  0.00177201  0.00189882  0.0031185   0.00228033\n",
      "  0.00150893  0.00225728] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 1.16114563 -0.10171526 -0.10299767 -0.24218481 -0.15491455 -0.1637676\n",
      " -0.22025493 -0.25262   ]\n",
      "Last-step D: [ 0.98527473  0.00230217  0.00171552  0.00183901  0.0030154   0.00220718\n",
      "  0.00146153  0.00218445] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 1.16257216 -0.10193822 -0.10316392 -0.24236309 -0.15520643 -0.16398145\n",
      " -0.22039663 -0.25283161]\n",
      "Last-step D: [ 0.98573471  0.00222957  0.00166249  0.00178284  0.00291875  0.00213853\n",
      "  0.00141701  0.00211611] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 1.16395544 -0.10215436 -0.10332518 -0.24253609 -0.15548922 -0.16418885\n",
      " -0.22053414 -0.25303679]\n",
      "Last-step D: [ 0.98616719  0.00216133  0.00161261  0.00172999  0.00282795  0.00207397\n",
      "  0.00137512  0.00205185] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [ 1.16529799 -0.10236406 -0.10348174 -0.2427041  -0.15576347 -0.16439016\n",
      " -0.2206677  -0.25323593]\n",
      "Last-step D: [ 0.98657455  0.00209707  0.0015656   0.00168017  0.00274251  0.00201315\n",
      "  0.00133562  0.00199133] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 1.16660209 -0.10256771 -0.10363387 -0.24286742 -0.15602967 -0.16458574\n",
      " -0.22079754 -0.25342935]\n",
      "Last-step D: [ 0.98695891  0.00203646  0.00152124  0.00163313  0.00266195  0.00195575\n",
      "  0.00129833  0.00193423] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 1.16786988 -0.10276563 -0.1037818  -0.24302628 -0.15628826 -0.16477589\n",
      " -0.22092384 -0.25361738]\n",
      "Last-step D: [ 0.98732216  0.00197918  0.00147929  0.00158864  0.00258589  0.0019015\n",
      "  0.00126306  0.00188027] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 1.16910328 -0.10295813 -0.10392576 -0.24318093 -0.15653965 -0.1649609\n",
      " -0.22104681 -0.25380029]\n",
      "Last-step D: [ 0.98766598  0.00192499  0.00143958  0.00154651  0.00251395  0.00185015\n",
      "  0.00122965  0.00182919] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 1.17030409 -0.10314549 -0.10406595 -0.24333159 -0.15678423 -0.16514105\n",
      " -0.22116661 -0.25397837]\n",
      "Last-step D: [ 0.98799187  0.00187363  0.00140193  0.00150654  0.00244581  0.00180147\n",
      "  0.00119797  0.00178078] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 1.17147397 -0.10332798 -0.10420257 -0.24347844 -0.15702235 -0.16531657\n",
      " -0.22128339 -0.25415186]\n",
      "Last-step D: [ 0.9883012   0.0018249   0.00136618  0.00146858  0.00238118  0.00175525\n",
      "  0.00116787  0.00173483] \n",
      "\n",
      "Final reward weights:  [ 1.17147397 -0.10332798 -0.10420257 -0.24347844 -0.15702235 -0.16531657\n",
      " -0.22128339 -0.25415186]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.17147397, -0.10332798, -0.10420257, -0.24347844, -0.15702235,\n",
       "       -0.16531657, -0.22128339, -0.25415186])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_chain(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_chain()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec\n",
    "\n",
    "test_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [-0.00288707 -0.00446737  0.01105331  0.00443539  0.00588242 -0.00819224\n",
      "  0.00256397]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.09711293 -0.00446737  0.01105331  0.00443539 -0.02786914 -0.04124972\n",
      " -0.030627  ]\n",
      "Last-step D: [  2.98353198e-11   2.99367566e-11   0.00000000e+00   3.00022240e-11\n",
      "   3.37515537e-01   3.30574816e-01   3.31909647e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.19711293 -0.00446737  0.01105331  0.00443539 -0.06156855 -0.07431652\n",
      " -0.06386078]\n",
      "Last-step D: [  5.60642016e-10   4.57111559e-10   0.00000000e+00   4.58632467e-10\n",
      "   3.36994163e-01   3.30668020e-01   3.32337815e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.29711293 -0.00446737  0.01105331  0.00443539 -0.09522407 -0.10739229\n",
      " -0.0971295 ]\n",
      "Last-step D: [  1.05350254e-08   6.85767206e-09   0.00000000e+00   6.88698826e-09\n",
      "   3.36555149e-01   3.30757630e-01   3.32687196e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.39711291 -0.00446738  0.01105331  0.00443538 -0.12884252 -0.14047667\n",
      " -0.13042663]\n",
      "Last-step D: [  1.97961787e-07   1.00407089e-07   0.00000000e+00   1.00915404e-07\n",
      "   3.36184470e-01   3.30843811e-01   3.32971319e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.49711254 -0.00446752  0.01105331  0.00443524 -0.16242938 -0.17356917\n",
      " -0.16374661]\n",
      "Last-step D: [  3.71974933e-06   1.41912645e-06   0.00000000e+00   1.42723956e-06\n",
      "   3.35868636e-01   3.30924980e-01   3.33199818e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.59710555 -0.00446942  0.01105331  0.00443333 -0.19598611 -0.20666634\n",
      " -0.19708192]\n",
      "Last-step D: [  6.98651512e-05   1.89749741e-05   0.00000000e+00   1.90937694e-05\n",
      "   3.35567310e-01   3.30971716e-01   3.33353040e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.69697514 -0.00449232  0.01105331  0.00441027 -0.22946401 -0.23971644\n",
      " -0.23037755]\n",
      "Last-step D: [  1.30404734e-03   2.29040791e-04   0.00000000e+00   2.30578048e-04\n",
      "   3.34778956e-01   3.30501036e-01   3.32956341e-01] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.79471308 -0.00470414  0.01105331  0.00419696 -0.26207788 -0.27194252\n",
      " -0.2628504 ]\n",
      "Last-step D: [ 0.0226206   0.00211818  0.          0.00213318  0.32613878  0.32226081\n",
      "  0.32472844] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.87131626 -0.00549968  0.01105331  0.00339556 -0.2871972  -0.29678686\n",
      " -0.28789299]\n",
      "Last-step D: [ 0.23396819  0.00795537  0.          0.00801399  0.25119318  0.24844333\n",
      "  0.25042594] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.90672729 -0.00614534  0.01105331  0.00274504 -0.29861662 -0.30808852\n",
      " -0.29928676]\n",
      "Last-step D: [ 0.64588969  0.00645661  0.          0.00650516  0.11419422  0.1130166\n",
      "  0.1139377 ] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.92642827 -0.00654625  0.01105331  0.0023411  -0.30494086 -0.31434891\n",
      " -0.30559825]\n",
      "Last-step D: [ 0.80299028  0.0040091   0.          0.00403943  0.06324234  0.06260397\n",
      "  0.06311487] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 0.93993065 -0.00683126  0.01105331  0.00205392 -0.3092681  -0.31863293\n",
      " -0.30991719]\n",
      "Last-step D: [ 0.86497616  0.00285014  0.          0.00287177  0.04327237  0.04284014\n",
      "  0.04318941] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.95017301 -0.0070514   0.01105331  0.00183211 -0.31254778 -0.32188007\n",
      " -0.31319077]\n",
      "Last-step D: [ 0.89757642  0.00220136  0.          0.0022181   0.03279682  0.03247145\n",
      "  0.03273586] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.95841115 -0.00723037  0.01105331  0.00165178 -0.31518433 -0.3244906\n",
      " -0.31582254]\n",
      "Last-step D: [ 0.91761855  0.00178967  0.          0.0018033   0.02636557  0.0261053\n",
      "  0.02631762] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.96529438 -0.00738096  0.01105331  0.00150004 -0.31738649 -0.32667111\n",
      " -0.31802076]\n",
      "Last-step D: [ 0.93116773  0.00150596  0.          0.00151744  0.0220216   0.02180505\n",
      "  0.02198222] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.97120131 -0.00751085  0.01105331  0.00136916 -0.31927583 -0.32854193\n",
      " -0.31990676]\n",
      "Last-step D: [ 0.9409307   0.00129888  0.          0.00130879  0.01889339  0.0187082\n",
      "  0.01886005] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.97637178 -0.00762497  0.01105331  0.00125416 -0.3209293  -0.33017923\n",
      " -0.32155735]\n",
      "Last-step D: [ 0.94829525  0.00114122  0.          0.00114993  0.01653469  0.01637306\n",
      "  0.01650585] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.98096721 -0.0077267   0.01105331  0.00115166 -0.32239866 -0.33163426\n",
      " -0.32302417]\n",
      "Last-step D: [ 0.95404572  0.00101725  0.          0.00102502  0.01469357  0.01455027\n",
      "  0.01466818] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.98510136 -0.00781842  0.01105331  0.00105924 -0.32372037 -0.3329431\n",
      " -0.32434361]\n",
      "Last-step D: [  9.58658559e-01   9.17259922e-04   0.00000000e+00   9.24272814e-04\n",
      "   1.32170620e-02   1.30884288e-02   1.31944172e-02] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [  9.88857379e-01  -7.90191531e-03   1.10533121e-02   9.75102299e-04\n",
      "  -3.24921065e-01  -3.34132139e-01  -3.25542266e-01]\n",
      "Last-step D: [  9.62439782e-01   8.34940905e-04   0.00000000e+00   8.41327694e-04\n",
      "   1.20070031e-02   1.18903618e-02   1.19865842e-02] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [  9.92297893e-01  -7.97851604e-03   1.10533121e-02   8.97915345e-04\n",
      "  -3.26020815e-01  -3.35221223e-01  -3.26640158e-01]\n",
      "Last-step D: [  9.65594859e-01   7.66007357e-04   0.00000000e+00   7.71869545e-04\n",
      "   1.09974997e-02   1.08908428e-02   1.09789220e-02] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [  9.95471209e-01  -8.04926148e-03   1.10533121e-02   8.26628275e-04\n",
      "  -3.27035085e-01  -3.36225671e-01  -3.27652725e-01]\n",
      "Last-step D: [  9.68266835e-01   7.07454353e-04   0.00000000e+00   7.12870701e-04\n",
      "   1.01426959e-02   1.00444782e-02   1.01256655e-02] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [  9.98415375e-01  -8.11497265e-03   1.10533121e-02   7.60413820e-04\n",
      "  -3.27976055e-01  -3.37157542e-01  -3.28592123e-01]\n",
      "Last-step D: [  9.70558348e-01   6.57111699e-04   0.00000000e+00   6.62144543e-04\n",
      "   9.40970051e-03   9.31870755e-03   9.39398789e-03] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [  1.00116088e+00  -8.17630998e-03   1.10533121e-02   6.98606536e-04\n",
      "  -3.28853486e-01  -3.38026499e-01  -3.29468097e-01]\n",
      "Last-step D: [  9.72544934e-01   6.13373337e-04   0.00000000e+00   6.18072842e-04\n",
      "   8.77431231e-03   8.68957269e-03   8.75973491e-03] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [  1.00373254e+00  -8.23381253e-03   1.10533121e-02   6.40663275e-04\n",
      "  -3.29675319e-01  -3.38840405e-01  -3.30288571e-01]\n",
      "Last-step D: [  9.74283414e-01   5.75025484e-04   0.00000000e+00   5.79432615e-04\n",
      "   8.21833084e-03   8.13905549e-03   8.20474114e-03] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [  1.00615081e+00  -8.28792592e-03   1.10533121e-02   5.86135018e-04\n",
      "  -3.30448100e-01  -3.39605740e-01  -3.31060079e-01]\n",
      "Last-step D: [  9.75817346e-01   5.41133932e-04   0.00000000e+00   5.45282569e-04\n",
      "   7.72780701e-03   7.65334645e-03   7.71508417e-03] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [  1.00843274e+00  -8.33902273e-03   1.10533121e-02   5.34646365e-04\n",
      "  -3.31177287e-01  -3.40327908e-01  -3.31788071e-01]\n",
      "Last-step D: [  9.77180673e-01   5.10968051e-04   0.00000000e+00   5.14886531e-04\n",
      "   7.29187169e-03   7.22168499e-03   7.27991551e-03] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [  1.01059271e+00  -8.38741756e-03   1.10533121e-02   4.85880310e-04\n",
      "  -3.31867480e-01  -3.41011464e-01  -3.32477137e-01]\n",
      "Last-step D: [  9.78400240e-01   4.83948280e-04   0.00000000e+00   4.87660542e-04\n",
      "   6.90193102e-03   6.83556299e-03   6.89065750e-03] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [  1.01264296e+00  -8.43337846e-03   1.10533121e-02   4.39566760e-04\n",
      "  -3.32522590e-01  -3.41660281e-01  -3.33131181e-01]\n",
      "Last-step D: [  9.79497549e-01   4.59609058e-04   0.00000000e+00   4.63135504e-04\n",
      "   6.55110137e-03   6.48816540e-03   6.54043946e-03] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [  1.01459396e+00  -8.47713568e-03   1.10533121e-02   3.95473729e-04\n",
      "  -3.33145971e-01  -3.42277678e-01  -3.33753550e-01]\n",
      "Last-step D: [  9.80490030e-01   4.37572147e-04   0.00000000e+00   4.40930307e-04\n",
      "   6.23380458e-03   6.17396955e-03   6.22369361e-03] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [  1.01645476e+00  -8.51888839e-03   1.10533121e-02   3.53400514e-04\n",
      "  -3.33740518e-01  -3.42866523e-01  -3.34347136e-01]\n",
      "Last-step D: [  9.81391954e-01   4.17527108e-04   0.00000000e+00   4.20732150e-04\n",
      "   5.94547285e-03   5.88845305e-03   5.93586065e-03] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [  1.01823325e+00  -8.55881006e-03   1.10533121e-02   3.13172322e-04\n",
      "  -3.34308751e-01  -3.43429311e-01  -3.34914453e-01]\n",
      "Last-step D: [  9.82215122e-01   3.99216777e-04   0.00000000e+00   4.02281919e-04\n",
      "   5.68233017e-03   5.62787738e-03   5.67317156e-03] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [  1.01993631e+00  -8.59705270e-03   1.10533121e-02   2.74636007e-04\n",
      "  -3.34852874e-01  -3.43968223e-01  -3.35457702e-01]\n",
      "Last-step D: [  9.82969373e-01   3.82426331e-04   0.00000000e+00   3.85363154e-04\n",
      "   5.44122814e-03   5.38912536e-03   5.43248373e-03] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [  1.02157001e+00  -8.63375019e-03   1.10533121e-02   2.37656644e-04\n",
      "  -3.35374826e-01  -3.44485181e-01  -3.35978817e-01]\n",
      "Last-step D: [  9.83662977e-01   3.66974925e-04   0.00000000e+00   3.69793635e-04\n",
      "   5.21952089e-03   5.16957735e-03   5.21115615e-03] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [  1.02313972e+00  -8.66902111e-03   1.10533121e-02   2.02114755e-04\n",
      "  -3.35876323e-01  -3.44981883e-01  -3.36479512e-01]\n",
      "Last-step D: [  9.84302934e-01   3.52709243e-04   0.00000000e+00   3.55418881e-04\n",
      "   5.01496884e-03   4.96701593e-03   5.00695333e-03] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35\n",
      "Reward vector: [  1.02465020e+00  -8.70297096e-03   1.10533121e-02   1.67904049e-04\n",
      "  -3.36358889e-01  -3.45459838e-01  -3.36961310e-01]\n",
      "Last-step D: [  9.84895209e-01   3.39498458e-04   0.00000000e+00   3.42107069e-04\n",
      "   4.82566367e-03   4.77955165e-03   4.81797042e-03] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [  1.02610571e+00  -8.73569399e-03   1.10533121e-02   1.34929545e-04\n",
      "  -3.36823886e-01  -3.45920394e-01  -3.37425567e-01]\n",
      "Last-step D: [  9.85444916e-01   3.27230261e-04   0.00000000e+00   3.29745034e-04\n",
      "   4.64996934e-03   4.60556466e-03   4.64257435e-03] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [  1.02751006e+00  -8.76727476e-03   1.10533121e-02   1.03106036e-04\n",
      "  -3.37272534e-01  -3.46364760e-01  -3.37873503e-01]\n",
      "Last-step D: [  9.85956467e-01   3.15807701e-04   0.00000000e+00   3.18235087e-04\n",
      "   4.48647528e-03   4.44365830e-03   4.47935710e-03] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [  1.02886669e+00  -8.79778942e-03   1.10533121e-02   7.23567904e-05\n",
      "  -3.37705930e-01  -3.46794022e-01  -3.38306212e-01]\n",
      "Last-step D: [  9.86433682e-01   3.05146650e-04   0.00000000e+00   3.07492461e-04\n",
      "   4.33395888e-03   4.29262203e-03   4.32709827e-03] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [  1.03017870e+00  -8.82730680e-03   1.10533121e-02   4.26124660e-05\n",
      "  -3.38125065e-01  -3.47209163e-01  -3.38724686e-01]\n",
      "Last-step D: [  9.86879891e-01   2.95173756e-04   0.00000000e+00   2.97443243e-04\n",
      "   4.19135524e-03   4.15140146e-03   4.18473488e-03] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [  1.03144890e+00  -8.85588927e-03   1.10533121e-02   1.38101963e-05\n",
      "  -3.38530838e-01  -3.47611070e-01  -3.39129820e-01]\n",
      "Last-step D: [  9.87298009e-01   2.85824770e-04   0.00000000e+00   2.88022697e-04\n",
      "   4.05773258e-03   4.01907398e-03   4.05133679e-03] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [  1.03267984e+00  -8.88359359e-03   1.10533121e-02  -1.41071918e-05\n",
      "  -3.38924066e-01  -3.48000553e-01  -3.39522428e-01]\n",
      "Last-step D: [  9.87690595e-01   2.77043183e-04   0.00000000e+00   2.79173881e-04\n",
      "   3.93227207e-03   3.89482883e-03   3.92608667e-03] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [  1.03387385e+00  -8.91047150e-03   1.10533121e-02  -4.11918431e-05\n",
      "  -3.39305491e-01  -3.48378348e-01  -3.39903255e-01]\n",
      "Last-step D: [  9.88059909e-01   2.68779091e-04   0.00000000e+00   2.70846513e-04\n",
      "   3.81425128e-03   3.77795067e-03   3.80826335e-03] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [  1.03503306e+00  -8.93657033e-03   1.10533121e-02  -6.74914455e-05\n",
      "  -3.39675794e-01  -3.48745128e-01  -3.40272977e-01]\n",
      "Last-step D: [  9.88407951e-01   2.60988264e-04   0.00000000e+00   2.62996025e-04\n",
      "   3.70303037e-03   3.66780599e-03   3.69722814e-03] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [  1.03615941e+00  -8.96193346e-03   1.10533121e-02  -9.30497229e-05\n",
      "  -3.40035598e-01  -3.49101512e-01  -3.40632219e-01]\n",
      "Last-step D: [  9.88736500e-01   2.53631361e-04   0.00000000e+00   2.55582774e-04\n",
      "   3.59804069e-03   3.56383169e-03   3.59241339e-03] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [  1.03725469e+00  -8.98660079e-03   1.10533121e-02  -1.17906861e-04\n",
      "  -3.40385475e-01  -3.49448064e-01  -3.40981550e-01]\n",
      "Last-step D: [  9.89047142e-01   2.46673272e-04   0.00000000e+00   2.48571384e-04\n",
      "   3.49877510e-03   3.46552564e-03   3.49331288e-03] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [  1.03832056e+00  -9.01060905e-03   1.10533121e-02  -1.42099880e-04\n",
      "  -3.40725953e-01  -3.49785308e-01  -3.41321497e-01]\n",
      "Last-step D: [  9.89341295e-01   2.40082566e-04   0.00000000e+00   2.41930186e-04\n",
      "   3.40477994e-03   3.37243862e-03   3.39947372e-03] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [  1.03935854e+00  -9.03399215e-03   1.10533121e-02  -1.65662954e-04\n",
      "  -3.41057518e-01  -3.50113725e-01  -3.41652546e-01]\n",
      "Last-step D: [  9.89620233e-01   2.33831024e-04   0.00000000e+00   2.35630743e-04\n",
      "   3.31564813e-03   3.28416754e-03   3.31048957e-03] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [  1.04037003e+00  -9.05678147e-03   1.10533121e-02  -1.88627700e-04\n",
      "  -3.41380620e-01  -3.50433760e-01  -3.41975146e-01]\n",
      "Last-step D: [  9.89885101e-01   2.27893237e-04   0.00000000e+00   2.29647453e-04\n",
      "   3.23101337e-03   3.20034970e-03   3.22599477e-03] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [  1.04135634e+00  -9.07900610e-03   1.10533121e-02  -2.11023420e-04\n",
      "  -3.41695674e-01  -3.50745826e-01  -3.42289712e-01]\n",
      "Last-step D: [  9.90136934e-01   2.22246267e-04   0.00000000e+00   2.23957203e-04\n",
      "   3.15054516e-03   3.12065785e-03   3.14565939e-03] \n",
      "\n",
      "Initial reward vector: [ 0.01207041  0.0029108   0.00877156  0.02208261  0.01292138  0.0031793\n",
      "  0.00159903]\n",
      "Epoch 0\n",
      "Reward vector: [ 0.01207041  0.0029108   0.10877156  0.02208261 -0.02140777 -0.03005733\n",
      " -0.03083519]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   3.39030931e-11   0.00000000e+00\n",
      "   3.43291478e-01   3.32366325e-01   3.24342197e-01] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [ 0.01207041  0.0029108   0.20877156  0.02208261 -0.05557573 -0.06329904\n",
      " -0.06342551]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   6.37277077e-10   0.00000000e+00\n",
      "   3.41679607e-01   3.32417153e-01   3.25903239e-01] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [ 0.01207041  0.0029108   0.30877156  0.02208261 -0.08960944 -0.0965451\n",
      " -0.09614575]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   1.19775747e-08   0.00000000e+00\n",
      "   3.40337085e-01   3.32460565e-01   3.27202338e-01] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [ 0.01207041  0.0029108   0.40877154  0.02208261 -0.12353128 -0.12979494\n",
      " -0.12897404]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   2.25099968e-07   0.00000000e+00\n",
      "   3.39218413e-01   3.32498437e-01   3.28282924e-01] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [ 0.01207041  0.0029108   0.50877111  0.02208261 -0.15735978 -0.16304796\n",
      " -0.1618921 ]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   4.23008074e-06   0.00000000e+00\n",
      "   3.38285001e-01   3.32530122e-01   3.29180647e-01] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [ 0.01207041  0.0029108   0.60876317  0.02208261 -0.19110859 -0.19630026\n",
      " -0.19488305]\n",
      "Last-step D: [  0.00000000e+00   0.00000000e+00   7.94561591e-05   0.00000000e+00\n",
      "   3.37488039e-01   3.32523008e-01   3.29909496e-01] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [ 0.01207041  0.0029108   0.70861486  0.02208261 -0.22475276 -0.22949321\n",
      " -0.22789763]\n",
      "Last-step D: [ 0.          0.          0.00148302  0.          0.33644177  0.33192948\n",
      "  0.33014573] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [ 0.01207041  0.0029108   0.80605148  0.02208261 -0.25759814 -0.26174856\n",
      " -0.26023351]\n",
      "Last-step D: [ 0.          0.          0.02563388  0.          0.3284538   0.32255352\n",
      "  0.3233588 ] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [ 0.01207041  0.0029108   0.88040241  0.02208261 -0.28283829 -0.28594074\n",
      " -0.28515211]\n",
      "Last-step D: [ 0.          0.          0.25649064  0.          0.25240147  0.24192183\n",
      "  0.24918607] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [ 0.01207041  0.0029108   0.91377169  0.02208261 -0.29423792 -0.29663618\n",
      " -0.29642633]\n",
      "Last-step D: [ 0.          0.          0.66630719  0.          0.11399626  0.1069544\n",
      "  0.11274215] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [ 0.01207041  0.0029108   0.93251168  0.02208261 -0.30064876 -0.30262089\n",
      " -0.30277076]\n",
      "Last-step D: [ 0.          0.          0.81260009  0.          0.06410849  0.05984713\n",
      "  0.06344429] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [ 0.01207041  0.0029108   0.94542956  0.02208261 -0.3050697  -0.30674125\n",
      " -0.30714735]\n",
      "Last-step D: [ 0.          0.          0.87082122  0.          0.04420932  0.04120354\n",
      "  0.04376592] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [ 0.01207041  0.0029108   0.95526202  0.02208261 -0.30843523 -0.30987566\n",
      " -0.31047986]\n",
      "Last-step D: [ 0.          0.          0.90167545  0.          0.03365533  0.03134411\n",
      "  0.03332511] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [ 0.01207041  0.0029108   0.96318821  0.02208261 -0.31114846 -0.31240159\n",
      " -0.3131669 ]\n",
      "Last-step D: [ 0.          0.          0.92073804  0.          0.02713227  0.02525927\n",
      "  0.02687042] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [ 0.01207041  0.0029108   0.96982132  0.02208261 -0.3134191  -0.31451502\n",
      " -0.31541592]\n",
      "Last-step D: [ 0.          0.          0.93366894  0.          0.02270647  0.02113436\n",
      "  0.02249023] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [ 0.01207041  0.0029108   0.97552036  0.02208261 -0.31537001 -0.31633062\n",
      " -0.31734846]\n",
      "Last-step D: [ 0.          0.          0.94300959  0.          0.01950908  0.018156\n",
      "  0.01932533] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [ 0.01207041  0.0029108   0.98051346  0.02208261 -0.31707925 -0.31792119\n",
      " -0.31904175]\n",
      "Last-step D: [ 0.          0.          0.95006896  0.          0.0170924   0.01590572\n",
      "  0.01693292] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [ 0.01207041  0.0029108   0.98495452  0.02208261 -0.3185995  -0.31933583\n",
      " -0.32054793]\n",
      "Last-step D: [ 0.          0.          0.9555894   0.          0.01520245  0.01414638\n",
      "  0.01506178] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [ 0.01207041  0.0029108   0.98895222  0.02208261 -0.31996795 -0.32060919\n",
      " -0.32190381]\n",
      "Last-step D: [ 0.         0.         0.9600231  0.         0.0136845  0.0127336\n",
      "  0.0135588] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [ 0.01207041  0.0029108   0.9925861   0.02208261 -0.32121184 -0.32176664\n",
      " -0.32313635]\n",
      "Last-step D: [ 0.          0.          0.96366114  0.          0.01243894  0.0115745\n",
      "  0.01232543] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [ 0.01207041  0.0029108   0.99591617  0.02208261 -0.32235171 -0.3228273\n",
      " -0.32426588]\n",
      "Last-step D: [ 0.          0.          0.96669931  0.          0.01139873  0.01060661\n",
      "  0.01129534] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [ 0.01207041  0.0029108   0.99898875  0.02208261 -0.32340343 -0.32380594\n",
      " -0.32530811]\n",
      "Last-step D: [ 0.          0.          0.96927416  0.          0.01051716  0.0097864\n",
      "  0.01042228] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [ 0.01207041  0.0029108   1.00184038  0.02208261 -0.32437949 -0.3247142\n",
      " -0.32627541]\n",
      "Last-step D: [ 0.          0.          0.97148375  0.          0.00976064  0.00908258\n",
      "  0.00967303] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [ 0.01207041  0.0029108   1.00450034  0.02208261 -0.32528994 -0.32556141\n",
      " -0.32717772]\n",
      "Last-step D: [ 0.          0.          0.97340037  0.          0.00910443  0.0084721\n",
      "  0.00902309] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [ 0.01207041  0.0029108   1.0069925   0.02208261 -0.32614293 -0.32635518\n",
      " -0.32802313]\n",
      "Last-step D: [ 0.          0.          0.97507844  0.          0.0085299   0.00793764\n",
      "  0.00845403] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [ 0.01207041  0.0029108   1.00933653  0.02208261 -0.3269452  -0.32710176\n",
      " -0.32881829]\n",
      "Last-step D: [ 0.          0.          0.9765597   0.          0.00802275  0.00746587\n",
      "  0.00795168] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [ 0.01207041  0.0029108   1.01154886  0.02208261 -0.32770239 -0.3278064\n",
      " -0.3295688 ]\n",
      "Last-step D: [ 0.          0.          0.97787672  0.          0.00757184  0.00704642\n",
      "  0.00750502] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [ 0.01207041  0.0029108   1.01364333  0.02208261 -0.32841922 -0.32847351\n",
      " -0.33027933]\n",
      "Last-step D: [ 0.          0.          0.97905528  0.          0.00716834  0.00667108\n",
      "  0.0071053 ] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [ 0.01207041  0.0029108   1.01563173  0.02208261 -0.32909974 -0.32910684\n",
      " -0.33095388]\n",
      "Last-step D: [ 0.          0.          0.98011602  0.          0.00680517  0.00633327\n",
      "  0.00674554] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [ 0.01207041  0.0029108   1.01752416  0.02208261 -0.3297474  -0.3297096\n",
      " -0.33159588]\n",
      "Last-step D: [ 0.          0.          0.98107571  0.          0.00647661  0.00602764\n",
      "  0.00642004] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [ 0.01207041  0.0029108   1.01932935  0.02208261 -0.33036519 -0.33028459\n",
      " -0.3322083 ]\n",
      "Last-step D: [ 0.          0.          0.98194805  0.          0.00617795  0.00574983\n",
      "  0.00612416] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02105491  0.02208261 -0.33095572 -0.33083421\n",
      " -0.33279371]\n",
      "Last-step D: [ 0.          0.          0.98274442  0.          0.00590531  0.00549622\n",
      "  0.00585404] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02270748  0.02208261 -0.33152127 -0.33136059\n",
      " -0.33335435]\n",
      "Last-step D: [ 0.          0.          0.98347428  0.          0.00565544  0.0052638\n",
      "  0.00560648] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02429292  0.02208261 -0.33206383 -0.33186559\n",
      " -0.33389223]\n",
      "Last-step D: [ 0.          0.          0.98414558  0.          0.00542562  0.00505002\n",
      "  0.00537878] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02581641  0.02208261 -0.33258518 -0.33235086\n",
      " -0.3344091 ]\n",
      "Last-step D: [ 0.          0.          0.98476509  0.          0.00521354  0.00485273\n",
      "  0.00516864] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02728256  0.02208261 -0.33308691 -0.33281788\n",
      " -0.33490651]\n",
      "Last-step D: [ 0.          0.          0.98533854  0.          0.00501722  0.00467012\n",
      "  0.00497412] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [ 0.01207041  0.0029108   1.02869547  0.02208261 -0.33357041 -0.33326794\n",
      " -0.33538586]\n",
      "Last-step D: [ 0.          0.          0.98587086  0.          0.00483499  0.0045006\n",
      "  0.00479355] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03005884  0.02208261 -0.33403694 -0.33370222\n",
      " -0.33584841]\n",
      "Last-step D: [ 0.          0.          0.98636631  0.          0.00466538  0.00434283\n",
      "  0.00462549] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03137599  0.02208261 -0.33448766 -0.33412178\n",
      " -0.33629528]\n",
      "Last-step D: [ 0.          0.          0.98682857  0.          0.00450713  0.00419562\n",
      "  0.00446868] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [ 0.01207041  0.0029108   1.0326499   0.02208261 -0.33492357 -0.33452758\n",
      " -0.33672748]\n",
      "Last-step D: [ 0.          0.          0.98726085  0.          0.00435915  0.00405796\n",
      "  0.00432204] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [ 0.01207041  0.0029108   1.0338833   0.02208261 -0.33534562 -0.33492047\n",
      " -0.33714594]\n",
      "Last-step D: [ 0.          0.          0.98766598  0.          0.00422046  0.00392895\n",
      "  0.00418461] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03507866  0.02208261 -0.33575464 -0.33530125\n",
      " -0.3375515 ]\n",
      "Last-step D: [ 0.          0.          0.98804641  0.          0.00409024  0.0038078\n",
      "  0.00405556] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03623823  0.02208261 -0.33615141 -0.33567063\n",
      " -0.33794491]\n",
      "Last-step D: [ 0.          0.          0.98840433  0.          0.00396771  0.00369382\n",
      "  0.00393414] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03736406  0.02208261 -0.33653664 -0.33602927\n",
      " -0.33832688]\n",
      "Last-step D: [ 0.          0.          0.98874167  0.          0.00385224  0.00358639\n",
      "  0.0038197 ] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03845805  0.02208261 -0.33691096 -0.33637777\n",
      " -0.33869805]\n",
      "Last-step D: [ 0.          0.          0.98906015  0.          0.00374322  0.00348497\n",
      "  0.00371166] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [ 0.01207041  0.0029108   1.03952192  0.02208261 -0.33727497 -0.33671668\n",
      " -0.339059  ]\n",
      "Last-step D: [ 0.          0.          0.9893613   0.          0.00364013  0.00338907\n",
      "  0.0036095 ] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [ 0.01207041  0.0029108   1.04055727  0.02208261 -0.33762922 -0.3370465\n",
      " -0.33941028]\n",
      "Last-step D: [ 0.          0.          0.9896465   0.          0.00354251  0.00329825\n",
      "  0.00351275] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [ 0.01207041  0.0029108   1.04156557  0.02208261 -0.33797422 -0.33736771\n",
      " -0.33975237]\n",
      "Last-step D: [ 0.          0.          0.98991696  0.          0.00344993  0.00321212\n",
      "  0.00342099] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [ 0.01207041  0.0029108   1.04254819  0.02208261 -0.33831042 -0.33768074\n",
      " -0.34008576]\n",
      "Last-step D: [ 0.          0.          0.99017381  0.          0.00336201  0.00313032\n",
      "  0.00333386] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [ 0.01207041  0.0029108   1.04350639  0.02208261 -0.33863826 -0.337986\n",
      " -0.34041086]\n",
      "Last-step D: [ 0.          0.          0.99041803  0.          0.00327841  0.00305255\n",
      "  0.00325101] \n",
      "\n",
      "Final reward weights, at s_0:  [  1.04135634e+00  -9.07900610e-03   1.10533121e-02  -2.11023420e-04\n",
      "  -3.41695674e-01  -3.50745826e-01  -3.42289712e-01]\n",
      "Final reward weights, at s_2:  [ 0.01207041  0.0029108   1.04350639  0.02208261 -0.33863826 -0.337986\n",
      " -0.34041086]\n"
     ]
    }
   ],
   "source": [
    "def test_water(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_water()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[0]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[0]=1\n",
    "    \n",
    "    r_vec_1 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[2]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[2]=1\n",
    "    \n",
    "    r_vec_2 = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    \n",
    "    print('Final reward weights, at s_0: ', r_vec_1)\n",
    "    print('Final reward weights, at s_2: ', r_vec_2)\n",
    "\n",
    "test_water()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
