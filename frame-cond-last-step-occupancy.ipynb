{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdps import MDP_toy_irreversibility\n",
    "from value_iter_and_policy import vi_boltzmann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.5  0.   0.5]\n",
      "[ 1.25  0.5   0.25  1.  ]\n",
      "[ 1.25   0.75   0.375  1.625]\n",
      "[ 1.375   0.8125  0.5625  2.25  ]\n",
      "[ 1.40625  0.96875  0.6875   2.9375 ]\n",
      "[ 1.484375  1.046875  0.828125  3.640625]\n",
      "[ 1.5234375  1.15625    0.9375     4.3828125]\n",
      "[ 1.578125    1.23046875  1.046875    5.14453125]\n",
      "[ 1.61523438  1.3125      1.13867188  5.93359375]\n",
      "[ 1.65625     1.37695312  1.22558594  6.74121094]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.15056818,  0.12517756,  0.1114169 ,  0.61283736])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_D_last_step(mdp, policy, P_0, T, verbose=False):\n",
    "    '''\n",
    "    computes the last-step occupancy measure \n",
    "    '''\n",
    "    D_prev = P_0 \n",
    "    \n",
    "    t = 0\n",
    "    for t in range(T):\n",
    "        \n",
    "        # for T-step OM we'd do D=np.copy(P_0). However, we want the last step one, so:\n",
    "        D = np.zeros_like(P_0)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for a in range(mdp.nA):\n",
    "                # for all s_prime reachable from s by taking a do:\n",
    "                for p_sprime, s_prime, _ in mdp.P[s][a]:                    \n",
    "                    D[s_prime] += D_prev[s] * policy[s, a] * p_sprime\n",
    "                    \n",
    "        D_prev = np.copy(D)\n",
    "        if verbose is True: print(D)\n",
    "    return D\n",
    "\n",
    "\n",
    "mdp = MDP_toy_irreversibility()    \n",
    "P_0=np.zeros(mdp.nS)\n",
    "P_0[0]=1\n",
    "\n",
    "# A small example demonstrating what last-step OM looks like for a uniformly random policy.\n",
    "# We can see that most of the probability mass is on the agent ending up in irreversible s_3.\n",
    "pi = np.ones([mdp.nS, mdp.nA])/mdp.nA\n",
    "compute_D_last_step(mdp, pi, P_0, T=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OM_method(mdp, current_state, P_0, horizon, temperature=1, epochs=1, learning_rate=0.2, r_vec=None):\n",
    "    '''\n",
    "    Modified MaxCausalEnt that maximizes last step occupancy measure for the current state\n",
    "    '''\n",
    "    \n",
    "    if r_vec is None:\n",
    "        r_vec = .01*np.random.randn(mdp.nS)\n",
    "        print('Initial reward vector: {}'.format(r_vec))\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        \n",
    "            # Compute the Boltzmann rational policy \\pi_{s,a} = \\exp(Q_{s,a} - V_s) \n",
    "            V, Q, policy = vi_boltzmann(mdp, 1, r_vec, horizon, temperature) \n",
    "            \n",
    "            D = compute_D_last_step(mdp, policy, P_0, horizon)   \n",
    "            dL_dr_vec = -(current_state - D)\n",
    "\n",
    "            # Gradient descent; gradiend may not be the actual gradient -- have to check the math,\n",
    "            # bit this should do the matching correctly\n",
    "            r_vec = r_vec - learning_rate * dL_dr_vec\n",
    "            \n",
    "            print('Epoch {}'.format(i))\n",
    "            print('Reward vector: {}'.format(r_vec))\n",
    "            #print('Policy: {}'.format(policy))\n",
    "            print('Last-step D: {} \\n'.format(D))\n",
    "\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(horizon=22, #number of timesteps we assume the expert has been acting previously\n",
    "         temperature_irl=1,\n",
    "         learning_rate=.1,\n",
    "         epochs = 50):\n",
    "\n",
    "    mdp = MDP_toy_irreversibility()    \n",
    "    \n",
    "    P_0=np.zeros(mdp.nS)\n",
    "    P_0[1]=1\n",
    "    \n",
    "    current_state=np.zeros(mdp.nS)\n",
    "    current_state[1]=1\n",
    "    \n",
    "    r_vec = OM_method(mdp, current_state, P_0, horizon, temperature_irl, epochs, learning_rate)\n",
    "    print('Final reward weights: ', r_vec)\n",
    "    return r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial reward vector: [ 0.00358513  0.00148062 -0.00887598 -0.00839574]\n",
      "Epoch 0\n",
      "Reward vector: [-0.0044922   0.08504633 -0.02502702 -0.06773307]\n",
      "Last-step D: [ 0.08077336  0.16434291  0.16151038  0.59337335] \n",
      "\n",
      "Epoch 1\n",
      "Reward vector: [-0.01465545  0.15949931 -0.05428284 -0.10276699]\n",
      "Last-step D: [ 0.10163247  0.25547018  0.29255822  0.35033912] \n",
      "\n",
      "Epoch 2\n",
      "Reward vector: [-0.0262588   0.22810878 -0.09052868 -0.12352727]\n",
      "Last-step D: [ 0.11603351  0.31390532  0.36245837  0.20760281] \n",
      "\n",
      "Epoch 3\n",
      "Reward vector: [-0.03897592  0.29351146 -0.12905346 -0.13768805]\n",
      "Last-step D: [ 0.12717119  0.3459732   0.38524782  0.1416078 ] \n",
      "\n",
      "Epoch 4\n",
      "Reward vector: [-0.05270106  0.35675709 -0.16808638 -0.14817562]\n",
      "Last-step D: [ 0.13725139  0.36754366  0.39032925  0.1048757 ] \n",
      "\n",
      "Epoch 5\n",
      "Reward vector: [-0.06737612  0.41835002 -0.20689362 -0.15628625]\n",
      "Last-step D: [ 0.14675063  0.3840707   0.38807235  0.08110632] \n",
      "\n",
      "Epoch 6\n",
      "Reward vector: [-0.08295235  0.47859062 -0.24513572 -0.16270851]\n",
      "Last-step D: [ 0.15576231  0.39759399  0.38242107  0.06422263] \n",
      "\n",
      "Epoch 7\n",
      "Reward vector: [-0.09938174  0.5376869  -0.28264986 -0.16786127]\n",
      "Last-step D: [ 0.16429383  0.40903726  0.37514134  0.05152758] \n",
      "\n",
      "Epoch 8\n",
      "Reward vector: [-0.1166154   0.59579739 -0.31936245 -0.1720255 ]\n",
      "Last-step D: [ 0.17233668  0.41889509  0.36712589  0.04164234] \n",
      "\n",
      "Epoch 9\n",
      "Reward vector: [-0.13460394  0.65305014 -0.3552489  -0.17540327]\n",
      "Last-step D: [ 0.17988532  0.42747248  0.35886456  0.03377764] \n",
      "\n",
      "Epoch 10\n",
      "Reward vector: [-0.15329811  0.70955212 -0.39031291 -0.17814707]\n",
      "Last-step D: [ 0.18694176  0.43498021  0.35064003  0.027438  ] \n",
      "\n",
      "Epoch 11\n",
      "Reward vector: [-0.17264967  0.76539443 -0.42457484 -0.18037588]\n",
      "Last-step D: [ 0.19351563  0.44157683  0.34261937  0.02228817] \n",
      "\n",
      "Epoch 12\n",
      "Reward vector: [-0.19261196  0.82065556 -0.45806494 -0.18218462]\n",
      "Last-step D: [ 0.19962289  0.44738877  0.33490102  0.01808733] \n",
      "\n",
      "Epoch 13\n",
      "Reward vector: [-0.21314037  0.87540349 -0.49081904 -0.18365005]\n",
      "Last-step D: [ 0.20528406  0.45252065  0.32754091  0.01465438] \n",
      "\n",
      "Epoch 14\n",
      "Reward vector: [-0.23419264  0.92969738 -0.52287582 -0.18483489]\n",
      "Last-step D: [ 0.21052271  0.45706108  0.32056782  0.01184839] \n",
      "\n",
      "Epoch 15\n",
      "Reward vector: [-0.25572905  0.98358878 -0.55427511 -0.18579059]\n",
      "Last-step D: [ 0.21536406  0.46108603  0.31399293  0.00955697] \n",
      "\n",
      "Epoch 16\n",
      "Reward vector: [-0.27771244  1.03712267 -0.5850567  -0.18655948]\n",
      "Last-step D: [ 0.21983398  0.46466116  0.30781592  0.00768893] \n",
      "\n",
      "Epoch 17\n",
      "Reward vector: [-0.30010826  1.09033833 -0.6152596  -0.18717643]\n",
      "Last-step D: [ 0.22395815  0.46784338  0.302029    0.00616946] \n",
      "\n",
      "Epoch 18\n",
      "Reward vector: [-0.32288441  1.14327012 -0.64492157 -0.1876701 ]\n",
      "Last-step D: [ 0.22776151  0.4706821   0.29661965  0.00493674] \n",
      "\n",
      "Epoch 19\n",
      "Reward vector: [-0.34601119  1.19594809 -0.67407881 -0.18806406]\n",
      "Last-step D: [ 0.23126783  0.47322024  0.29157241  0.00393952] \n",
      "\n",
      "Epoch 20\n",
      "Reward vector: [-0.36946115  1.24839859 -0.70276583 -0.18837758]\n",
      "Last-step D: [ 0.23449955  0.47549502  0.28687021  0.00313521] \n",
      "\n",
      "Epoch 21\n",
      "Reward vector: [-0.39320891  1.30064472 -0.73101535 -0.18862643]\n",
      "Last-step D: [ 0.23747757  0.47753873  0.28249519  0.0024885 ] \n",
      "\n",
      "Epoch 22\n",
      "Reward vector: [-0.41723103  1.35270678 -0.75885828 -0.18882344]\n",
      "Last-step D: [ 0.24022126  0.47937934  0.2784293   0.0019701 ] \n",
      "\n",
      "Epoch 23\n",
      "Reward vector: [-0.44150587  1.40460268 -0.78632375 -0.18897902]\n",
      "Last-step D: [ 0.24274842  0.48104107  0.27465469  0.00155581] \n",
      "\n",
      "Epoch 24\n",
      "Reward vector: [-0.46601341  1.45634819 -0.81343915 -0.18910159]\n",
      "Last-step D: [ 0.2450754   0.48254486  0.27115401  0.00122573] \n",
      "\n",
      "Epoch 25\n",
      "Reward vector: [-0.49073513  1.50795731 -0.8402302  -0.18919794]\n",
      "Last-step D: [ 0.24721715  0.48390885  0.26791052  0.00096348] \n",
      "\n",
      "Epoch 26\n",
      "Reward vector: [-0.51565386  1.55944243 -0.86672103 -0.18927351]\n",
      "Last-step D: [ 0.24918731  0.48514872  0.26490827  0.0007557 ] \n",
      "\n",
      "Epoch 27\n",
      "Reward vector: [-0.54075369  1.61081462 -0.89293424 -0.18933266]\n",
      "Last-step D: [ 0.25099833  0.48627809  0.26213206  0.00059153] \n",
      "\n",
      "Epoch 28\n",
      "Reward vector: [-0.56601985  1.66208375 -0.91889099 -0.18937887]\n",
      "Last-step D: [  2.52661585e-01   4.87308750e-01   2.59567539e-01   4.62125333e-04] \n",
      "\n",
      "Epoch 29\n",
      "Reward vector: [-0.5914386   1.71325865 -0.94461111 -0.18941491]\n",
      "Last-step D: [  2.54187482e-01   4.88250963e-01   2.57201179e-01   3.60375689e-04] \n",
      "\n",
      "Epoch 30\n",
      "Reward vector: [-0.61699715  1.76434729 -0.97011313 -0.18944297]\n",
      "Last-step D: [  2.55585543e-01   4.89113672e-01   2.55020235e-01   2.80549189e-04] \n",
      "\n",
      "Epoch 31\n",
      "Reward vector: [-0.64268361  1.81535682 -0.99541441 -0.18946477]\n",
      "Last-step D: [  2.56864511e-01   4.89904694e-01   2.53012740e-01   2.18054778e-04] \n",
      "\n",
      "Epoch 32\n",
      "Reward vector: [-0.66848685  1.86629373 -1.02053115 -0.18948169]\n",
      "Last-step D: [  2.58032430e-01   4.90630882e-01   2.51167461e-01   1.69226502e-04] \n",
      "\n",
      "Epoch 33\n",
      "Reward vector: [-0.69439652  1.9171639  -1.04547854 -0.18949481]\n",
      "Last-step D: [  2.59096724e-01   4.91298268e-01   2.49473861e-01   1.31146949e-04] \n",
      "\n",
      "Epoch 34\n",
      "Reward vector: [-0.72040295  1.96797268 -1.07027074 -0.18950496]\n",
      "Last-step D: [  2.60064261e-01   4.91912178e-01   2.47922059e-01   1.01501766e-04] \n",
      "\n",
      "Epoch 35\n",
      "Reward vector: [-0.74649709  2.01872495 -1.09492102 -0.18951281]\n",
      "Last-step D: [  2.60941413e-01   4.92477335e-01   2.46502792e-01   7.84604548e-05] \n",
      "\n",
      "Epoch 36\n",
      "Reward vector: [-0.7726705   2.06942516 -1.11944176 -0.18951886]\n",
      "Last-step D: [  2.61734107e-01   4.92997941e-01   2.45207373e-01   6.05792370e-05] \n",
      "\n",
      "Epoch 37\n",
      "Reward vector: [-0.79891529  2.12007738 -1.14384453 -0.18952354]\n",
      "Last-step D: [  2.62447874e-01   4.93477752e-01   2.44027652e-01   4.67223062e-05] \n",
      "\n",
      "Epoch 38\n",
      "Reward vector: [-0.82522407  2.17068537 -1.16814012 -0.18952714]\n",
      "Last-step D: [  2.63087880e-01   4.93920137e-01   2.42955985e-01   3.59982896e-05] \n",
      "\n",
      "Epoch 39\n",
      "Reward vector: [-0.85158997  2.22125255 -1.19233864 -0.18952991]\n",
      "Last-step D: [  2.63658967e-01   4.94328127e-01   2.41985196e-01   2.77092146e-05] \n",
      "\n",
      "Epoch 40\n",
      "Reward vector: [-0.87800654  2.27178211 -1.2164495  -0.18953204]\n",
      "Last-step D: [  2.64165679e-01   4.94704461e-01   2.41108550e-01   2.13096873e-05] \n",
      "\n",
      "Epoch 41\n",
      "Reward vector: [-0.90446777  2.32227695 -1.24048147 -0.18953367]\n",
      "Last-step D: [  2.64612284e-01   4.95051621e-01   2.40319720e-01   1.63743642e-05] \n",
      "\n",
      "Epoch 42\n",
      "Reward vector: [-0.93096805  2.37273976 -1.26444275 -0.18953493]\n",
      "Last-step D: [  2.65002802e-01   4.95371863e-01   2.39612762e-01   1.25721251e-05] \n",
      "\n",
      "Epoch 43\n",
      "Reward vector: [-0.95750215  2.42317304 -1.28834096 -0.1895359 ]\n",
      "Last-step D: [  2.65341018e-01   4.95667243e-01   2.38982093e-01   9.64563162e-06] \n",
      "\n",
      "Epoch 44\n",
      "Reward vector: [-0.9840652   2.47357907 -1.3121832  -0.18953664]\n",
      "Last-step D: [  2.65630500e-01   4.95939640e-01   2.38422464e-01   7.39519850e-06] \n",
      "\n",
      "Epoch 45\n",
      "Reward vector: [-1.01065266  2.52395999 -1.3359761  -0.1895372 ]\n",
      "Last-step D: [  2.65874616e-01   4.96190775e-01   2.37928943e-01   5.66610142e-06] \n",
      "\n",
      "Epoch 46\n",
      "Reward vector: [-1.03726032  2.57431777 -1.35972579 -0.18953764]\n",
      "Last-step D: [  2.66076541e-01   4.96422229e-01   2.37496891e-01   4.33861552e-06] \n",
      "\n",
      "Epoch 47\n",
      "Reward vector: [-1.06388424  2.62465423 -1.38343798 -0.18953797]\n",
      "Last-step D: [  2.66239275e-01   4.96635453e-01   2.37121952e-01   3.32021468e-06] \n",
      "\n",
      "Epoch 48\n",
      "Reward vector: [-1.09052081  2.67497105 -1.40711798 -0.18953822]\n",
      "Last-step D: [  2.66365646e-01   4.96831787e-01   2.36800028e-01   2.53947651e-06] \n",
      "\n",
      "Epoch 49\n",
      "Reward vector: [-1.11716664  2.7252698  -1.43077071 -0.18953842]\n",
      "Last-step D: [  2.66458325e-01   4.97012464e-01   2.36527269e-01   1.94132918e-06] \n",
      "\n",
      "Final reward weights:  [-1.11716664  2.7252698  -1.43077071 -0.18953842]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.11716664,  2.7252698 , -1.43077071, -0.18953842])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(1)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
